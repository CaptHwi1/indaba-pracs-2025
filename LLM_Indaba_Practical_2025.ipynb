{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLM Practical Indaba 2025\n",
        "\n",
        "<!-- To include imahe -->\n",
        "\n",
        "<!-- <a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Foundations_of_LLMs/foundations_of_llms_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> -->\n",
        "\n",
        "Â© Deep Learning Indaba 2025. Apache License 2.0.\n",
        "\n",
        "**Authors: Tejumade, Annie, Jabez, Abla, Amel, Massimo, Sebastian**\n",
        "\n",
        "**Reviewers: [TBD]**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [<font color='orange'>Hugging Face Introduction</font>, <font color='green'>Attention Mechanism</font>, <font color='green'>Transformer Architecture</font>, <font color='green'>Training your own LLM from scratch</font>]\n",
        "\n",
        "Level: <font color='orange'>Beginner</font>, <font color='green'>Intermediate</font>, <font color='blue'>Advanced</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understand the idea behind [Attention](https://arxiv.org/abs/1706.03762) and why it is used.\n",
        "* Present and describe the fundamental building blocks of the [Transformer Architecture](https://arxiv.org/abs/1706.03762) along with an intuition on such an architecture design.\n",
        "* Build and train your own LLM.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Basic knowledge of Deep Learning.\n",
        "* Familiarity with Natural Language Processing (NLP).\n",
        "* Understanding of sequence-to-sequence models.\n",
        "* Basic understanding of Linear Algebra.\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdDHcI_ArCR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"beginner\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "\n",
        "if experience == \"beginner\": sections_to_follow = \"\"\"we recommend you to not attempt to do every coding task but instead, skip through to every section and ensure you interact with the LoRA finetuned LLM presented in the last section as well as with the pretrained LLM to get a practical understanding of how these models behave\"\"\"\n",
        "\n",
        "elif experience == \"intermediate\": sections_to_follow = \"\"\"we recommend you go through every section in this notebook and try the coding tasks tagged as beginner or intermediate. If you get stuck on the code ask a tutor for help or move on to better use the time of the practical\"\"\"\n",
        "\n",
        "elif experience == \"advanced\": sections_to_follow = \"\"\"we recommend you go through every section and try every coding task until you get it to work\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Based on your experience, {sections_to_follow}.\\nNote: this is just a guideline, feel free to explore the colab as you'd like if you feel comfort able!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installations, Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for deep learning, NLP, and plotting\n",
        "!pip install transformers datasets  # Transformers and datasets libraries for NLP tasks\n",
        "!pip install seaborn umap-learn     # Seaborn for plotting, UMAP for dimensionality reduction\n",
        "!pip install livelossplot           # LiveLossPlot for tracking model training progress\n",
        "!pip install -q transformers[torch] # Transformers with PyTorch backend\n",
        "!pip install -q peft                # Parameter-Efficient Fine-Tuning library\n",
        "!pip install accelerate -U          # Accelerate library for performance\n",
        "!pip install gensim\n",
        "\n",
        "# Install utilities for debugging and console output formatting\n",
        "!pip install -q ipdb                # Interactive Python Debugger\n",
        "!pip install -q colorama            # Colored terminal text output\n",
        "\n",
        "!pip install  gemma==3 # needs python >=3.11"
      ],
      "metadata": {
        "id": "GG5qBhCn6xVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# Import system and math utilities\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# Check for connected accelerators (GPU or TPU) and set up accordingly\n",
        "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"A GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "# Avoid GPU memory allocation to be done by JAX\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "# Import libraries for JAX-based deep learning\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "# Import NLP and model-related libraries\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "# Import image processing and plotting libraries\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Import additional utilities for working with text and models\n",
        "import torch\n",
        "import torchvision\n",
        "import itertools\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Download an example image to use in the notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "# Import libraries for NLP preprocessing and working with pre-trained models\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "# Import Hugging Face tools and IPython widgets\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import colorama\n",
        "\n",
        "# Set Matplotlib to output SVG format for better quality plots\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Plotting Functions. (Run Cell)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"\n",
        "    Plots the position encodings matrix.\n",
        "\n",
        "    Args:\n",
        "        P: Position encoding matrix (2D array).\n",
        "        max_tokens: Maximum number of tokens (rows) to plot.\n",
        "        d_model: Dimensionality of the model (columns) to plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the plot size based on the number of tokens and model dimensions\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "\n",
        "    # Plot the position encoding matrix with a color map for better visualization\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "\n",
        "    # Add a color bar to indicate the encoding values\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    # Show embedding indices as ticks if the dimensionality is small\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "\n",
        "    # Show position indices as ticks if the number of tokens is small\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "\n",
        "    # Label the axes\n",
        "    plt.xlabel(\"Embedding index\")\n",
        "    plt.ylabel(\"Position index\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of patches and plots them.\n",
        "\n",
        "    Args:\n",
        "        patches: A list or array of image patches to plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the figure for plotting patches\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "\n",
        "    # Create a subplot for each patch and display it\n",
        "    axes = []\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "\n",
        "    # Adjust layout to prevent overlap and display the plot\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Projects high-dimensional embeddings onto 2D space and plots them.\n",
        "\n",
        "    Args:\n",
        "        embeddings: High-dimensional embedding vectors to project.\n",
        "        labels: Labels corresponding to each embedding for coloring in the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import UMAP and Seaborn for dimensionality reduction and plotting\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Reduce the dimensionality of the embeddings to 2D using UMAP\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    # Plot the 2D projections with labels using Seaborn for better aesthetics\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projected text embeddings\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"\n",
        "    Plots an attention weight matrix with custom axis ticks.\n",
        "\n",
        "    Args:\n",
        "        weight_matrix: The attention weight matrix to plot.\n",
        "        x_ticks: Labels for the x-axis (typically the query tokens).\n",
        "        y_ticks: Labels for the y-axis (typically the key tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the plot size\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot the attention weight matrix as a heatmap\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "\n",
        "    # Set custom ticks on the x and y axes\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "\n",
        "    # Label the plot\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMkaKekB_pR4"
      },
      "outputs": [],
      "source": [
        "# @title Helper Text Processing Functions. (Run Cell)\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def print_sample(prompt, sample, model_name=\"\", generation_time=None):\n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family:monospace; border:1px solid #ccc; padding:10px\">\n",
        "        <div><b style='color:teal;'>ð¤ Model:</b> <span>{model_name}</span></div>\n",
        "        {'<div><b style=\"color:orange;\">â±ï¸ Generation Time:</b> ' + f'{generation_time:.2f}s</div>' if generation_time else ''}\n",
        "        <div><b style='color:green;'>ð Prompt:</b> {prompt}</div>\n",
        "        <div><b style='color:purple;'>â¨ Generated:</b> {sample}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ð¤ Load a Model from Hugging Face and Interact Locally [Beginner] (10 mins)\n",
        "\n",
        "ð¤ **Assigned to**: Abla\n",
        "\n",
        "### ð¯ Objective\n",
        "\n",
        "- â Learn how to **load a model from Hugging Face** and run **inference** using an LLM  \n",
        "- â Load a lightweight model (e.g. `gpt-neo-125m`) and **prompt** it with a simple question\n",
        "- â **Experiment** with different generation parameters such as:\n",
        "  - `temperature`\n",
        "  - different prompts\n",
        "  - `max_length`, `top_k`, `top_p`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ð ï¸ Proposed Changes\n",
        "\n",
        "- â Add support for **newer lightweight models**:\n",
        "  - [Gemma 3.1B (IT)](https://huggingface.co/google/gemma-3-1b-it)\n",
        "  - [Phi-4](https://huggingface.co/microsoft/phi-4)\n",
        "  - [Qwen 3 0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)"
      ],
      "metadata": {
        "id": "8Lrb9vL2uO5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Introduction & Setup**\n",
        "\n",
        "ð Welcome to the World of Large Language Models!\n",
        "We're thrilled to have you on board! ð Before we dive into the hands-on part of our journey, let's take a quick detour into the fascinating world of [Hugging Face](https://huggingface.co/)âan incredible open-source platform for building and deploying cutting-edge language models.\n",
        "\n",
        "##### **ð ï¸ Installation and Imports**\n"
      ],
      "metadata": {
        "id": "_tADD6ZfEX-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this first!)\n",
        "!pip install transformers torch accelerate ipywidgets -q\n",
        "!pip install -q colorama"
      ],
      "metadata": {
        "id": "EjJrKRatFTRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import colorama\n",
        "\n",
        "print(\"â Setup complete!\")\n",
        "print(f\"ð¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ð¤ Transformers version: {transformers.__version__}\")\n",
        "print(f\"ð» CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "n9Bs7fR7FXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also create a simple function to display our results nicely:\n"
      ],
      "metadata": {
        "id": "isA03cHZFkMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def print_sample(prompt, sample, model_name=\"\", generation_time=None):\n",
        "    html = f\"\"\"\n",
        "    <div style=\"font-family:monospace; border:1px solid #ccc; padding:10px\">\n",
        "        <div><b style='color:teal;'>ð¤ Model:</b> <span>{model_name}</span></div>\n",
        "        {'<div><b style=\"color:orange;\">â±ï¸ Generation Time:</b> ' + f'{generation_time:.2f}s</div>' if generation_time else ''}\n",
        "        <div><b style='color:green;'>ð Prompt:</b> {prompt}</div>\n",
        "        <div><b style='color:purple;'>â¨ Generated:</b> {sample}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))"
      ],
      "metadata": {
        "id": "jD_43p8sFnYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Understanding Large Language Models**"
      ],
      "metadata": {
        "id": "s3BQCNjiFtNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ð§  What are Large Language Models?\n",
        "\n",
        "Large Language Models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like text. They work by learning patterns in language and predicting the most likely next word given some context.\n",
        "\n",
        "**Key concepts:**\n",
        "\n",
        "*   Pattern Recognition: LLMs analyze billions of words to understand language\n",
        "*   Next-Word Prediction: At their core, they guess the most probable next word\n",
        "*   Context Understanding: They consider the entire input when making predictions\n"
      ],
      "metadata": {
        "id": "me-nBsxtGBA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **About HuggingFace**\n",
        "\n",
        "![Alt Text](https://www.hugging-face.org/wp-content/uploads/2023/11/hugging-faces.png)"
      ],
      "metadata": {
        "id": "ltfz9jstGSTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HuggingFace** is the \"GitHub of AI\" - a platform that democratizes access to cutting-edge AI models. Founded in 2016, they provide:\n",
        "\n",
        "* [Model Hub](https://huggingface.co/models): Thousands of pre-trained models ready to use  \n",
        "* [Transformers Library](https://huggingface.co/docs/transformers): Easy-to-use tools for working with language models  \n",
        "* [Datasets](https://huggingface.co/datasets): Curated datasets for training and evaluation  \n",
        "* [Spaces](https://huggingface.co/spaces): Platform for hosting ML demos and applications  \n",
        "\n",
        "In this Colab we print prompts in <span style=\"color:pink;\"><b>pink</b></span> and samples generated from a model in <span style=\"color:blue;\"><b>blue</b></span> like in the example below:\n"
      ],
      "metadata": {
        "id": "7s0Lc8rnGZt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_sample(prompt='My fake prompt', sample=' is awesome!')"
      ],
      "metadata": {
        "id": "VNt4n7hrGOY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3. Your First Language Model**"
      ],
      "metadata": {
        "id": "M1e-FfGzGrr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive into how simple it is to load and interact with a model from **Hugging Face**! ð¤\n",
        "\n",
        "For this tutorial, we've pre-configured multiple model options for you to experiment with:\n",
        "\n",
        "* **EleutherAI/gpt-neo-125M** â A lightweight model with 125 million parameters. Itâs fast and memory-efficientâgreat for getting started!\n",
        "* **gpt2** and **gpt2-medium** â Classic models trained by OpenAI, with 117M and 355M parameters respectively. The medium variant offers more fluency and coherence.\n",
        "* **tiiuae/falcon-rw-1b** â A larger open-source model from the Falcon family, with 1 billion parameters.\n",
        "* **google/gemma-2b-it** â A powerful instruction-tuned model from Google, great for understanding context and following prompts.\n",
        "* **microsoft/phi-4** â A cutting-edge model by Microsoft focused on high-quality language generation with smaller memory footprint.\n",
        "\n",
        "You can switch between these models by restarting the Colab kernel and updating the `model_name` variable in the cell below.\n",
        "\n",
        "> ð¡ **Note:** The loading and interaction steps shown here apply to **any** Hugging Face model that supports text generation via the `pipeline` API. Feel free to explore beyond this list!\n"
      ],
      "metadata": {
        "id": "n8NCoWUnGwD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Login to Hugging Face**"
      ],
      "metadata": {
        "id": "DNbBe9KmGwGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login to Hugging Face to access private/gated models\n",
        "# login()"
      ],
      "metadata": {
        "id": "woLdtrhLGloN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's generate some text:"
      ],
      "metadata": {
        "id": "c87TjRSRHCmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model name to \"EleutherAI/gpt-neo-125M\" (this can be changed via the dropdown options)\n",
        "model_name = \"Qwen/Qwen3-0.6B\"  # @param [\"EleutherAI/gpt-neo-125M\", \"gpt2\", \"gpt2-medium\", \"tiiuae/falcon-rw-1b\",\"google/gemma-2b-it\",\"microsoft/phi-4\",\"Qwen/Qwen3-0.6B\",\"google/gemma-3-1b-it\"]\n",
        "\n",
        "# Define the prompt for the text generation model\n",
        "test_prompt = 'What is love?'  # @param {type: \"string\"}\n",
        "\n",
        "# Create a text generation pipeline using the specified model\n",
        "generator = transformers.pipeline('text-generation', model=model_name)\n",
        "\n",
        "# Generate text based on the provided prompt\n",
        "# 'do_sample=True' enables sampling to introduce randomness in generation, and 'min_length=30' ensures at least 30 tokens are generated\n",
        "model_output = generator(test_prompt, do_sample=True, min_length=30)\n",
        "\n",
        "# Print the generated text sample, removing the original prompt from the output\n",
        "print_sample(test_prompt, model_output[0]['generated_text'].split(test_prompt)[1].rstrip())"
      ],
      "metadata": {
        "id": "YvRivN3VG_yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ð¡Tip:*** Try running the code above with different prompts or with the same prompt more than once!"
      ],
      "metadata": {
        "id": "IiXSAfkdHJ0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ð¤ Discussion:*** Why do you think the generated text changes every time, even with the same prompt? Write your response in the input field below and discuss with your neighbour."
      ],
      "metadata": {
        "id": "xxouM28THJXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>Answer</strong></summary>\n",
        "\n",
        "The model uses sampling with randomness (temperature > 0) to generate diverse outputs.  \n",
        "Even with the same input, the probabilistic nature of text generation leads to different results.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "AMwaKFMnHUJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Understanding Generation Parameters**\n",
        "\n",
        "Generation parameters control how the model produces text. Let's explore the most important ones:\n",
        "\n",
        "### Temperature  \n",
        "Controls the randomness of predictions:\n",
        "\n",
        "- **Low (0.1â0.5):** Conservative, predictable outputs  \n",
        "- **Medium (0.6â1.0):** Balanced creativity and coherence  \n",
        "- **High (1.1â2.0):** Very creative but potentially incoherent  \n",
        "\n",
        "### Top-p (Nucleus Sampling)  \n",
        "Controls diversity by limiting the vocabulary considered:\n",
        "\n",
        "- **Low (0.1â0.3):** Very focused on most likely words  \n",
        "- **High (0.8â1.0):** Considers more word possibilities  \n",
        "\n",
        "---\n",
        "\n",
        "**Let's experiment with these parameters:**\n"
      ],
      "metadata": {
        "id": "5nflrXCXHY6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose Model and Prompt { run: \"auto\" }\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # @param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "prompt = \"Once upon a time in a magical forest,\"  # @param {type:\"string\"}\n",
        "temperature = 1  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "top_p = 0.2  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "max_new_tokens = 64  # @param {type:\"slider\", min:10, max:256, step:1}\n",
        "seed = 2  # @param {type:\"integer\"}\n",
        "\n",
        "# Load the model based on selection\n",
        "if 'gpt2' in model_name:\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "else:\n",
        "    raise NotImplementedError(f\"{model_name} is not yet supported.\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "# Align tokenizer padding\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "def run_sample(\n",
        "    model,  # The language model weâll use to generate text\n",
        "    tokenizer,  # The tokenizer that converts text into a format the model understands\n",
        "    prompt: str,  # The text prompt we'll give to the model to start the text generation\n",
        "    seed: int | None = None,  # Optional: A number to make the results predictable each time\n",
        "    temperature: float = 0.6,  # Controls how random the modelâs output is; lower values make it more focused\n",
        "    top_p: float = 0.9,  # Controls how much of the most likely words are considered; higher values consider more options\n",
        "    max_new_tokens: int = 64,  # The maximum number of words or tokens the model will add to the prompt\n",
        ") -> str:\n",
        "    # This function generates text based on a given prompt using a language model,\n",
        "    # with options to control randomness, the number of tokens generated, and reproducibility.\n",
        "\n",
        "    # Convert the prompt text into tokens that the model can process\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Extract the tokens (input IDs) and attention mask (to focus on important parts) from the inputs\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Move the tokens and attention mask to the same device as the model (like a GPU if available)\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    # Set up how we want the model to generate text\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "        do_sample=True,  # Allow the model to add some randomness to its text generation\n",
        "        temperature=temperature,  # Adjust how random the output is; lower means more focused\n",
        "        top_p=top_p,  # Consider the most likely words that make up the top 90% of possibilities\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Use the token ID that represents padding (extra space)\n",
        "        top_k=0,  # We're not limiting to the top-k words, so we set this to 0\n",
        "    )\n",
        "\n",
        "    # If a seed is provided, set it so that the results are repeatable (same output each time)\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Generate text using the model with the settings we defined\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,  # Provide the input tokens to the model\n",
        "        attention_mask=attention_mask,  # Provide the attention mask to help the model focus\n",
        "        return_dict_in_generate=True,  # Ask the model to return detailed information\n",
        "        output_scores=True,  # Include the scores (confidence levels) for the generated tokens\n",
        "        max_new_tokens=max_new_tokens,  # Set the maximum number of tokens to generate\n",
        "        generation_config=generation_config,  # Apply our custom text generation settings\n",
        "    )\n",
        "\n",
        "    # Make sure only one sequence (output) is generated, to keep things simple\n",
        "    assert len(generation_output.sequences) == 1\n",
        "\n",
        "    # Get the generated sequence of tokens\n",
        "    output_sequence = generation_output.sequences[0]\n",
        "\n",
        "    # Convert the generated tokens back into readable text\n",
        "    output_string = tokenizer.decode(output_sequence)\n",
        "\n",
        "    # Print the prompt and the generated response\n",
        "    print_sample(prompt, output_string)\n",
        "\n",
        "    # Return the generated text response\n",
        "    return output_string\n",
        "\n",
        "# Run the interactive generation\n",
        "_ = run_sample(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompt=prompt,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    seed=seed,\n",
        "    max_new_tokens=max_new_tokens\n",
        ")\n"
      ],
      "metadata": {
        "id": "cxq7nR0pHnJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ð¯ Try This:** Experiment with different prompts and temperature values. What patterns do you notice?\n"
      ],
      "metadata": {
        "id": "filsIJLTIEkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Building a Real-World Applications**\n",
        "\n",
        "Language models have many practical applications. Let's explore a few:"
      ],
      "metadata": {
        "id": "J5U3YbK_IGel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Generation**"
      ],
      "metadata": {
        "id": "TQxfHAkuIZCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_prompt = \"Write a Python function that calculates the fibonacci sequence:\"\n",
        "code_result = run_sample(model, tokenizer, code_prompt, temperature=0.3, max_new_tokens=200)\n",
        "print(\"ð» Code Generation:\")\n",
        "print(code_result)"
      ],
      "metadata": {
        "id": "AdnWggRcIXwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question Answering**"
      ],
      "metadata": {
        "id": "5PbWOseNIr6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = \"What are the main advantages of using version control in software development?\"\n",
        "qa_result = run_sample(model, tokenizer, qa_prompt, temperature=0.5, max_new_tokens=80)"
      ],
      "metadata": {
        "id": "07daslXuIpgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creative Writing**"
      ],
      "metadata": {
        "id": "qDThwJmWIyCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story_prompt = \"Write the opening paragraph of a science fiction story:\"\n",
        "story_result = run_sample(model, tokenizer, story_prompt, temperature=0.9, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "YmEK-JD5IzMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty cool, right? ð¤© While this might have seemed revolutionary back in 2021, many of you have probably interacted with **Large Language Models (LLMs)** by now.  \n",
        "Today, we're going to take things a step further â by **training our own Shakespeare-inspired LLM**! This hands-on experience will help us understand how these models actually work **under the hood**.\n",
        "\n",
        "But before we dive into training, we need to build a **solid foundation**.  \n",
        "We'll start by exploring what **Large Language Models** really are, and review the **core machine learning concepts** behind this powerful technology.\n",
        "\n",
        "At the heart of today's **state-of-the-art (SoTA)** LLMs are two key ideas:  \n",
        "\n",
        "\n",
        "*   ð¹ The **Attention Mechanism**\n",
        "*   ð¹ The **Transformer Architecture**\n",
        "\n",
        "\n",
        "\n",
        "We'll unpack both of these in the next sections of the tutorial. ðð¡\n"
      ],
      "metadata": {
        "id": "SEpNUKkoJOUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ð Attention [Intermediate] (25 mins)\n",
        "\n",
        "ð¥ **Assigned to**: Amel\n",
        "\n",
        "### ð¯ Objective:\n",
        "\n",
        "- â Implement **attention**, **masked attention**, and **multi-head attention**\n",
        "- â Explore **visualizations** to understand each mechanism\n",
        "- â Implement **self-attention**\n",
        "- â Write code to compute **Q (Query)**, **K (Key)**, and **V (Value)** vectors  \n",
        "- â Implement **masked self-attention**\n",
        "- â Build **multi-head attention** from self-attention\n",
        "\n",
        "### ð ï¸ Proposed Changes\n",
        "\n",
        "- â Improve visualisation for attention matrices\n",
        "- Add images to support visual learners"
      ],
      "metadata": {
        "id": "vY02IFQouwjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From RNNs to Transformers\n",
        "\n",
        "\n",
        "RNNs are particularly effective in capturing short-term dependencies in sequences. However, they suffer from the vanishing gradient problem, where the influence of earlier inputs diminishes exponentially as the sequence progresses, making it difficult to capture long-term dependencies.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=17SK-eyFNUdg9WPToZt6QEq_XwkO8p5Y9\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "When leveraging NLP kind of tasks, an encoder-decoder architecture is used:\n",
        "\n",
        "* The encoder transforms the input sequence into a fixed-length context vector summarizing the input.\n",
        "* The decoder then generates the output sequence using this context vector to initialize its hidden state.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1qAr639VTfxTSXy16W_llYxS7SsVPXImv\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "RNNs problems:\n",
        "\n",
        "\n",
        "1. Slow computation for long sequences\n",
        "2. Vanishing or exploding gradients\n",
        "3. Difficulty in accessing information from long time ago1. Slow computation for long sequences\n",
        "\n",
        "Solution?\n",
        "\n",
        "### Attention is all you need\n",
        "\n",
        "\n",
        "Imagine you're looking for a book in a library based on a brief description.\n",
        "The sentence you hear is: âItâs about a young wizard â¡ð§ââï¸ð who attends a magical school.â\n",
        "You donât focus on every word equally â your mind zooms in on âwizardâ and âmagical school ð© ð°â. These are the keywords that trigger your memory and help you immediately think of Harry Potterð¦ð¬\n",
        "\n",
        "The Attention mechanism is based on a common-sensical intuition that we âattend toâ a certain part when processing a large amount of information.\n",
        "\n",
        "Note: The Attention mechanism enables the transformers to have extremely long-term memory. A transformer model can âattendâ or âfocusâ on all previous tokens that have been generated.\n",
        "\n",
        "\n",
        "In order to understand the entire sentence, we will learn to correlate and attend to certain words based on the context of the entire sentence.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1sa9lVMIA6y6H-hvZwFSoc6B3TxQ_a6yd\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "For instance, in the first sentence in the image above, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "We can build better models by developing mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualising what it knows about some parts of the input based on other parts. In the following sections, we will explore the mechanisms that enable us to train deep learning models to attend to input data in the context of other input data.\n"
      ],
      "metadata": {
        "id": "lmEs_xbtJa-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Machines Learn to Pay Attention (Like We Do)\n",
        "\n",
        "One way to implement attention in neural networks is by representing each word (or even parts of a word) as a vector.\n",
        "\n",
        "So, whatâs a vector? A vector is simply an array of numbers (called real-valued numbers) that can have different lengths. Think of it like a list of values that describe certain properties of a word. These vectors allow us to measure how similar two words are to each other. One common way to measure this similarity is by calculating something called the dot product.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1mReprFfL9ezlIRh55Co0yzX3EjiwcHsf\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "To illustrate how the dot product can create meaningful attention weights, we'll use pre-trained word2vec embeddings. These word2vec embeddings are generated by a neural network that learned to create similar embeddings for words with similar meanings.\n",
        "\n",
        "By calculating the matrix of dot products between all vectors, we get an attention matrix. This will indicate which words are correlated and therefore should \"attend\" to each other."
      ],
      "metadata": {
        "id": "xH9KdF08Lhg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    \"\"\"\n",
        "    Calculate the dot product between the hidden states and previous states.\n",
        "\n",
        "    Args:\n",
        "        hidden_states: A tensor with shape [T_hidden, dm]\n",
        "        previous_state: A tensor with shape [T_previous, dm]\n",
        "    \"\"\"\n",
        "\n",
        "    # Hint: To calculate the attention scores, think about how you can use the `previous_state` vector\n",
        "    # and the `hidden_states` matrix. You want to find out how much each element in `previous_state`\n",
        "    # should \"pay attention\" to each element in `hidden_states`. Remember that in matrix multiplication,\n",
        "    # you can find the relationship between two sets of vectors by multiplying one by the transpose of the other.\n",
        "    # Hint: Use `jnp.matmul` to perform the matrix multiplication between `previous_state` and the\n",
        "    # transpose of `hidden_states` (`hidden_states.T`).\n",
        "    scores = ...  # FINISH ME\n",
        "\n",
        "    # Hint: Now that you have the scores, you need to convert them into probabilities.\n",
        "    # A softmax function is typically used in attention mechanisms to turn raw scores into probabilities\n",
        "    # that sum to 1. This will help in determining how much focus should be placed on each hidden state.\n",
        "    # Hint: Use `jax.nn.softmax` to apply the softmax function to `scores`.\n",
        "    w_n = ...  # FINISH ME\n",
        "\n",
        "    # Multiply the weights by the hidden states to get the context vector\n",
        "    # Hint: Use `jnp.matmul` again to multiply the attention weights `w_n` by `hidden_states`\n",
        "    # to get the context vector.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t"
      ],
      "metadata": {
        "id": "v-xcLmAFLl6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Run me to test your code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "\n",
        "try:\n",
        "  w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "  w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "  c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "  assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
        "  assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
        "\n",
        "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
        "except:\n",
        "  print(\"It looks like the function isn't fully implemented yet. Try modifying it.\")"
      ],
      "metadata": {
        "id": "mFDyXq77Ll8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# when changing these words, note that if the word is not in the original\n",
        "# training corpus it will not be shown in the weight matrix plot.\n",
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    # Calculate the attention scores:\n",
        "    # Multiply the previous state vector by the transpose of the hidden states matrix.\n",
        "    # This gives us a matrix of scores that show how much attention each element in the previous state\n",
        "    # should pay to each element in the hidden states.\n",
        "    # The result is a matrix of shape [T, N], where:\n",
        "    # T is the number of elements in the hidden states,\n",
        "    # N is the number of elements in the previous state.\n",
        "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "\n",
        "    # Apply the softmax function to the scores to convert them into probabilities.\n",
        "    # This normalizes the scores so that they sum up to 1 for each element,\n",
        "    # allowing us to interpret them as how much attention should be given to each hidden state.\n",
        "    w_n = jax.nn.softmax(scores)\n",
        "\n",
        "    # Calculate the context vector (c_t):\n",
        "    # Multiply the attention weights (w_n) by the hidden states.\n",
        "    # This combines the hidden states based on how much attention each one deserves,\n",
        "    # resulting in a new vector that represents the weighted sum of the hidden states.\n",
        "    # The resulting shape is [T, d], where:\n",
        "    # T is the number of elements in the previous state,\n",
        "    # d is the dimension of the hidden states.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    # Return the attention weights and the context vector.\n",
        "    return w_n, c_t"
      ],
      "metadata": {
        "id": "mqBReNvBLl_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ],
      "metadata": {
        "id": "vRPNhlqrLmCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the matrix, we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.\n",
        "\n",
        "Note: Dot product is only one of the ways to implement the scoring function for attention mechanisms, there is a more extensive list in this [blog post](https://lilianweng.github.io/posts/2018-06-24-attention/) by Dr Lilian Weng."
      ],
      "metadata": {
        "id": "sQNRqDqELyTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Between Self Attention & Multi Head attention\n",
        "\n",
        "Self-attention and multi-head attention (MHA) are fundamental components of the transformer architecture. In this section, we'll thoroughly explain the intuition behind these concepts and their implementation. Later, in the Transformers section, you'll learn how these attention mechanisms are used to create a sequence-to-sequence model that relies entirely on attention.\n",
        "\n",
        "As we move forward, we'll represent sentences by breaking them down into individual words and encoding each word using the word2vec model discussed earlier. In the Transformers section, we'll explore in more detail how input sequences are transformed into a series of vectors."
      ],
      "metadata": {
        "id": "_qJdLHPBL1I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Embed a sentence using word2vec; for example use cases only.\n",
        "    \"\"\"\n",
        "    # clean sentence (not necessary if using a proper LLM tokenizer)\n",
        "    sentence = remove_punctuation(sentence)\n",
        "\n",
        "    # extract individual words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # get the word2vec embedding for each word in the sentence\n",
        "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
        "\n",
        "    # return with extra dimension (useful for creating batches later)\n",
        "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
      ],
      "metadata": {
        "id": "SfEhWwBaLmFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parallel, allowing longer sequences to be utilised.\n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "* Queries, keys and values\n",
        "* Scaled dot product attention\n",
        "* Masks\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1VwPK-JVOe_NyY4QwKcaCxp4YGpxVIu1u\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "TUPfggF9L9tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "\n",
        "    # define the method for weight initialisation\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # initialise three linear layers to do the QKV transformations.\n",
        "    # note: this can also be one layer, how do you think you would do it?\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    # transform and return the matrices\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ],
      "metadata": {
        "id": "_bUZBG5uLmHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what's the query, Key and values?\n",
        "\n",
        "ð¯ Real-Life Analogy:\n",
        "Imagine you're in a library, trying to find the most relevant facts for a question.\n",
        "\n",
        "* You have a question in mind: thatâs your query.\n",
        "\n",
        "* Each book in the library has a title or description: that's its key.\n",
        "\n",
        "* Inside each book is the actual content: thatâs the value.\n",
        "\n",
        "In self-attention, every word in a sentence plays all three roles:\n",
        "\n",
        "* It creates a query: \"What am I looking for?\"\n",
        "\n",
        "* It presents a key: \"What information do I contain?\"\n",
        "\n",
        "* It offers a value: \"Hereâs what I can contribute.\"\n",
        "\n",
        "\n",
        "In general:\n",
        "* Self-Attention is permutation invariant.\n",
        "\n",
        "* Self-Attention requires no parameters. Up to now the interaction between words\n",
        "has been driven by their embedding and the positional encodings.\n",
        "\n",
        "* We expect values along the diagonal (of the Matrix) to be the highest.\n",
        "\n",
        "* If we donât want some positions to interact, we can always set their values to ââ\n",
        "\n",
        "Conclusion: Self-Attention allows the model to relate words to each other."
      ],
      "metadata": {
        "id": "ksZYdi6nMGUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled dot product attention\n",
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in all attention mechanisms; we must first find a score for each vector in the sequence and then use these scores to create a new context vector. In self-attention scoring is done using scaled dot product attention, and then the normalised scores are used as weights to sum the value vectors and create the context vector.\n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "where the attention scores are calculated by $\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)$ and the scores are then multiplied by $V$ to get the context vector.\n",
        "\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. We then multiply $V$ by each weight and finally sum all weighted vectors $v_{weighted}$ together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training."
      ],
      "metadata": {
        "id": "ldE3HlzrMI0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    \"\"\"\n",
        "    Formula to return scaled dot product attention given QKV matrices\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "\n",
        "    # get the raw scores (logits) from dot producting the queries and keys\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "\n",
        "    # scale the raw scores and apply the softmax function to get the attention scores/weights\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # multiply the weights by the value matrix to get the output\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "_b5cyg7ALmKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers we would need to train these. Instead, we are going to make things simple and use $X=Q=V=K$."
      ],
      "metadata": {
        "id": "XFaCf5ipMNoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a sentence\n",
        "sentence = \"I drink coke, but eat steak\"\n",
        "\n",
        "# embed and create QKV matrices\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "outputs, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# plot the words and the attention weights between them\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ],
      "metadata": {
        "id": "_E_oxD2GLmM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\".\n",
        "\n",
        "More resources:\n",
        "\n",
        "[Attention with Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ],
      "metadata": {
        "id": "cz_DkHbBMf4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Attention\n",
        "\n",
        "Masked attention is a technique used in attention mechanismsâespecially in Transformersâto prevent a model from \"looking ahead\" when it shouldn't.\n",
        "\n",
        "ð Intuition: Like Reading Without Spoilers\n",
        "Imagine you're reading a mystery novel one chapter at a time. You want to guess who the culprit is without skipping ahead to the end. Masked attention works the same way:\n",
        "\n",
        "'At position t, the model is only allowed to attend to tokens at or before time t, not after.'\n",
        "\n",
        "\n",
        "ðµï¸ââï¸ Why Use Masked Attention?\n",
        "\n",
        "ð§± Padding in Uneven-Length Sequences\n",
        "When batching sequences (e.g., sentences or time series) of different lengths, we usually pad the shorter ones so all sequences have the same length. But those padding tokens are just placeholdersâthey carry no real information.\n",
        "\n",
        "â If we donât mask them, the model might treat padding as meaningful content, which can confuse learning.\n",
        "\n",
        "\n",
        "2. ð Preventing Look-Ahead in Decoder Models\n",
        "In models that generate sequences (like GPT), we train them using the full output sentence at once. But during actual generation, the model should only see past and present tokensânot the future ones.\n",
        "\n",
        "ð§  Imagine writing a story one word at a time. You shouldn't be allowed to read ahead before writing the next word!\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>.\n",
        "\n"
      ],
      "metadata": {
        "id": "yJ4lTjELMj68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "# the mask makes sure that positions only attend to previous positions in the input (causal mask)\n",
        "# we will use this later to insert -inf values into the raw scores\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "\n",
        "# plot\n",
        "sns.heatmap(mask, cmap=\"Blues\")\n",
        "plt.title(\"Example of mask that can be applied\");"
      ],
      "metadata": {
        "id": "YMdbuQLSMlqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now adapt our scaled dot product attention function to implement masked attention."
      ],
      "metadata": {
        "id": "Y_7Wpv28Mp-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled dot product attention with a causal mask (only allowed to attend to previous positions)\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "\n",
        "    # get scaled logits using dot product as before\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    # add optional mask where values along the mask are set to -inf\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    # calcualte the attention weights via softmax\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # sum with the values to get the output\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "_T9qSjMmMpOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The beast with many heads: Multi-Head attention\n",
        "\n",
        "\n",
        "We talked about self attention mechanisim in the last section, How does the multi-head attention relate to the self-attention mechanism (scaled-dot product attention)?\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1e0C2tC29XylPRVfwXo_-NLzisQbLIdxl\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n",
        "Multi-head self-attention is a variant of self-attention used in the Transformer model. It involves running multiple sets of attention computations in parallel, each focusing on different relationships and aspects of the input sequence.\n",
        "rather than only computing the attention once, the MHA mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, Attention is All You Need, \"multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\"\n",
        "\n",
        "Multi-head attention can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which is why multiple heads of attention also work.\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1ulHkifKMzFSHl7-pJnUpc5VP-H2FssED\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "X31b1Pt6MvJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or more precisely something like this: A stack of scaled dot product attention\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1lfMZAgs6bR5_0blSB95SAPuX1TNpNaCC\" alt=\"Positional Encoding Vectors\" width=\"500\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>\n",
        "\n"
      ],
      "metadata": {
        "id": "xnOdT_r1M2U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at how to implement multi-head attention. In simple terms, multi-head attention is like running the attention process multiple times in parallel, using different copies of the Q, K, and V matrices for each \"head.\" This helps the model focus on different parts of the input at the same time. If you're interested in learning more, check out [this blog by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) for a detailed explanation."
      ],
      "metadata": {
        "id": "npyhGNKRM9CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int  # Number of attention heads\n",
        "    d_m: int  # Dimension of the model's embeddings\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the sequence-to-QKV transformation module\n",
        "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "\n",
        "        # Define the initializer for the output linear layer weights\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
        "        )\n",
        "\n",
        "        # Initialize the output projection layer Wo (used after attention)\n",
        "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "        # If Q, K, or V are not provided, use the input X to generate them\n",
        "        if None in [Q, K, V]:\n",
        "            assert not X is None, \"X has to be provided if either Q, K, or V are not provided\"\n",
        "\n",
        "            # Generate Q, K, and V matrices from the input X\n",
        "            Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "        # Extract the batch size (B), sequence length (T), and embedding size (d_m)\n",
        "        B, T, d_m = K.shape\n",
        "\n",
        "        # Calculate the size of each attention head's embedding (d_m / num_heads)\n",
        "        head_size = d_m // self.num_heads\n",
        "\n",
        "        # Reshape Q, K, V to have separate dimensions for the heads\n",
        "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
        "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention to each head\n",
        "        attention, attention_weights = scaled_dot_product_attention(\n",
        "            q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "        # Reshape the attention output back to its original dimensions\n",
        "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
        "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
        "\n",
        "        # Apply the output linear transformation Wo to the attention output\n",
        "        X_new = self.Wo(attention)\n",
        "\n",
        "        # If return_weights is True, return both the transformed output and attention weights\n",
        "        if return_weights:\n",
        "            return X_new, attention_weights\n",
        "        else:\n",
        "            # Otherwise, return just the transformed output\n",
        "            return X_new"
      ],
      "metadata": {
        "id": "kurPdKRQMuL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What to keep in mind:\n",
        "\n",
        "ð Attention:\n",
        "\n",
        "* Captures long-range dependencies.\n",
        "\n",
        "* Enables parallelization (unlike RNNs).\n",
        "\n",
        "* Attention is position-agnostic unless combined with positional encoding.\n",
        "\n",
        "\n",
        "ð¡ Masked attention :\n",
        "\n",
        "* Causal mask (look-ahead): Ensures autoregressive behavior (e.g., token t sees only tokens â¤ t).\n",
        "\n",
        "* Padding mask: Prevents attention to meaningless padded positions.\n",
        "\n",
        "* Implemented by masking attention logits before softmax using -inf.\n",
        "\n",
        "â¨ Multi Head Attention\n",
        "\n",
        "* Donât just look one wayâlook at multiple patterns at once\n",
        "\n",
        "* Helps capture multiple types of dependencies (e.g., syntax, semantics)."
      ],
      "metadata": {
        "id": "nF3tNzT_NGIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ð§± Tokenization (15 minutes)\n",
        "\n",
        "ð¥ **Assigned to**: Teju\n",
        "\n",
        "### Objective\n",
        "* Walk away with knowledge of how your language is tokenized and what cost implications that has\n",
        "* Code example of tokenization [Beginner]\n",
        "  * Load a tokenizer and tokenize input sentence\n",
        "\n",
        "* Tokenizer Playground [Beginner]\n",
        "  * https://huggingface.co/spaces/Xenova/the-tokenizer-playground  \n",
        "  * https://platform.openai.com/tokenizer\n",
        "\n",
        "* Learners can be asked to input the same sentence in their languages e.g.  \n",
        "  âI am attending Indaba at Kigaliâ using the same tokenizer and observe the different token lengths + cost\n",
        "\n",
        "* https://github.com/openai/tiktoken/pull/314  \n",
        "* Language Model Tokenizers Introduce Unfairness Between Languages â https://arxiv.org/pdf/2305.15425  \n",
        "* Add link to Karpathy's tokenization video"
      ],
      "metadata": {
        "id": "AtWMaTddww65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Have you ever thought about how a language model reads your text?\n",
        "\n",
        "Language models don't \"read\" text the way we doâas strings of words forming coherent sentences. Instead, they break down the texts into smaller units called *tokens*. These tokens are then converted into numbers (token ID) that the model actually uses during training or inference. The process of breaking text into smaller component is called *tokenization*.\n",
        "\n",
        "A token can be:\n",
        "\n",
        "* A single character (`i`, `n`, `d`, `a`, `b`)\n",
        "* A subword (`ind`, `aba`)\n",
        "* A whole word (`indaba`)\n",
        "\n",
        "Different modelsâlike GPT, Gemma, LLaMA, Mistral, and othersâuse different tokenizers, and each makes its own decisions about how to break text into tokens. The most common tokenization method used in LLMs is **Byte Pair Encoding (BPE)**. If youâre curious about how it works, this [excellent video](https://www.youtube.com/watch?v=zduSFxRajkE) explains really well.\n",
        "\n",
        "> The key idea behind tokenization is **granularity**âhow small should a model break text down in order to understand and predict what comes next? The goal is to find a balance: break text into small enough pieces that the model can generalize well, but not so small that it explodes the number of tokens. A good tokenizer keeps the vocabulary compact, handles diverse languages efficiently, and compresses text well so that fewer tokens are needed to represent meaningâespecially across multilingual inputs."
      ],
      "metadata": {
        "id": "WO9OWhQYMti4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ð¯ Try it Yourself:** Tokenizer Playground\n",
        "\n",
        "Let's make this real. Visit any of these:\n",
        "\n",
        "* [Tiktokenizer Playground (GPT-2)](https://tiktokenizer.vercel.app/?model=gpt2)\n",
        "* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
        "\n",
        "Paste the sentence:\n",
        "`Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.`\n",
        "\n",
        "ð¯ Now try the same sentence in another language you speakâYorÃ¹bÃ¡, Kiswahili, French, etc. Write down what you noticed.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1XpIVAOk281R7i13IMYQHe0HZZG6tUrjw\" alt=\"TikTokenizer\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "_Ku_PEI0PF4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You probably noticed that:\n",
        "\n",
        "* The number of tokens changes.\n",
        "* Some languages require *more* tokens to say the same thing."
      ],
      "metadata": {
        "id": "tqsL-jLVPXdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How much does it cost when you chat with an LLM in your language?\n",
        "\n",
        "Depending on how many tokens your message gets broken into, the cost of interacting with a language model can vary. This is because LLMs charge per token, not per word, sentence, or character.\n",
        "\n",
        "For example:\n",
        "\n",
        "* **GPT-4.0-turbo** (as of June 2025) [costs](https://openai.com/api/pricing/) **\\$2 per 1 million tokens** for input.\n",
        "* **Gemma 2.5 Pro** (via Gemini API as of June 2025) [costs](https://ai.google.dev/gemini-api/docs/pricing) **\\$1.25 per 1 million tokens**.\n",
        "\n",
        "Since different models use different tokenization approaches, they break down languages differently and that can lead to higher costs for some languages than others, even when the sentence means exactly the same thing.\n",
        "\n",
        "\n",
        "### Let's calculate token cost\n",
        "\n",
        "If the cost is **\\$2 per 1 million tokens**, hereâs how the cost scales:\n",
        "\n",
        "$$\n",
        "\\text{Cost} = \\text{Token Count} \\times \\left(\\frac{2}{1{,}000{,}000}\\right)\n",
        "$$\n",
        "\n",
        "#### ð° Example Estimates:\n",
        "\n",
        "| Token Count   | Calculation                               | Cost (USD)    |\n",
        "| ------------- | ----------------------------------------- | ------------- |\n",
        "| 10 tokens     | $10 \\times \\frac{2}{1{,}000{,}000}$       | **\\$0.00002** |\n",
        "| 100 tokens    | $100 \\times \\frac{2}{1{,}000{,}000}$      | **\\$0.0002**  |\n",
        "| 1,000 tokens  | $1000 \\times \\frac{2}{1{,}000{,}000}$     | **\\$0.002**   |\n",
        "| 10,000 tokens | $10{,}000 \\times \\frac{2}{1{,}000{,}000}$ | **\\$0.02**    |\n",
        "\n",
        "Now imagine you're generating or processing millions of requests in a local language that happens to tokenize inefficiently. That could mean spending more to say the same thing simply because your language doesn't work well with the tokenizer that the language model uses.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1m0mCSEEuBxNzb8pJfMANqsvikRorBubE\" alt=\"ChatGPT Pricing\" width=\"800\"/>\n",
        "  <figcaption><em></em></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "tixtBEtRPZ5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ð¸ How Much Does My Language Cost? â Tokenization in Code \\[Intermediate]\n",
        "\n",
        "**Setup:**\n",
        "We'll test how different models tokenize the same sentence across languages, using:\n",
        "\n",
        "* **GPT-2** (general English-language tokenizer)\n",
        "* **Gemma** (multilingual tokenizer)\n",
        "* A **language-specific tokenizer**\n",
        "\n",
        "Let's start with **GPT-2**.\n"
      ],
      "metadata": {
        "id": "QVTduxk4PdYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma import gm"
      ],
      "metadata": {
        "id": "7nlX9HI8Penu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "def get_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Function that takes in a model name and returns the tokenizer for that model.\n",
        "    \"\"\"\n",
        "    if model_name == \"gemma3\":\n",
        "        tokenizer = gm.text.Gemma3Tokenizer()\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def tokenize(text: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Function that takes in a string and a tokenizer and returns the tokenized version of the string.\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer(model_name)\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    tokens = [tokenizer.decode(t) for t in token_ids]\n",
        "    if model_name != \"gemma3\":\n",
        "        tokens = [token.replace('Ä ', ' ') for token in tokens] # Replace the 'Ä ' prefix used by some tokenizers with a space\n",
        "    return tokens, token_ids\n",
        "\n",
        "def token_cost(tokens: list, model_name: str):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of tokens and returns the token cost for the given model.\n",
        "    \"\"\"\n",
        "    # cost per token for Gemma 2.5 Pro https://ai.google.dev/gemini-api/docs/pricing\n",
        "    # for now, assume all tokenizer cost the same\n",
        "    cost_per_token = 2/1000000  # cost per token for GPT4.1\n",
        "\n",
        "    return len(tokens) * cost_per_token  # Gemma3 uses a fixed cost per token"
      ],
      "metadata": {
        "id": "GHHdtb9cPex4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"  #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "tokenizer = get_tokenizer(model_name)  # Default tokenizer, can be changed as needed\n",
        "sentence = \"This is a sample sentence for tokenization.\" #@param {type:\"string\"}\n",
        "tokens, token_ids = tokenize(sentence, model_name)\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens:\",  tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "print(\"Token count:\", len(tokens))\n",
        "print(\"Token cost:\", token_cost(tokens, model_name))"
      ],
      "metadata": {
        "id": "5wO5Y7_NPmqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next,\n",
        "- Try a the same sentence in a different language for example swahili or yoruba\n",
        "- Observe and record the number of tokens"
      ],
      "metadata": {
        "id": "e-Svrwz-PsEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = {\"English\": \"Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.\",\n",
        "             \"German\": \"Willkommen zum Indaba LLM Tutorial in Kigali. Macht euch bereit die Welt von LLMs zu erkunden.\",\n",
        "             \"French\": \"Bienvenue au tutoriel Indaba sur les LLM qui se dÃ©roule Ã  Kigali. PrÃ©parez-vous Ã  explorer le monde des LLM.\",\n",
        "             \"Lithuania\": \"Sveiki atvykÄ Ä¯ Indaba LLM (DidÅ¾iÅ³jÅ³ KalbiniÅ³ ModeliÅ³) mokymus vykstanÄius Kigalyje. PasiruoÅ¡kite tyrinÄti LLM pasaulÄ¯.\",\n",
        "             \"Yoruba\": \"Kaabá» si ikáº¹ká» Indaba LLM ti n á¹£áº¹láº¹ ni Kigali. á¹¢etan lati á¹£awari agbaye ti LLMs.\",\n",
        "             \"Swahili\": \"Karibu kwenye mafunzo ya Indaba LLM yanayofanyika Kigali. Jitayarishe kuchunguza ulimwengu wa LLM.\",\n",
        "             \"Arabic\":  \".ÙØ±Ø­Ø¨ÙØ§ Ø¨ÙÙ ÙÙ ÙØ±Ø´Ø© Ø¹ÙÙ  Ø­ÙÙ Ø§ÙÙÙØ§Ø°Ø¬ Ø§ÙÙØºÙÙØ© Ø§ÙÙØ¨ÙØ±Ø© Ø§ÙØªÙ ØªÙÙØ§Ù ÙÙ ÙÙØºØ§ÙÙ. Ø§Ø³ØªØ¹Ø¯ÙØ§ ÙØ§Ø³ØªÙØ´Ø§Ù Ø¹Ø§ÙÙ Ø§ÙÙÙØ§Ø°Ø¬ Ø§ÙÙØºÙÙØ© Ø§ÙÙØ¨ÙØ±Ø©\",\n",
        "             \"Kinyarwanda\": \"Murakaza neza mu isomo rya Indaba LLM riri kubera i Kigali. Mwitegure kuvumbura isi ya za LLM.\"\n",
        "            }"
      ],
      "metadata": {
        "id": "O5qcyoRWPrBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"   #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "for language, sentence in sentences.items():\n",
        "    # For each language, tokenize the sentence and print the results\n",
        "    tokens, token_ids = tokenize(sentence, model_name)\n",
        "    print(f\"Language: {language}, Model: {model_name}\")\n",
        "    print(\"-\" * 50)  # Separator for clarity\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Token IDs:\", token_ids)\n",
        "    print(\"Token count:\", len(tokens))\n",
        "    print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
        "    print(\"-\" * 50)  # Separator for clarity"
      ],
      "metadata": {
        "id": "EhPM1MyDPw8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice how GPT-2 breaks down Arabic text character by character, often byte-by-byte, instead of capturing meaningful units? Thatâs because GPT-2 was trained primarily on English data and wasnât optimized to handle Arabic. While you can still tokenize Arabic using the GPT-2 tokenizer, it typically results in a much higher token count compared to the same content in Englishâwhich also means higher cost.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wD2dDhQ2P2cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâs try the [Gemma 3](https://developers.googleblog.com/en/introducing-gemma3/#:~:text=Gemma%203%20uses%20a%20new,TPUs%20using%20the%20JAX%20Framework.) tokenizer. It features a new multilingual tokenizer designed to provide support for 140+ languages.\n"
      ],
      "metadata": {
        "id": "alop4gEjP48e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gemma3\"\n",
        "for language, sentence in sentences.items():\n",
        "    # For each language, tokenize the sentence and print the results\n",
        "    tokens, token_ids = tokenize(sentence, model_name)\n",
        "    print(f\"Language: {language}, Model: {model_name}\")\n",
        "    print(\"-\" * 50)  # Separator for clarity\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Token IDs:\", token_ids)\n",
        "    print(\"Token count:\", len(tokens))\n",
        "    print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
        "    print(\"-\" * 50)  # Separator for clarity"
      ],
      "metadata": {
        "id": "ajJMBSYkP09h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about using a language-specific tokenizer? For example, try `asafaya/bert-base-arabic` on Arabic textâit's designed to handle the structure and nuances of the language much better than general-purpose tokenizers. Notice how the token countâand therefore the costâdrops significantly when you use a tokenizer that's specifically tailored for Arabic?"
      ],
      "metadata": {
        "id": "z8EekZSfP_Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"asafaya/bert-base-arabic\"\n",
        "language = \"Arabic\"\n",
        "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
        "tokens, token_ids = tokenize(sentence, model_name)\n",
        "print(f\"Language: {language}, Model: {model_name}\")\n",
        "print(\"-\" * 50)  # Separator for clarity\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "print(\"Token count:\", len(tokens))\n",
        "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
        "print(\"-\" * 50)  # Separator for clarity\n",
        "\n",
        "\n",
        "model_name = \"gemma3\"\n",
        "language = \"Arabic\"\n",
        "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
        "tokens, token_ids = tokenize(sentence, model_name)\n",
        "print(f\"Language: {language}, Model: {model_name}\")\n",
        "print(\"-\" * 50)  # Separator for clarity\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "print(\"Token count:\", len(tokens))\n",
        "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
        "print(\"-\" * 50)  # Separator for clarity\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "language = \"Arabic\"\n",
        "sentence = sentences[language]  # Get the Arabic sentence from the dictionary\n",
        "tokens, token_ids = tokenize(sentence, model_name)\n",
        "print(f\"Language: {language}, Model: {model_name}\")\n",
        "print(\"-\" * 50)  # Separator for clarity\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "print(\"Token count:\", len(tokens))\n",
        "print(\"Token cost (scaled by 1000000):\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
        "print(\"-\" * 50)  # Separator for clarity\n"
      ],
      "metadata": {
        "id": "LjzJYNccP9uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models you want to compare\n",
        "models = [\"asafaya/bert-base-arabic\", \"gemma3\", \"gpt2\"]\n",
        "costs = []\n",
        "language = \"Arabic\"  # Language to use for the cost comparison\n",
        "# Calculate the token cost for each model using the sentences dictionary\n",
        "for model in models:\n",
        "    total_cost = 0\n",
        "    tokens, _ = tokenize(sentences[language], model)\n",
        "    total_cost = token_cost(tokens, model)\n",
        "    costs.append(total_cost * 1000000)  # Scale by 1 million for display\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.bar(models, costs, color='k')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Scaled Cost (USD)')\n",
        "plt.title(f'Cost Comparison of Models for {language} Language')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qH-jn4JvQGJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ð§µ Key Takeaway\n",
        "\n",
        "* **Be aware of how LLMs represent your language**, especially if you're using commercial APIs. The way your text is tokenized directly affects cost.\n",
        "* If you're training your own LLM, **pay close attention to tokenization**. You might want to **adapt the tokenizer to your language** to reduce token count and make representation more compact and efficient.\n",
        "* Recent work by **Cohere** explores building a [**universal tokenizer**](https://arxiv.org/pdf/2506.10766) that works well across multiple languages. This kind of research is trying to level the playing field.\n",
        "* Also check out:\n",
        "\n",
        "  * ð *[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://aclanthology.org/2023.emnlp-main.614.pdf)*\n",
        "  * ð *[Language Model Tokenizers Introduce Unfairness Between Languages](https://arxiv.org/pdf/2305.154255)*\n",
        "\n",
        "These studies highlighted early on that tokenizers introduced **structural unfairness**, especially for low-resource languages. Because of this, several commercial LLM providers have since started training **more representative tokenizers** to reduce token cost disparities across languages.\n",
        "\n",
        "Bottom line: **Tokenization isn't just a technical detail, itâs a language access issue.**\n",
        "\n",
        "\n",
        "**Drawback of using raw token**:\n",
        "\n",
        "One drawback of using raw tokens is that they lack any indication of the word's position in the sequence. This is evident when considering sentences like \"I am happy\" and \"Am I happy\" - these two phrases have distinct meanings, and the model needs to grasp the word order to understand the intended message accurately.\n",
        "\n",
        "To address this, when converting the inputs into vectors, position vectors are introduced and added to these vectors to indicate the **position** of each word.\n"
      ],
      "metadata": {
        "id": "WwCo9941QRo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðï¸ Training your own LLM (Transformers) (30 minutes)\n",
        "\n",
        "ð¥ **Assigned to**: Jabez\n",
        "\n",
        "### Objective\n",
        "\n",
        "* Load a dataset and train an LLM  \n",
        "* Visualise positional encodings [Beginner]  \n",
        "* Implement:\n",
        "  * Positional encodings  \n",
        "  * FFN block  \n",
        "  * Layer norm  \n",
        "  * Decoder block  \n",
        "  * Full LLM [Intermediate/Advanced]  \n",
        "* Define loss function  \n",
        "* Load the training dataset  \n",
        "* Write training script  \n",
        "* Run inference on the trained model [Beginner]\n"
      ],
      "metadata": {
        "id": "5X4tRtSZxGHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2 Positional encodings"
      ],
      "metadata": {
        "id": "DFIaERwORCym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://kazemnejad.com/img/transformer_architecture_positional_encoding/model_arc.jpg\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "By now, you should have a solid understanding of how attention works, as well as how words are tokenized and mapped into embeddings. These embeddings are vectors of numbers that represent each token.\n",
        "\n",
        "However, it's important to understand one key limitation of self-attention mechanisms: it is **permutation invariant**. This means that, by default, attention has no way of knowing the position of each word in a sentence. As we know, word order is crucial in language modeling. To illustrate this, consider the following two sentences:\n",
        "\n",
        "- \"I have to read this book\"  \n",
        "- \"I have this book to read\"\n",
        "\n",
        "Both sentences contain the exact same words (*look closely*), but their meanings are clearly different. This demonstrates that the position of words affects meaning. Since both the encoder and decoder blocks process all tokens in parallel, the original order of tokens would be lost without additional information. To give the model a sense of order, we add a small \"position vector\" to each word's embedding. These additional vectors are known as **positional encodings**. With positional encodings, each token carries information about both its meaning and its position in the sentence before the attention mechanism is applied.\n",
        "\n",
        "An example of how positional encodings modify token embeddings is shown below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png\" alt=\"Positional Encoding Vectors\" width=\"650\"/>\n",
        "  <figcaption><em>To give the model a sense of word order, we add positional encoding vectors which follow a specific pattern. Source: https://jalammar.github.io/illustrated-transformer/ </em></figcaption>\n",
        "</figure>\n",
        "\n",
        "If we assume that the tokens get mapped to an embedding with a dimension D of 4, the positional encodings could look something like this:\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" alt=\"Positional Encoding Vectors\" width=\"650\"/>\n",
        "  <figcaption><em>An example of positional encoding added to embedding dimension size of 4. Source: https://jalammar.github.io/illustrated-transformer/ </em></figcaption>\n",
        "</figure>\n",
        "\n",
        "Ideally, we want these positional encodings to contain certain desirable properties. ([Reference 1](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/), [Reference 2](https://huggingface.co/blog/designing-positional-encoding)):\n",
        "1. **Each position has a unique and consistent encoding**  \n",
        "   Every position in a sequence needs a distinct encoding vector. Importantly, this encoding should remain the same regardless of the overall length of the sequence. For example, position 5 should have the *same* encoding whether the input sequence has 10 tokens or 10,000.\n",
        "\n",
        "2. **Linear relationships between positions**  \n",
        "   The difference between encodings should reflect the distance between positions. Think of a number line: if position 3 is two steps from 1, and position 6 is three steps from 3, the encoding should preserve those kinds of relationships. This helps the model reason about relative positions in a sentence.\n",
        "\n",
        "3. **Generalises to unseen (longer) sequences**  \n",
        "   The encoding scheme should still work when the model is given longer sequences than it was trained on. In other words, it should not âbreakâ when encountering inputs longer than expected.\n",
        "\n",
        "4. **Deterministic, not random**  \n",
        "   The encodings must come from a fixed, repeatable rule (not randomness).  \n",
        "   This helps the model learn the pattern of the encoding and use it effectively."
      ],
      "metadata": {
        "id": "nUb1lKukQtWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Sine and cosine functions**"
      ],
      "metadata": {
        "id": "nxkDif_aRGKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To meet the desirable properties discussed above, the authors of [*Attention is All You Need*](https://arxiv.org/pdf/1706.03762) propose a simple technique for **positional encoding**. This method injects information about token order into embeddings by applying a combination of sine and cosine functions at varying frequencies.\n",
        "\n",
        "The position encoding for a given position `pos`, at embedding dimension index `i`, with total embedding size `d_model`, is defined as:\n",
        "\n",
        "$$\n",
        "PE_{\\text{pos}, i} =\n",
        "\\begin{cases}\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{i / d_{\\text{model}}}}\\right), & \\text{if } i \\bmod 2 = 0 \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{(i - 1) / d_{\\text{model}}}}\\right), & \\text{if } i \\bmod 2 = 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Assuming a model with embedding size \\( d_{\\text{model}} = 8 \\), the positional encoding vector for position `pos` becomes:\n",
        "\n",
        "$$\n",
        "PE_{\\text{pos}} =\n",
        "\\begin{bmatrix}\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
        "\\sin\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right) \\\\\n",
        "\\cos\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "> **Note:** Even indices use sine, and odd indices use cosine. The division by powers of 10000 ensures that each dimension encodes a different frequency.\n",
        "\n",
        "---\n",
        "\n",
        "To understand why these encodings work in practice, let's create a function to visualize them and play around with the `token_sequence_length` and the `token_embedding` dimension.\n"
      ],
      "metadata": {
        "id": "WJIyboPkRG-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions * frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P\n",
        "\n",
        "token_sequence_length = 50 # @param {type: \"number\"}\n",
        "token_embedding = 768  # @param {type: \"number\"}\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ],
      "metadata": {
        "id": "lQVRmEJKRLMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how every row in the graph, corresponding to a specific token position displays a distinct wave pattern across the embedding dimensions. This means each position has a fixed and unique encoding, which allows the model to distinguish tokens based on their position in the sequence. The encodings donât change between runs; theyâre entirely determined by the formula.\n",
        "\n",
        "### ð¤ **Group Activity**:\n",
        "\n",
        "- <font color='blue'>Take a moment with your friend to explore why this specific pattern appears when `token_sequence_length` is set to 1000, and `token_embedding` is 768.</font>\n",
        "- <font color='blue'>Experiment with smaller values for `token_sequence_length` and `token_embedding` to build a deeper understanding and enhance your discussion.</font>\n",
        "- <font color='blue'>Curious about the constant 10000? Ask your friend why they think itâs used in the functions above.</font>\n",
        "- <font color='blue'>Now, try setting `token_sequence_length` to 50 and `token_embedding` to a much larger value, like 10000. What do you observe? Do we always need a large token embedding?</font>\n"
      ],
      "metadata": {
        "id": "pSxfPQhoRHBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Transformer block   <font color='green'>Intermediate</font>"
      ],
      "metadata": {
        "id": "e71jR6TYRHEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like an MLP (a simple neural network that processes input data through multiple layers) or a CNN (a type of neural network that excels at recognizing patterns in images by using convolution layers), transformers are made up of a stack of transformer blocks. In this section, we'll build each of the components needed to create one of these transformer blocks."
      ],
      "metadata": {
        "id": "4Kobmi8IRHHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.3.1 Feed Forward Network (FFN) / Multilayer perceptron (MLP) <font color='orange'>Beginner</font>\n",
        "\n",
        "\n",
        "<div style=\"display: flex; align-items: center; justify-content: center; gap: 40px;\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1gyHqjfJUg_BLoFhAH6_KqsKxOQWvYtvD\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"Drawing\" width=\"260\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "5yAG_MbgRWEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original model, these blocks consist of a simple 2-layer MLP (Multi-Layer Perceptron) that uses ReLU activation. However, GeLU (Gaussian Error Linear Unit) has become very popular, and we will be using it throughout this practical. The formula below represents the feedforward neural network (FFN) with GeLU activation. In this network, the input `x` is first passed through two linear layers with weights `W1` and `W2`, followed by bias terms `b1` and `b2`. The ReLU activation function, often represented by the `max` function, is replaced by the GeLU activation function in this case.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the multi-head attention block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized."
      ],
      "metadata": {
        "id": "bv5FUmp0RYd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code implementation for a feed forward neural network (Run me!)\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A 2-layer MLP which widens then narrows the input.\n",
        "\n",
        "    Args:\n",
        "      widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "    \"\"\"\n",
        "    # widening_factor controls how much the input dimension is expanded in the first layer.\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    # init_scale controls the scaling factor for weight initialization.\n",
        "    init_scale: float = 0.25\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # Get the size of the last dimension of the input (embedding size).\n",
        "        d_m = x.shape[-1]\n",
        "\n",
        "        # Calculate the size of the first layer by multiplying the embedding size by the widening factor.\n",
        "        layer1_size = self.widening_factor * d_m\n",
        "\n",
        "        # Initialize the weights for both layers using a variance scaling initializer.\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "        )\n",
        "\n",
        "        # Define the first dense layer, which expands the input size.\n",
        "        layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "\n",
        "        # Define the second dense layer, which reduces the size back to the original dimension.\n",
        "        layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "        # Apply the first dense layer followed by a GELU activation function.\n",
        "        x = jax.nn.gelu(layer1(x))\n",
        "\n",
        "        # Apply the second dense layer to project the data back to its original dimension.\n",
        "        x = layer2(x)\n",
        "\n",
        "        # Return the final output.\n",
        "        return x"
      ],
      "metadata": {
        "id": "t8mvJkVdRdZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Add and Norm block <font color='orange'>Beginner</font>\n",
        "\n",
        "<div style=\"display: flex; align-items: center; justify-content: center; gap: 40px;\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1lj8pqO6ttjbcTRUEW1rbtRlueiLPxoSr\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
        "  <img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" alt=\"Drawing\" width=\"400\"/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "J2Us0NGFRUPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers.\n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-\\mu_{i}}{\\sigma_i^2 + Ïµ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $Î³$, and `bias` $Î²$, with:\n",
        "\n",
        "$y_i = Î³\\hat{x}_i + Î² = LN_{Î³,Î²}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block.\n",
        "\n",
        "To implement the Add & Norm block, we define a Flax module that takes in the original and processed inputs, adds them together, and then applies flax.linen.LayerNorm across the last dimension to normalize the result. This helps stabilize training by standardizing the summed representation."
      ],
      "metadata": {
        "id": "s-VmAFgbRkb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code implementation for Add and Norm block (Run me!)\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "    \"\"\"A block that implements the 'Add and Norm' operation\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, processed_x):\n",
        "        # Step 1: Add the original input (x) to the processed input (processed_x).\n",
        "        added = x + processed_x\n",
        "\n",
        "        # Step 2: Apply layer normalization to the result of the addition.\n",
        "        # - LayerNorm helps to stabilize and improve the training process by normalizing the output.\n",
        "        # - reduction_axes=-1 indicates that normalization is applied across the last dimension (typically the embedding dimension).\n",
        "        # - use_scale=True and use_bias=True allow the layer to learn scaling and bias parameters for further fine-tuning.\n",
        "        normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "\n",
        "        # Return the normalized result.\n",
        "        return normalised(added)"
      ],
      "metadata": {
        "id": "KbbBqhEORmKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Building the Transformer Decoder / LLM <font color='green'>Intermediate</font>"
      ],
      "metadata": {
        "id": "i0Z_7oRRRqPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "Most of the groundwork has happened. We have built the positional encoding block, the MHA block, the feed-forward block and the add&norm block.\n",
        "\n",
        "The only part needed is passing inputs to each decoder block and applying the masked MHA block found in the decoder blocks.\n",
        "\n",
        "**Code task:** Code up a FLAX Module that implements the (FFN(norm(MHA(norm(X))))) for the decoder block"
      ],
      "metadata": {
        "id": "j2xafT-jRtct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Decoder Block Implementation\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block.\n",
        "\n",
        "    Args:\n",
        "        num_heads: The number of attention heads in the Multi-Head Attention (MHA) block.\n",
        "        d_m: The size of the token embeddings.\n",
        "        widening_factor: The factor by which the hidden layer size is expanded in the MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    num_heads: int\n",
        "    d_m: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the Multi-Head Attention (MHA) block\n",
        "        self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "\n",
        "        # Initialize the AddNorm blocks for residual connections and normalization\n",
        "        self.add_norm1 = AddNorm()  # First AddNorm block after MHA\n",
        "        self.add_norm2 = AddNorm()  # Second AddNorm block after the MLP\n",
        "\n",
        "        # Initialize the FeedForwardBlock (MLP) which processes the data after attention\n",
        "        self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weight=True):\n",
        "        \"\"\"\n",
        "        Forward pass through the DecoderBlock.\n",
        "\n",
        "        Args:\n",
        "            X: Batch of input tokens fed into the decoder, shape [B, T_decoder, d_m]\n",
        "            mask [optional, default=None]: Mask to control which positions the attention is allowed to consider, shape [T_decoder, T_decoder].\n",
        "            return_att_weight [optional, default=True]: If True, returns the attention weights along with the output.\n",
        "\n",
        "        Returns:\n",
        "            If return_att_weight is True, returns a tuple (X, attention_weights_1).\n",
        "            Otherwise, returns the processed token representations X.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply Multi-Head Attention to the input tokens (X) with optional masking\n",
        "        attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "        # Apply the first AddNorm block (adds the original input X and normalizes)\n",
        "        X = self.add_norm1(X, attention)\n",
        "\n",
        "        # Pass the result through the FeedForwardBlock (MLP) to further process the data\n",
        "        projection = self.MLP(X)\n",
        "\n",
        "        # Apply the second AddNorm block (adds the input from the previous step and normalizes)\n",
        "        X = self.add_norm2(X, projection)\n",
        "\n",
        "        # Return the final output X, and optionally the attention weights\n",
        "        return (X, attention_weights_1) if return_att_weight else X\n"
      ],
      "metadata": {
        "id": "6JR03oeHRrZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we just put everything together, adding in the positional encodings as well as stacking multiple transformer blocks and adding our prediction layer."
      ],
      "metadata": {
        "id": "uwyJ7wfvR3iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer model consisting of several layers of decoder blocks.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Number of attention heads in each Multi-Head Attention (MHA) block.\n",
        "        num_layers: Number of decoder blocks in the model.\n",
        "        d_m: Dimensionality of the token embeddings.\n",
        "        vocab_size: Size of the vocabulary (number of unique tokens).\n",
        "        widening_factor: Factor by which the hidden layer size is expanded in the MLP.\n",
        "    \"\"\"\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    d_m: int\n",
        "    vocab_size: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize a list of decoder blocks, one for each layer in the model\n",
        "        self.blocks = [\n",
        "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialize an embedding layer to convert token IDs into token embeddings\n",
        "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
        "\n",
        "        # Initialize a dense layer for predicting the next token in the sequence\n",
        "        self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the LLM model.\n",
        "\n",
        "        Args:\n",
        "            X: Batch of input token IDs, shape [B, T_decoder] where B is batch size and T_decoder is sequence length.\n",
        "            mask [optional, default=None]: Mask to control which positions the attention can focus on, shape [T_decoder, T_decoder].\n",
        "            return_att_weights [optional, default=False]: Whether to return the attention weights.\n",
        "\n",
        "        Returns:\n",
        "            logits: The predicted probabilities for each token in the vocabulary.\n",
        "            If return_att_weights is True, also returns the attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert token IDs to embeddings (shape [B, T_decoder, d_m])\n",
        "        X = self.embedding(X)\n",
        "\n",
        "        # Get the sequence length of the input\n",
        "        sequence_len = X.shape[-2]\n",
        "\n",
        "        # Generate positional encodings and add them to the token embeddings\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "\n",
        "        # Initialize a list to store attention weights if needed\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "\n",
        "        # Pass the embeddings through each decoder block in sequence\n",
        "        for block in self.blocks:\n",
        "            out = block(X, mask, return_att_weights)\n",
        "            if return_att_weights:\n",
        "                # If returning attention weights, unpack the output\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                # Otherwise, just update the input for the next block\n",
        "                X = out\n",
        "\n",
        "        # Apply a dense layer followed by a log softmax to get logits (predicted token probabilities)\n",
        "        logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "        # Return the logits, and optionally, the attention weights\n",
        "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
      ],
      "metadata": {
        "id": "mOjE40uJR5sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ],
      "metadata": {
        "id": "iAhAuP_wR9ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model dimensions and input shapes\n",
        "batch_size = 18\n",
        "sequence_length = 32\n",
        "embedding_dim = 16\n",
        "num_decoder_layers = 8\n",
        "vocab_size = 25670\n",
        "\n",
        "# Initialize the language model\n",
        "llm = LLM(\n",
        "    num_heads=1,\n",
        "    num_layers=1,\n",
        "    d_m=embedding_dim,\n",
        "    vocab_size=vocab_size,\n",
        "    widening_factor=4\n",
        ")\n",
        "\n",
        "# Create a lower-triangular attention mask for causal decoding\n",
        "causal_mask = jnp.tril(np.ones((sequence_length, sequence_length)))\n",
        "\n",
        "# Generate random input token IDs\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "input_token_ids = jax.random.randint(rng_key, [batch_size, sequence_length], 0, vocab_size)\n",
        "\n",
        "# Initialize model parameters\n",
        "model_params = llm.init(rng_key, input_token_ids, mask=causal_mask)\n",
        "\n",
        "# Run the model and extract logits and attention weights from the decoder\n",
        "logits, decoder_attention_weights = llm.apply(\n",
        "    model_params,\n",
        "    input_token_ids,\n",
        "    mask=causal_mask,\n",
        "    return_att_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "YvnKc43XR_c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final sanity check, we can confirm that our attention weights are working correctly. As shown in the figure below, the decoder's attention weights only focus on previous tokens, as expected."
      ],
      "metadata": {
        "id": "N8vowzz-SC2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "plt.suptitle(\"LLM attention weights\")\n",
        "sns.heatmap(decoder_attention_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5NbvRSNzSF0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Training your LLM"
      ],
      "metadata": {
        "id": "7nsFaXhdSKZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.1 Training objective <font color='green'>Intermediate</font>\n"
      ],
      "metadata": {
        "id": "o6BUm34sSRJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sentence is nothing but a string of words. A LLM aims to predict the next word by considering the current context, namely the words that have come before.\n",
        "\n",
        "Here's the basic idea:\n",
        "\n",
        "To calculate the probability of a full sentence \"word1, word2, ..., last word\" appearing in a given context $c$, the procedure is to break down the sentence into individual words and consider the probability of each word given the words that precede it. These individual probabilities are then multiplied together:\n",
        "\n",
        "$$\\text{Probability of sentence} = \\text{Probability of word1} \\times \\text{Probability of word2} \\times \\ldots \\times \\text{Probability of last word}$$\n",
        "\n",
        "This method is akin to building up a narrative one piece at a time based on the preceding storyline.\n",
        "\n",
        "Mathematically, this is expressed as the likelihood (probability) of a sequence of words $y_1, y_2, ..., y_n$ in a given context $c$, which is achieved by multiplying the probabilities of each word $y_t$ calculated given the predecessors ($y_{<t}$) and the context $c$:\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} P\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "Here $y_{<t}$ stands for the sequence $y_1, y_2, ..., y_{t-1}$, while $c$ represents the context.\n",
        "\n",
        "This is analogous to solving a jigsaw puzzle where the next piece is predictively placed based on what's already in place.\n",
        "\n",
        "Remember just when training a transformer, we do not work in words, but in tokens. During the training process, the model's parameters are fine-tuned by computing the cross-entropy loss across the predicted token, and the correct token, and then performing backpropagation. The loss for time step \"t\" is computed as:\n",
        "\n",
        "$$ \\text{Loss}_t = - \\sum_{w \\in V} y_t\\log (\\hat{y}_t) $$\n",
        "\n",
        "Here $y_t$ is the actual token at time step $t$, and $\\hat{y}_t$ is the token predicted by the model at the same time step. The loss for the entire sentence is then computed as:\n",
        "\n",
        "$$ \\text{Sentence Loss} = \\frac{1}{n} \\sum^{n}_{t=1} \\text{Loss}_t $$\n",
        "\n",
        "where $n$ is the length of the sequence.\n",
        "\n",
        "This iterative process ultimately hones the model's predictive capabilities over time.\n",
        "\n",
        "**Code task**: Implement the cross-entropy loss function below."
      ],
      "metadata": {
        "id": "7jJ2TTwjSV-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Compute the cross-entropy loss between predicted token ID and true ID.\n",
        "\n",
        "  Args:\n",
        "    logits: An array of shape [batch_size, sequence_length, vocab_size]\n",
        "    targets: The targets we are trying to predict\n",
        "\n",
        "  Returns:\n",
        "    loss: A scalar value representing the mean batch loss\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "\n",
        "  # Hint: Compute the cross-entropy loss by first applying `jax.nn.log_softmax(logits)`\n",
        "  # to get the log probabilities for each class. Then, multiply these log probabilities\n",
        "  # by the `target_labels` to focus on the correct class's probability. Sum this result\n",
        "  # along the last axis to get the loss for each token. Finally, apply the mask to the loss,\n",
        "  # sum the masked losses, and normalize by the number of non-padding tokens.\n",
        "  loss = ...# FINISH ME\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "jeAWLPJbSVLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ],
      "metadata": {
        "id": "Rwp5yuhrSdgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def sequence_loss_fn(logits, targets):\n",
        "    \"\"\"Compute the sequence loss between predicted logits and target labels.\"\"\"\n",
        "\n",
        "    # Convert the target indices to one-hot encoded vectors.\n",
        "    # Each target label is converted into a one-hot vector of size VOCAB_SIZE.\n",
        "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "\n",
        "    # Ensure that the shape of logits matches the shape of the one-hot encoded targets.\n",
        "    # This is important because we need to compute the loss across matching dimensions.\n",
        "    assert logits.shape == target_labels.shape\n",
        "\n",
        "    # Create a mask that ignores padding tokens in the loss calculation.\n",
        "    # The mask is True (1) where the target value is greater than 0 and False (0) otherwise.\n",
        "    mask = jnp.greater(targets, 0)\n",
        "\n",
        "    # Compute the cross-entropy loss for each token.\n",
        "    # Cross-entropy is calculated as the negative log probability of the correct class.\n",
        "    # jax.nn.log_softmax(logits) gives us the log probabilities for each class.\n",
        "    # We multiply by the target_labels to select the log probability of the correct class.\n",
        "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "    # Apply the mask to the loss to ignore padding positions and sum up the losses.\n",
        "    # We then normalize the total loss by the number of non-padding tokens.\n",
        "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "wIPqzOq1Sg9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.2 Training models <font color='blue'>Advanced</font>"
      ],
      "metadata": {
        "id": "7Jp_1cbQSnzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section, we define all the processes required to train the model using the objective described above. A lot of this is now the work required to do training using FLAX.\n",
        "\n",
        "Below we gather the dataset and we shall be training on, which is Karpathy's shakespeare dataset. Its not so important to understand this code, so either just run the cell to load the data, or view the code if you want to understand it.\n"
      ],
      "metadata": {
        "id": "nWx1nrZZSmZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Shakespeare dataset and iterator (optional, but run the cell)\n",
        "\n",
        "# Trick to avoid errors when downloading tinyshakespeare.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into words\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from words to unique IDs\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to words\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convert the words in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ],
      "metadata": {
        "id": "2MyUYK9DSqbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now look how our data is structured for training"
      ],
      "metadata": {
        "id": "eJf97wcmSv2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size"
      ],
      "metadata": {
        "id": "o3UEb_1hSzPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let us train our LLM and see how it performs in producing Shakespearian text. First, we will define what happens for every training step."
      ],
      "metadata": {
        "id": "wVKnUqggS5ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "    \"\"\"\n",
        "    Perform a single training step.\n",
        "\n",
        "    Args:\n",
        "        params: The current parameters of the model.\n",
        "        optimizer_state: The current state of the optimizer.\n",
        "        batch: A dictionary containing the input data and target labels for the batch.\n",
        "        apply_fn: The function used to apply the model to the inputs.\n",
        "        update_fn: The function used to update the model parameters based on the gradients.\n",
        "\n",
        "    Returns:\n",
        "        Updated parameters, updated optimizer state, and the computed loss for the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Get the sequence length (T) from the input data.\n",
        "        T = batch['input'].shape[1]\n",
        "\n",
        "        # Apply the model to the input data, using a lower triangular mask to enforce causality.\n",
        "        # jnp.tril(np.ones((T, T))) creates a lower triangular matrix of ones.\n",
        "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "\n",
        "        # Calculate the loss between the predicted logits and the target labels.\n",
        "        loss = sequence_loss_fn(logits, batch['target'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Compute the loss and its gradients with respect to the parameters.\n",
        "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "    # Update the optimizer state and calculate the parameter updates based on the gradients.\n",
        "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "\n",
        "    # Apply the updates to the parameters.\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Return the updated parameters, optimizer state, and the loss for the batch.\n",
        "    return params, optimizer_state, loss"
      ],
      "metadata": {
        "id": "PMNCtFxxS91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we initialise our optimizer and model. Feel free to play with the hyperparameters during the practical."
      ],
      "metadata": {
        "id": "12uUhzgcTRBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all hyperparameters\n",
        "d_model = 128            # Dimension of token embeddings (d_m)\n",
        "num_heads = 4            # Number of attention heads in Multi-Head Attention\n",
        "num_layers = 1           # Number of decoder blocks in the model\n",
        "widening_factor = 2      # Factor to widen the hidden layer size in the MLP\n",
        "LR = 2e-3                # Learning rate for the optimizer\n",
        "batch_size = 32          # Number of samples per training batch\n",
        "seq_length = 64          # Length of each input sequence (number of tokens)\n",
        "\n",
        "# Set up the training data\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size  # Get the size of the vocabulary from the dataset\n",
        "batch = next(train_dataset)            # Get the first batch of input data\n",
        "\n",
        "# Set the random number generator key for model initialization\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# Initialize the LLM model with the specified hyperparameters\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "\n",
        "# Create a causal mask to ensure that the model only attends to previous tokens\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "\n",
        "# Initialize the model parameters using the first batch of input data and the mask\n",
        "params = llm.init(rng, batch['input'], mask)\n",
        "\n",
        "# Set up the optimizer using the Adam optimization algorithm with the specified learning rate\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)  # Initialize the optimizer state with the model parameters"
      ],
      "metadata": {
        "id": "1NJjGy-qTSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train! This will take a few minutes.. While it trains, have you greeted your neighbour yet?"
      ],
      "metadata": {
        "id": "QdwUHcn6TZQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Training loop\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ],
      "metadata": {
        "id": "Vlk7laQVTYNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5.3 Inspecting the trained LLM <font color='orange'>Beginner</font>\n"
      ],
      "metadata": {
        "id": "qE5N87UWT_uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reminder:** remember to run all code presented so far in this section before runnning the cells below!\n",
        "\n",
        "Lets generate some text now and see how our model did. DO NOT STOP THE CELL ONCE IT IS RUNNING, THIS WILL CHRASH THE SESSION."
      ],
      "metadata": {
        "id": "MGBOSUgdUE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)\n",
        "\n",
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"Love\"\n",
        "    print(prompt, end=\"\")\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
        "      prediction = generate_prediction(params, input, llm.apply)\n",
        "      prediction = id_2_word[int(prediction)]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ],
      "metadata": {
        "id": "p4XVDaTHUICU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we implemented everything above by taking the token ID with the maximum probability of being correct. This is greedy decoding, as we only took the most likely token. It worked well in this use case, but there are cases where we will see a degrading performance when taking this greedy approach, specifically when we are interested in generating realistic text.\n",
        "\n",
        "Other methods exist for sampling from the decoder, with a famous algorithm being beam search. We provide resources below for anyone interested in learning more about this.\n",
        "\n",
        "[Greedy Decoding](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Beam Search](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)"
      ],
      "metadata": {
        "id": "k_7rSlfaUOeD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "# **Conclusion**\n",
        "**Summary:**\n",
        "\n",
        "You've now mastered the essentials of how a Large Language Model (LLM) works, from the fundamentals of attention mechanisms to training your own LLM! These powerful tools have the potential to transform a wide range of tasks. However, like any deep learning model, their magic lies in applying them to the right problems with the right data.\n",
        "\n",
        "Ready to take your skills to the next level? Dive into fine-tuning your own LLMs and unleash even more potential! I highly recommend exploring last year's practical on Parameter Efficient Fine-Tuning Methods for a comprehensive overview of advanced techniques. The journey doesn't stop hereâthere's so much more to discover! [LLMs for Everyone 2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "The world of LLMs is yours to exploreâgo ahead and create something amazing! ðð\n",
        "\n",
        "---\n",
        "\n",
        "**Next Steps:**\n",
        "[**Efficiently Finetuning LLMs with Hugging Face**](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "\n",
        "**References:** for further references check the links referenced throughout\n",
        "specific sections of this colab.\n",
        "\n",
        "* [Attention is all you need paper](https://arxiv.org/abs/1706.03762)\n",
        "* [Additional videos on transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
        "* [LoRA paper](https://arxiv.org/abs/2106.09685)\n",
        "* [RLHF](https://huggingface.co/blog/rlhf) (how ChatGPT was trained)\n",
        "* [Extending context length](https://kaiokendev.github.io/context):\n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1liTp04PTXeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arcUSo_sSc-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "6EqhIg1odqg0",
        "_tADD6ZfEX-u"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}