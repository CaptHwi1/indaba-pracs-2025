{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2s4kN_QPQVe"
   },
   "source": [
    "# Pratique LLM Indaba 2025\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/take-memories/images/f_auto,dpr_auto,q_auto,w_2000,c_fill,h_1200/gm/hbb8oblj5tozmimydbaz/rwanda-sehenswurdigkeiten\" width=\"60%\"/>\n",
    "\n",
    "¬© Deep Learning Indaba 2025. Licence Apache 2.0.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2025/blob/main/practicals/LLMs/Part_1/LLM_Indaba_Practical_French_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Colab\"/></a>\n",
    "\n",
    "**Auteurs :** Tejumade Afonja, Qurat Ul Ain (Annie), Jabez Magomere, Abla Hagani, Amel Sellami, Massimo Nicosia, Sebastian Bodenstein.\n",
    "\n",
    "**Relecteurs :** Ulrich A. Mbou Sob, Siddarth Singh, Sasha Abramowitz, Ruan de Kock.\n",
    "\n",
    "**Traducteurs** Amel Sellami, Abla Hagani.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Les grands mod√®les de langage (LLMs) comme ChatGPT et Gemini ont r√©volutionn√© le domaine du traitement automatique du langage naturel, mais comprendre leur fonctionnement interne peut √™tre complexe. Ce cours pratique est con√ßu pour d√©mystifier les concepts cl√©s derri√®re ces mod√®les, en commen√ßant par l‚Äôid√©e fondamentale de l‚Äôattention, puis en explorant l‚Äôarchitecture qui alimente les syst√®mes les plus avanc√©s d‚Äôaujourd‚Äôhui. Au fil du parcours, vous acquerrez des connaissances pratiques sur la mani√®re dont la langue et la tokenisation influencent le comportement et les co√ªts des mod√®les, et vous entra√Ænerez m√™me votre propre petit mod√®le de langage (SLM).\n",
    "\n",
    "Tout au long de cette pratique, nous couvrirons les domaines cl√©s suivants :\n",
    "\n",
    "1. **Chargement et interaction avec les LLMs :** Exp√©rimentez directement avec des mod√®les pr√©-entra√Æn√©s de Hugging Face et comprenez comment g√©n√©rer du texte et contr√¥ler la sortie via diff√©rents param√®tres.\n",
    "\n",
    "2. **Explorer les applications concr√®tes :** D√©couvrez comment les LLMs sont utilis√©s dans diverses t√¢ches telles que la g√©n√©ration de code, la r√©ponse aux questions, l‚Äô√©criture cr√©ative, et la r√©ponse aux questions visuelles.\n",
    "3. **Architecture Transformer :** Plongez dans les √©l√©ments constitutifs des LLMs modernes, incluant un rappel rapide de l‚Äôarchitecture Transformer et de ses composants cl√©s comme l‚Äôattention automatique (self-attention) et l‚Äôattention multi-t√™tes (multi-head attention).\n",
    "4. **Tokenisation et embeddings :** Apprenez comment le texte est converti en repr√©sentations num√©riques compr√©hensibles par les LLMs, et explorez l‚Äôimpact des diff√©rentes strat√©gies de tokenisation selon les langues.\n",
    "5. **Entra√Ænement de votre propre LLM :** Impl√©mentez et entra√Ænez un mod√®le d√©codeur Transformer simplifi√© √† partir de z√©ro en utilisant un jeu de donn√©es des ≈ìuvres de Shakespeare.\n",
    "\n",
    "6. **Le co√ªt de la langue :** Analysez comment la tokenisation peut affecter le co√ªt d‚Äôutilisation des LLMs commerciaux, particuli√®rement selon les diff√©rentes langues.\n",
    "\n",
    "Cette pratique est adapt√©e aux personnes ayant un niveau d√©butant √† interm√©diaire en apprentissage profond et en traitement du langage naturel. Nous recommandons d‚Äôavoir une compr√©hension de base en alg√®bre lin√©aire.\n",
    "\n",
    "Commen√ßons !\n",
    "\n",
    "**Sujets :**\n",
    "\n",
    "Contenu : \\[<font color='orange'>Introduction √† Hugging Face</font>, <font color='orange'>Interagir avec les LLMs</font>, <font color='orange'>Tokenisation</font>, <font color='orange'>Embeddings</font>, <font color='orange'>Architecture Transformer</font>, <font color='green'>M√©canisme d‚ÄôAttention</font>, <font color='green'>Entra√Æner votre propre LLM depuis z√©ro</font>]\n",
    "\n",
    "Niveau : <font color='orange'>D√©butant</font>, <font color='green'>Interm√©diaire</font>\n",
    "\n",
    "**Objectifs d‚Äôapprentissage :**\n",
    "\n",
    "* Comprendre le concept derri√®re [l‚Äôattention](https://arxiv.org/abs/1706.03762) et pourquoi elle est utilis√©e.\n",
    "* Pr√©senter et d√©crire les blocs fondamentaux de l‚Äô[architecture Transformer](https://arxiv.org/abs/1706.03762) ainsi qu‚Äôune intuition sur la conception de cette architecture.\n",
    "* Comparer les tokenizers √† travers diff√©rentes langues et analyser comment ces diff√©rences influencent les co√ªts mon√©taires associ√©s.\n",
    "* Construire et entra√Æner votre propre LLM.\n",
    "\n",
    "**Pr√©requis :**\n",
    "\n",
    "* Connaissances de base en apprentissage profond.\n",
    "* Familiarit√© avec le traitement du langage naturel (NLP).\n",
    "* Compr√©hension de base en alg√®bre lin√©aire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM-X0-PoZOAB"
   },
   "source": [
    "**Plan:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "7_PaafMKZFHb"
   },
   "source": [
    "# [Pratique LLM Indaba 2025](#scrollTo=m2s4kN_QPQVe)\n",
    "\n",
    ">>[Installation et configuration [D√©butant]](#scrollTo=Zgn0dW3IH9NQ)\n",
    "\n",
    ">>>[[Lancez-moi] Installations, Imports et Fonctions utilitaires](#scrollTo=6EqhIg1odqg0)\n",
    "\n",
    ">>[ü§ñ Charger un mod√®le depuis Hugging Face et interagir localement [D√©butant]](#scrollTo=ElXJ1BGzH5BJ)\n",
    "\n",
    ">>>[üéØ Objectif](#scrollTo=8Lrb9vL2uO5A)\n",
    "\n",
    ">>>>>[üß† Qu'est-ce que les grands mod√®les de langage ?](#scrollTo=me-nBsxtGBA1)\n",
    "\n",
    ">>>>[√Ä propos de HuggingFace](#scrollTo=ltfz9jstGSTJ)\n",
    "\n",
    ">>>>>[Votre premier mod√®le de langage](#scrollTo=M1e-FfGzGrr_)\n",
    "\n",
    ">>>>[Comprendre les param√®tres de g√©n√©ration](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>[Temp√©rature](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>[Top-p (√âchantillonnage par noyau)](#scrollTo=5nflrXCXHY6u)\n",
    "\n",
    ">>>>[Mod√®les de langage dans des applications r√©elles](#scrollTo=J5U3YbK_IGel)\n",
    "\n",
    ">>[üîç R√©capitulatif rapide de l‚Äôarchitecture Transformer [D√©butant]](#scrollTo=svfeQO7VIOIu)\n",
    "\n",
    ">>>[Vue d'ensemble du d√©codeur Transformer](#scrollTo=brwKNHT0-Hhl)\n",
    "\n",
    ">>[üß± Tokenisation [D√©butant]](#scrollTo=AtWMaTddww65)\n",
    "\n",
    ">>>[üéØ Essayez par vous-m√™me : Terrain de jeu du tokenizer](#scrollTo=_Ku_PEI0PF4l)\n",
    "\n",
    ">>>[Jouez avec le tokenizer Gemma](#scrollTo=p6qNp4IhGP-K)\n",
    "\n",
    ">>[ìä≥ Embeddings [D√©butant]](#scrollTo=9YPYutB1TAes)\n",
    "\n",
    ">>[‚ïë Codages positionnels : pourquoi l‚Äôordre compte [D√©butant]](#scrollTo=WNO703V9SBcI)\n",
    "\n",
    ">>>>>[Fonctions sinus et cosinus : une fa√ßon simple d‚Äôajouter des informations de position](#scrollTo=nxkDif_aRGKy)\n",
    "\n",
    ">>[Les embeddings informent l‚Äôattention [D√©butant]](#scrollTo=Qkh0KgRPdDf8)\n",
    "\n",
    ">>[üîç Attention [Interm√©diaire]](#scrollTo=vY02IFQouwjN)\n",
    "\n",
    ">>>[Entre self-attention et multi-head attention](#scrollTo=_qJdLHPBL1I8)\n",
    "\n",
    ">>>[Self-Attention](#scrollTo=TUPfggF9L9tE)\n",
    "\n",
    ">>>[Attention masqu√©e](#scrollTo=yJ4lTjELMj68)\n",
    "\n",
    ">>>[La b√™te aux multiples t√™tes : Multi-Head Attention](#scrollTo=X31b1Pt6MvJ8)\n",
    "\n",
    ">>>[Attention par produit scalaire mis √† l‚Äô√©chelle](#scrollTo=WN0q3iq9SMdn)\n",
    "\n",
    ">>[√Ä garder √† l‚Äôesprit :](#scrollTo=nF3tNzT_NGIm)\n",
    "\n",
    ">>[üèóÔ∏è Entra√Ænement de votre propre LLM (Transformers) [Interm√©diaire]](#scrollTo=5X4tRtSZxGHg)\n",
    "\n",
    ">>>[Objectif](#scrollTo=5X4tRtSZxGHg)\n",
    "\n",
    ">>>[Bloc Transformer [Interm√©diaire]](#scrollTo=e71jR6TYRHEP)\n",
    "\n",
    ">>>>[R√©seau Feed Forward (FFN) / Perceptron multicouche (MLP) [D√©butant]](#scrollTo=5yAG_MbgRWEs)\n",
    "\n",
    ">>>>[Bloc Add & Norm [D√©butant]](#scrollTo=J2Us0NGFRUPn)\n",
    "\n",
    ">>>[Construction du d√©codeur Transformer / LLM [Interm√©diaire]](#scrollTo=i0Z_7oRRRqPg)\n",
    "\n",
    ">>>[Entra√Ænement de votre LLM](#scrollTo=7nsFaXhdSKZG)\n",
    "\n",
    ">>>>[Objectif d‚Äôentra√Ænement [Interm√©diaire]](#scrollTo=o6BUm34sSRJH)\n",
    "\n",
    ">>>>[Mod√®les d‚Äôentra√Ænement [Interm√©diaire]](#scrollTo=7Jp_1cbQSnzq)\n",
    "\n",
    ">>>>[Inspection du LLM entra√Æn√© [D√©butant]](#scrollTo=qE5N87UWT_uK)\n",
    "\n",
    ">>[√Ä m√©diter : combien co√ªte une conversation avec un LLM dans votre langue ?](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>[Calculons le co√ªt des tokens](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>>[üí∞ Estimations d‚Äôexemple :](#scrollTo=tixtBEtRPZ5n)\n",
    "\n",
    ">>>>[üí∏ Combien co√ªte ma langue ? ‚Äî Tokenisation en code](#scrollTo=QVTduxk4PdYC)\n",
    "\n",
    ">>>>[üßµ Points cl√©s](#scrollTo=WwCo9941QRo2)\n",
    "\n",
    ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
    "\n",
    ">[Retour d‚Äôexp√©rience](#scrollTo=o1ndpYE50BpG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOJzzH88ZJL6"
   },
   "source": [
    "**Avant** de commencer:\n",
    "\n",
    "Pour cela, vous devrez utiliser un GPU pour acc√©l√©rer l'entra√Ænement.Pour ce faire, acc√©dez au menu \"Runtime\" dans Colab, s√©lectionnez \"Modifier le type d'ex√©cution\", puis dans le menu contextuel, choisissez \"GPU\" dans la case \"Accelerator mat√©riel\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "952qogb79nnY"
   },
   "source": [
    "***Niveau** d'exp√©rience sugg√©r√© dans ce sujet:\n",
    "\n",
    "| Level         | Experience                            |\n",
    "| --- | --- |\n",
    "`D√©butant`      | C‚Äôest la premi√®re fois que je d√©couvre ce domaine. |\n",
    "`Interm√©diaire` | J‚Äôai suivi quelques cours ou introductions de base sur ce sujet. |\n",
    "`Avanc√©`        | Je travaille dans ce domaine/sujet au quotidien. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgn0dW3IH9NQ"
   },
   "source": [
    "## Installation et configuration [<font color = 'orange'> d√©butant </font>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YBdDHcI_ArCR"
   },
   "outputs": [],
   "source": [
    "# @title **Chemins √† suivre¬†:** Quel est votre niveau d'exp√©rience sur les sujets pr√©sent√©s dans ce carnet¬†? (Ex√©cuter Cell)\n",
    "experience = 'beginner' #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "sections_to_follow=''\n",
    "\n",
    "\n",
    "if experience == 'beginner': sections_to_follow = '''Nous vous recommandons de ne pas essayer de r√©aliser toutes les t√¢ches de codage, mais plut√¥t de parcourir chaque section et de vous assurer d‚Äôex√©cuter chaque cellule afin d‚Äôacqu√©rir une compr√©hension pratique du comportement de ces mod√®les.'''\n",
    "\n",
    "elif experience == 'intermediate': sections_to_follow = '''\n",
    "Nous vous recommandons de parcourir chaque section de ce notebook et d‚Äôessayer les t√¢ches de codage marqu√©es comme d√©butant ou interm√©diaire. Si vous bloquez sur le code, demandez de l‚Äôaide √† un tuteur ou passez √† une meilleure utilisation du temps de la pratique.\n",
    "'''\n",
    "\n",
    "elif experience == 'advanced': sections_to_follow = '''Nous vous recommandons de parcourir chaque section et d‚Äôessayer chaque t√¢che de codage jusqu‚Äô√† ce que vous l‚Äôobteniez √† fonctionner.'''\n",
    "\n",
    "\n",
    "print(f'D‚Äôapr√®s votre exp√©rience, {sections_to_follow}.\\n Note : ceci est juste une recommandation, n‚Äôh√©sitez pas √† explorer le colab comme bon vous semble si vous vous sentez √† l‚Äôaise !')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EqhIg1odqg0"
   },
   "source": [
    "### [Ex√©cutez-moi] Installations, importations et fonctions d'assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwvNaDj1VrPp"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "required_version = (3, 11)\n",
    "current_version = sys.version_info[:2]\n",
    "\n",
    "if current_version != required_version:\n",
    "    print(f\"‚ö†Ô∏è Warning: Expected Python {required_version[0]}.{required_version[1]}, but running {current_version[0]}.{current_version[1]}. Some package may not work as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIYFviZCPd0w"
   },
   "outputs": [],
   "source": [
    "# Utilisez ¬´¬†uv pip install¬†¬ª pour exploiter le t√©l√©chargement/cache parall√®le d‚Äôuv et r√©aliser des installations beaucoup plus rapides\n",
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUmZvGi-ozwR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output  # pour effacer la sortie de la cellule une fois termin√©\n",
    "\n",
    "# Group√©s par fonctionnalit√© :\n",
    "#  ‚Ä¢ Seaborn & UMAP             ‚Üí Trac√© de graphiques et r√©duction de la dimensionnalit√©\n",
    "#  ‚Ä¢ LiveLossPlot               ‚Üí Visualisation des m√©triques d'entra√Ænement en temps r√©el\n",
    "#  ‚Ä¢ Accelerate & PEFT          ‚Üí Acc√©l√©ration mat√©rielle et ajustement fin (fine-tuning) efficace en param√®tres\n",
    "#  ‚Ä¢ gensim, nltk               ‚Üí Mod√©lisation th√©matique et utilitaires NLP\n",
    "#  ‚Ä¢ torchvision                ‚Üí Ensembles de donn√©es et transformations pour la vision par ordinateur\n",
    "#  ‚Ä¢ ipywidgets                 ‚Üí Widgets de notebook interactifs\n",
    "#  ‚Ä¢ ipdb                       ‚Üí D√©bogueur interactif\n",
    "#  ‚Ä¢ colorama                   ‚Üí Formatage de la sortie de la console avec des couleurs\n",
    "#  ‚Ä¢ clear_output               ‚Üí Efface la sortie de la cellule du notebook une fois l'installation termin√©e\n",
    "#  ‚Ä¢ Transformers & Datasets    ‚Üí Biblioth√®ques de base pour le NLP\n",
    "#  ‚Ä¢ Gemma==3                   ‚Üí Bundle de tokenizers & mod√®les (√©pingl√© √† la v3 pour la compatibilit√©)\n",
    "\n",
    "!uv pip install  \\\n",
    "    seaborn \\\n",
    "    umap-learn \\\n",
    "    livelossplot \\\n",
    "    accelerate \\\n",
    "    peft \\\n",
    "    # gensim \\\n",
    "    nltk \\\n",
    "    # torchvision \\\n",
    "    datasets \\\n",
    "    # ipywidgets \\\n",
    "    ipdb \\\n",
    "    # colorama \\\n",
    "    tf-keras \\\n",
    "    transformers \\\n",
    "    huggingface_hub \\\n",
    "    # numpy==1.22.0\n",
    "\n",
    "# efface la longue sortie d'installation\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9-wAi7pZ3DI"
   },
   "outputs": [],
   "source": [
    "# gemma 3 ne fonctionne pas avec uv install, il doit donc √™tre install√© s√©par√©ment\n",
    "!pip install gemma==3\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4boGA9rYdt9l"
   },
   "outputs": [],
   "source": [
    "# Importer les utilitaires syst√®me et math√©matiques\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import urllib.request\n",
    "import requests\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# V√©rifier les acc√©l√©rateurs connect√©s (GPU ou TPU) et configurer en cons√©quence\n",
    "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
    "    print(\"Un GPU est connect√©.\")\n",
    "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
    "    print(\"Un TPU est connect√©.\")\n",
    "    import jax.tools.colab_tpu\n",
    "    jax.tools.colab_tpu.setup_tpu()\n",
    "else:\n",
    "    print(\"Seul un acc√©l√©rateur CPU est connect√©.\")\n",
    "\n",
    "# √âviter que JAX n'alloue de la m√©moire GPU\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
    "\n",
    "# Importer les biblioth√®ques pour l'apprentissage profond bas√© sur JAX\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import optax\n",
    "\n",
    "# Importer les biblioth√®ques li√©es au NLP et aux mod√®les\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer,  AutoModel\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering # Pour le traitement d'images.\n",
    "\n",
    "from gemma import gm\n",
    "\n",
    "# Importer les biblioth√®ques de traitement d'images et de tra√ßage de graphiques\n",
    "from livelossplot import PlotLosses\n",
    "import matplotlib.pyplot as plt\n",
    "import  numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Importer des utilitaires suppl√©mentaires pour travailler avec du texte et des mod√®les\n",
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# T√©l√©charger une image d'exemple √† utiliser dans le notebook\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
    "    \"cat.png\",\n",
    ")\n",
    "\n",
    "# Importer les biblioth√®ques pour le pr√©traitement du NLP et pour travailler avec des mod√®les pr√©-entra√Æn√©s\n",
    "# import gensim\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "nltk.download(\"word2vec_sample\")\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Importer les outils Hugging Face et les widgets IPython\n",
    "# import huggingface_hub\n",
    "# import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "# import colorama\n",
    "\n",
    "# Configurer Matplotlib pour que la sortie soit au format SVG afin d'obtenir des graphiques de meilleure qualit√©\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-zS13TTVrPp"
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Configuration termin√©e !\")\n",
    "print(f\"üêç Version de Python : {sys.version}\")\n",
    "print(f\"üî• Version de PyTorch : {torch.__version__}\")\n",
    "print(f\"ü§ó Version de Transformers : {transformers.__version__}\")\n",
    "print(f\"üíª CUDA disponible : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9X10jhocGaS"
   },
   "outputs": [],
   "source": [
    "# @title [Ex√©cutez-moi] Fonctions d'assistance pour le trac√© de graphiques.\n",
    "\n",
    "def plot_position_encodings(P, max_tokens, d_model):\n",
    "    \"\"\"\n",
    "    Trace la matrice des encodages de position.\n",
    "\n",
    "    Args:\n",
    "        P: Matrice d'encodage de position (tableau 2D).\n",
    "        max_tokens: Nombre maximum de tokens (lignes) √† tracer.\n",
    "        d_model: Dimensionalit√© du mod√®le (colonnes) √† tracer.\n",
    "    \"\"\"\n",
    "\n",
    "    # D√©finir la taille du graphique en fonction du nombre de tokens et des dimensions du mod√®le\n",
    "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
    "\n",
    "    # Tracer la matrice d'encodage de position avec une carte de couleurs pour une meilleure visualisation\n",
    "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
    "\n",
    "    # Ajouter une barre de couleurs pour indiquer les valeurs d'encodage\n",
    "    plt.colorbar(im, cmap=\"blue\")\n",
    "\n",
    "    # Afficher les indices d'embedding comme des graduations si la dimensionnalit√© est petite\n",
    "    if d_model <= 64:\n",
    "        plt.xticks(range(d_model))\n",
    "\n",
    "    # Afficher les indices de position comme des graduations si le nombre de tokens est petit\n",
    "    if max_tokens <= 32:\n",
    "        plt.yticks(range(max_tokens))\n",
    "\n",
    "    # √âtiqueter les axes\n",
    "    plt.xlabel(\"Indice de l'embedding\")\n",
    "    plt.ylabel(\"Indice de la position\")\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"\n",
    "    Trace une matrice de poids d'attention avec des graduations d'axes personnalis√©es.\n",
    "\n",
    "    Args:\n",
    "        weight_matrix: La matrice de poids d'attention √† tracer.\n",
    "        x_ticks: √âtiquettes pour l'axe des x (g√©n√©ralement les tokens de la requ√™te).\n",
    "        y_ticks: √âtiquettes pour l'axe des y (g√©n√©ralement les tokens de la cl√©).\n",
    "    \"\"\"\n",
    "\n",
    "    # D√©finir la taille du graphique\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Tracer la matrice de poids d'attention sous forme de carte de chaleur (heatmap)\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "\n",
    "    # D√©finir les graduations personnalis√©es sur les axes x et y\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
    "\n",
    "    # √âtiqueter le graphique\n",
    "    plt.title(\"Matrice d'attention\")\n",
    "    plt.xlabel(\"Score d'attention\")\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # V√©rifier que le contenu est une image\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"image/\"):\n",
    "            raise ValueError(f\"Le contenu de l'URL n'est pas une image. Content-Type: {content_type}\")\n",
    "\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "    except:\n",
    "        print(f\"Impossible de charger l'image depuis {url}\\n \")\n",
    "        return None\n",
    "\n",
    "def resize_image(img, new_width=300):\n",
    "    w, h = img.size\n",
    "    new_height = int((new_width / w) * h)\n",
    "    return img.resize((new_width, new_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMkaKekB_pR4"
   },
   "outputs": [],
   "source": [
    "# @title [Ex√©cutez-moi] Fonctions d'assistance pour le traitement de texte.\n",
    "\n",
    "def get_word2vec_embedding(words: list[str]):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les embeddings pour une liste de mots donn√©e √† partir d'un fichier texte de style Word2Vec.\n",
    "\n",
    "    Le fichier doit commencer par une ligne d'en-t√™te :\n",
    "        <taille_vocabulaire> <dimension_vecteur>\n",
    "    suivie d'un mot + son vecteur par ligne, par exemple :\n",
    "        faon 0.0891758 0.121832 ‚Ä¶ 0.0872918\n",
    "\n",
    "    Args:\n",
    "        words: It√©rable de tokens pour lesquels vous souhaitez des embeddings.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: jnp.ndarray de forme (n_found, dimension_vecteur)\n",
    "        found_words: List[str] des mots (dans le m√™me ordre que les embeddings).\n",
    "    \"\"\"\n",
    "    words_set = set(words)\n",
    "    found_embeddings = []\n",
    "    found_words = []\n",
    "    # T√©l√©charger depuis le Hub\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"AmelSellami/pruned-word2vec\",\n",
    "        filename=\"pruned.word2vec.txt\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Lire et analyser l'en-t√™te\n",
    "        header = f.readline().strip().split()\n",
    "        if len(header) != 2:\n",
    "            raise ValueError(f\"En-t√™te invalide dans {file_path!r}: {header}\")\n",
    "        vocab_size, dim = map(int, header)\n",
    "\n",
    "        # Balayer chaque ligne pour trouver les mots cibles\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            token = parts[0]\n",
    "            if token in words_set:\n",
    "                # analyser les flottants ; s'attendre √† exactement `dim` nombres\n",
    "                vals = parts[1:]\n",
    "                if len(vals) != dim:\n",
    "                    raise ValueError(f\"Taille de vecteur inattendue pour {token!r}: re√ßu {len(vals)} vs {dim}\")\n",
    "                vec = [float(x) for x in vals]\n",
    "                found_embeddings.append(vec)\n",
    "                found_words.append(token)\n",
    "                words_set.remove(token)\n",
    "                if not words_set:\n",
    "                    break  # tout trouv√©\n",
    "\n",
    "    embeddings = jnp.array(found_embeddings)\n",
    "    return embeddings, found_words\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Fonction qui prend une cha√Æne de caract√®res et supprime toute la ponctuation.\"\"\"\n",
    "    import re\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def print_sample(prompt, sample, model_name=\"\", generation_time=None):\n",
    "\n",
    "    if prompt in sample:\n",
    "      sample = sample.split(prompt)[1].rstrip()\n",
    "    html = f\"\"\"\n",
    "    <div style=\"font-family:monospace; border:1px solid #ccc; padding:10px\">\n",
    "        <div><b style='color:teal;'>ü§ñ Mod√®le:</b> <span>{model_name}</span></div>\n",
    "        {'<div><b style=\"color:orange;\">‚è±Ô∏è Temps de g√©n√©ration:</b> ' + f'{generation_time:.2f}s</div>' if generation_time else ''}\n",
    "        <div><b style='color:green;'>üìù Requ√™te :</b> {prompt}</div>\n",
    "        <div><b style='color:purple;'>‚ú® G√©n√©r√© :</b> {sample}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name: str):\n",
    "    \"\"\"\n",
    "    Fonction qui prend un nom de mod√®le et renvoie le tokenizer pour ce mod√®le.\n",
    "    \"\"\"\n",
    "    if model_name == \"gemma3\":\n",
    "        tokenizer = gm.text.Gemma3Tokenizer()\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize(text: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Fonction qui prend une cha√Æne de caract√®res et un tokenizer et renvoie la version tokenis√©e de la cha√Æne.\n",
    "    \"\"\"\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = [tokenizer.decode(t) for t in token_ids]\n",
    "    if model_name != \"gemma3\":\n",
    "        tokens = [token.replace('ƒ†', ' ') for token in tokens] # Remplace le pr√©fixe 'ƒ†' utilis√© par certains tokenizers par un espace\n",
    "    return tokens, token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElXJ1BGzH5BJ"
   },
   "source": [
    "## ü§ñ Charger un mod√®le depuis Hugging Face et interagir localement [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lrb9vL2uO5A"
   },
   "source": [
    "\n",
    "### üéØ Objectif\n",
    "\n",
    "* Apprendre √† **charger un mod√®le depuis Hugging Face** et ex√©cuter une inf√©rence √† l‚Äôaide d‚Äôun LLM\n",
    "\n",
    "* Charger un mod√®le l√©ger (par ex. gpt-neo-125m) et le solliciter avec une question simple\n",
    "\n",
    "* Exp√©rimenter avec diff√©rents param√®tres de g√©n√©ration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tADD6ZfEX-u"
   },
   "source": [
    "\n",
    "üìö Bienvenue dans le monde des grands mod√®les de langue!\n",
    "Nous sommes ravis de vous avoir √† bord!üéâ Avant de plonger dans la partie pratique de notre voyage, prenons un d√©tour rapide dans le monde fascinant de [Hugging Face](https://huggingface.co/) - une incroyable plateforme open source pour cr√©er et d√©ployer des mod√®les de langage √† la pointe de la technologie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me-nBsxtGBA1"
   },
   "source": [
    "##### üß† **Quels sont les grands mod√®les de langue?**\n",
    "\n",
    "Les mod√®les de grandes langues (LLMS) sont des syst√®mes d'IA form√©s sur de grandes quantit√©s de donn√©es de texte pour comprendre et g√©n√©rer du texte de type humain.Ils travaillent en apprenant des mod√®les dans la langue et en pr√©disant le mot le plus probable √©tant donn√© un certain contexte.\n",
    "\n",
    "**Concepts** cl√©s:\n",
    "\n",
    "*   Reconnaissance des mod√®les: les LLMS analysent des milliards de mots pour comprendre la langue\n",
    "*   Pr√©diction du mot de prochain: √Ä la base, ils devinent le mot suivant le plus probable\n",
    "*   Compr√©hension du contexte: ils consid√®rent l'ensemble de l'entr√©e lors de la r√©alisation des pr√©dictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltfz9jstGSTJ"
   },
   "source": [
    "#### **√Ä propos de Huggingface**\n",
    "\n",
    "<img src=\"https://www.hugging-face.org/wp-content/uploads/2023/11/hugging-faces.png\" alt=\"Alt Text\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s0Lc8rnGZt6"
   },
   "source": [
    "**Huggingface** est le \"Github de l'IA\" - une plate-forme qui d√©mocratise l'acc√®s aux mod√®les d'IA de pointe.Fond√©e en 2016, ils fournissent:\n",
    "\n",
    "\n",
    "* [Model Hub](https://huggingface.co/models): des milliers de mod√®les pr√©-form√©s pr√™ts √† l'emploi\n",
    "* [Biblioth√®que Transformers](https://huggingface.co/docs/transformateurs): outils faciles √† utiliser pour travailler avec des mod√®les de langue\n",
    "* [Ensembles de donn√©es](https://huggingface.co/datasets): ensembles de donn√©es organis√©s pour la formation et l'√©valuation\n",
    "* [Espaces](https://huggingface.co/spaces): plate-forme pour h√©berger des d√©mos et des applications ML\n",
    "\n",
    "\n",
    "Dans ce Colab, nous affichons les promt en <span style=\"color:green;\"><b>vert</b></span> et les √©chantillons g√©n√©r√©s par un mod√®le en <span style=\"color:purple;\"><b>violet</b></span>, comme dans l‚Äôexemple ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNt4n7hrGOY4"
   },
   "outputs": [],
   "source": [
    "print_sample(prompt='My fake prompt', sample=' is awesome!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1e-FfGzGrr_"
   },
   "source": [
    "##### **Votre premier mod√®le de langage **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8NCoWUnGwD9"
   },
   "source": [
    "**Plongeons** √† quel point il est simple de charger et d'interagir avec un mod√®le de l'Hugging Face!ü§ó\n",
    "\n",
    "Pour ce tutoriel, nous avons pr√©configur√© plusieurs options de mod√®le pour vous pour exp√©rimenter:\n",
    "\n",
    "* **ELEUTHERAI / GPT-NEO-125M** - Un mod√®le l√©ger avec 125 millions de param√®tres.C'est rapide et √©conome en m√©moire - grand pour commencer!\n",
    "* **GPT2 et GPT2-Medium** - Mod√®les classiques form√©s par OpenAI, avec des param√®tres de 117m et 355 m respectivement.La variante moyenne offre plus de ma√Ætrise et de coh√©rence.\n",
    "* **TIIUAE / FALCON-RW-1B** - Un mod√®le open source plus grand de la famille Falcon, avec 1 milliard de param√®tres.\n",
    "* **Microsoft / PHI-4** - Un mod√®le de pointe de Microsoft s'est concentr√© sur la g√©n√©ration de langage de haute qualit√© avec une empreinte m√©moire plus petite.\n",
    "\n",
    "Vous pouvez changer de mod√®le en red√©marrant le noyau Colab et en mettant √† jour la variable `model_name` dans la cellule ci-dessous.\n",
    "\n",
    "> üí° **Note :** Les √©tapes de chargement et d‚Äôinteraction pr√©sent√©es ici s‚Äôappliquent √† **tout** mod√®le Hugging Face qui prend en charge la g√©n√©ration de texte via l‚ÄôAPI `pipeline`. N‚Äôh√©sitez pas √† explorer au-del√† de cette liste !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c87TjRSRHCmg"
   },
   "source": [
    "Note : le mod√®le `microsoft/phi-4` peut prendre plus d‚Äôune demi-heure pour charger tous les fichiers n√©cessaires. Nous vous recommandons d‚Äôutiliser d‚Äôautres mod√®les pendant ce pratique et de vous familiariser avec Phi-4 plus tard, √† votre rythme.\n",
    "\n",
    "G√©n√©rons du texte :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvRivN3VG_yT"
   },
   "outputs": [],
   "source": [
    "# D√©finir le nom du mod√®le sur 'EleutherAI/gpt-neo-125M' (il peut √™tre modifi√© via les options du menu d√©roulant).\n",
    "model_name = 'gpt2'  # @param ['EleutherAI/gpt-neo-125M', 'gpt2', 'gpt2-medium', 'Qwen/Qwen3-0.6B', 'tiiuae/falcon-rw-1b','microsoft/phi-4']\n",
    "\n",
    "# D√©finir la requ√™te pour le mod√®le de g√©n√©ration de texte.\n",
    "test_prompt = 'Once upon a time in a magical Kigali'  # @param {type: 'string'}\n",
    "\n",
    "# Cr√©er un pipeline de g√©n√©ration de texte en utilisant le mod√®le sp√©cifi√©.\n",
    "generator = transformers.pipeline('text-generation', model=model_name)\n",
    "\n",
    "# G√©n√©rer du texte √† partir de la requ√™te fournie.\n",
    "# 'do_sample=True' active l'√©chantillonnage pour introduire de l'al√©atoire dans la g√©n√©ration,\n",
    "# et 'min_length=30' garantit qu'au moins 30 tokens sont g√©n√©r√©s.\n",
    "model_output = generator(test_prompt, do_sample=True, min_length=30)\n",
    "\n",
    "clear_output() # Effacer la sortie pour garder le notebook propre.\n",
    "\n",
    "# Afficher l'√©chantillon de texte g√©n√©r√©.\n",
    "print_sample(test_prompt, model_output[0]['generated_text'], model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiXSAfkdHJ0m"
   },
   "source": [
    "**üí°tip:** essayez d'ex√©cuter le code ci-dessus avec diff√©rentes invites ou avec la m√™me invite plus d'une fois!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxouM28THJXB"
   },
   "source": [
    "**ü§î** Discussion: Pourquoi pensez-vous que le texte g√©n√©r√© change √† chaque fois, m√™me avec la m√™me invite? √âcrivez votre r√©ponse dans le champ d'entr√©e ci-dessous et discutez avec votre voisin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMwaKFMnHUJg"
   },
   "source": [
    "<Dettots>\n",
    "<summary> <strong> R√©ponse </strong> </summary>\n",
    "\n",
    "Le mod√®le utilise l'√©chantillonnage avec le hasard (temp√©rature> 0) pour g√©n√©rer diverses sorties.\n",
    "M√™me avec la m√™me entr√©e, la nature probabiliste de la g√©n√©ration de texte conduit √† des r√©sultats diff√©rents.\n",
    "\n",
    "</fords>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nflrXCXHY6u"
   },
   "source": [
    "#### **Comprendre les param√®tres de g√©n√©ration**\n",
    "\n",
    "Les param√®tres de g√©n√©ration contr√¥lent comment le mod√®le produit du texte.Explorons les plus importants:\n",
    "\n",
    "### Temp√©rature\n",
    "\n",
    "Contr√¥le le caract√®re al√©atoire des pr√©dictions:\n",
    "\n",
    "- **Faible (0,1 √† 0,5):** sorties conservatrices et pr√©visibles\n",
    "\n",
    "- **Medium (0,6‚Äì1,0):** cr√©ativit√© et coh√©rence √©quilibr√©es\n",
    "\n",
    "- **√âlev√© (1,1‚Äì2,0):** tr√®s cr√©atif mais potentiellement incoh√©rent\n",
    "\n",
    "### TOP-P (√©chantillonnage du noyau)\n",
    "Contr√¥le la diversit√© en limitant le vocabulaire consid√©r√©:\n",
    "\n",
    "- **Faible (0,1 √† 0,3):** tr√®s concentr√© sur les mots les plus probables\n",
    "- **High (0.8‚Äì1.0):** Considers more word possibilities  \n",
    "\n",
    "\n",
    "Exp√©rimentons avec ces param√®tres:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cxq7nR0pHnJv"
   },
   "outputs": [],
   "source": [
    "# @title Choisir le mod√®le et la requ√™te { run: \"auto\" }\n",
    "model_name = \"gpt2-medium\"  # @param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "prompt = \"Once upon a time in a magical Kigali,\"  # @param {type:\"string\"}\n",
    "temperature = 1  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "top_p = 0.2  # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
    "max_new_tokens = 64  # @param {type:\"slider\", min:10, max:256, step:1}\n",
    "seed = 2  # @param {type:\"integer\"}\n",
    "\n",
    "\n",
    "def run_sample(\n",
    "    model_name,  # Le mod√®le de langage que nous utiliserons pour g√©n√©rer du texte\n",
    "    prompt: str,  # La requ√™te textuelle que nous donnerons au mod√®le pour d√©marrer la g√©n√©ration\n",
    "    seed: int | None = None,  # Optionnel : un nombre pour rendre les r√©sultats pr√©visibles √† chaque fois\n",
    "    temperature: float = 0.6,  # Contr√¥le le caract√®re al√©atoire de la sortie du mod√®le ; les valeurs plus basses le rendent plus focalis√©\n",
    "    top_p: float = 0.9,  # Contr√¥le quelle proportion des mots les plus probables est prise en compte ; les valeurs plus √©lev√©es consid√®rent plus d'options\n",
    "    max_new_tokens: int = 64,  # Le nombre maximum de mots ou tokens que le mod√®le ajoutera √† la requ√™te\n",
    ") -> str:\n",
    "    # Cette fonction g√©n√®re du texte √† partir d'une requ√™te donn√©e en utilisant un mod√®le de langage,\n",
    "    # avec des options pour contr√¥ler l'al√©atoire, le nombre de tokens g√©n√©r√©s et la reproductibilit√©.\n",
    "\n",
    "    # Charger le mod√®le en fonction de la s√©lection\n",
    "    if 'gpt2' in model_name:\n",
    "        tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
    "        model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{model_name} n'est pas encore support√©.\")\n",
    "\n",
    "    clear_output()\n",
    "    # D√©placer le mod√®le vers le GPU s'il est disponible\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    # Aligner le padding du tokenizer\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Convertir le texte de la requ√™te en tokens que le mod√®le peut traiter\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Extraire les tokens (IDs d'entr√©e) et le masque d'attention (pour se concentrer sur les parties importantes) des entr√©es\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # D√©placer les tokens et le masque d'attention sur le m√™me appareil que le mod√®le (comme un GPU s'il est disponible)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    # D√©finir la mani√®re dont nous voulons que le mod√®le g√©n√®re du texte\n",
    "    generation_config = transformers.GenerationConfig(\n",
    "        do_sample=True,  # Permettre au mod√®le d'ajouter de l'al√©atoire √† sa g√©n√©ration de texte\n",
    "        temperature=temperature,  # Ajuster le degr√© d'al√©atoire de la sortie ; une valeur plus basse signifie plus de concentration\n",
    "        top_p=top_p,  # Consid√©rer les mots les plus probables qui constituent les 90 % des possibilit√©s\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Utiliser l'ID du token qui repr√©sente le padding (espace suppl√©mentaire)\n",
    "        top_k=0,  # Nous ne limitons pas aux top-k mots, donc nous mettons cette valeur √† 0\n",
    "    )\n",
    "\n",
    "    # Si une graine est fournie, la d√©finir pour que les r√©sultats soient reproductibles (m√™me sortie √† chaque fois)\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # G√©n√©rer du texte en utilisant le mod√®le avec les param√®tres que nous avons d√©finis\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,  # Fournir les tokens d'entr√©e au mod√®le\n",
    "        attention_mask=attention_mask,  # Fournir le masque d'attention pour aider le mod√®le √† se concentrer\n",
    "        return_dict_in_generate=True,  # Demander au mod√®le de retourner des informations d√©taill√©es\n",
    "        output_scores=True,  # Inclure les scores (niveaux de confiance) pour les tokens g√©n√©r√©s\n",
    "        max_new_tokens=max_new_tokens,  # D√©finir le nombre maximum de tokens √† g√©n√©rer\n",
    "        generation_config=generation_config,  # Appliquer nos param√®tres de g√©n√©ration de texte personnalis√©s\n",
    "    )\n",
    "\n",
    "    # S'assurer qu'une seule s√©quence (sortie) est g√©n√©r√©e, pour simplifier\n",
    "    assert len(generation_output.sequences) == 1\n",
    "\n",
    "    # Obtenir la s√©quence de tokens g√©n√©r√©e\n",
    "    output_sequence = generation_output.sequences[0]\n",
    "\n",
    "    # Convertir les tokens g√©n√©r√©s en texte lisible\n",
    "    output_string = tokenizer.decode(output_sequence)\n",
    "\n",
    "    # Afficher la requ√™te et la r√©ponse g√©n√©r√©e\n",
    "    print_sample(prompt, output_string, model_name=model_name)\n",
    "\n",
    "    # Retourner la r√©ponse de texte g√©n√©r√©e\n",
    "    return output_string\n",
    "\n",
    "# Ex√©cuter la g√©n√©ration interactive\n",
    "_ = run_sample(\n",
    "    model_name=model_name,\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    seed=seed,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "filsIJLTIEkv"
   },
   "source": [
    "\n",
    "üéØ **Essayez ceci :** Exp√©rimentez avec diff√©rents prompts et valeurs de temp√©rature. Quels motifs remarquez-vous ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5U3YbK_IGel"
   },
   "source": [
    "#### **Mod√®les linguistiques dans les applications du monde r√©el**\n",
    "\n",
    "Les mod√®les linguistiques ont de nombreuses applications pratiques.Explorons quelques-uns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQxfHAkuIZCM"
   },
   "source": [
    "**G√©n√©ration** de code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdnWggRcIXwo"
   },
   "outputs": [],
   "source": [
    "code_prompt = 'Write a Python function that calculates the fibonacci sequence:' # @param {type:'string'}\n",
    "model_name = 'gpt2'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "code_result = run_sample(model_name, code_prompt, temperature=0.3, max_new_tokens=200)\n",
    "print('üíª Code Generation:')\n",
    "print(code_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PbWOseNIr6l"
   },
   "source": [
    "**Question** R√©pondre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07daslXuIpgI"
   },
   "outputs": [],
   "source": [
    "qa_prompt = 'What are the main advantages of using version control in software development?' # @param {type:'string'}\n",
    "model_name = 'gpt2'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "qa_result = run_sample(model_name, qa_prompt, temperature=0.5, max_new_tokens=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDThwJmWIyCR"
   },
   "source": [
    "**√âcriture** cr√©ative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmEK-JD5IzMf"
   },
   "outputs": [],
   "source": [
    "story_prompt = 'Write the opening paragraph of a science fiction story:' # @param {type:'string'}\n",
    "model_name = 'EleutherAI/gpt-neo-125M'  # @param ['gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M']\n",
    "story_result = run_sample(model_name, story_prompt, temperature=0.9, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7bDaHeKa0kN"
   },
   "source": [
    "**Question** de vision R√©pondre\n",
    "\n",
    "La r√©ponse √† la question visuelle (VQA) est une t√¢che d'IA multimodale qui combine la vision informatique et la compr√©hension du langage naturel.L'objectif est simple mais puissant: √©tant donn√© une image et une question de langue naturelle √† ce sujet, le mod√®le doit g√©n√©rer une r√©ponse pertinente et pr√©cise.\n",
    "\n",
    "Dans l'exemple ci-dessous, nous utiliserons un mod√®le pr√©-form√© de HuggingFace pour montrer comment VQA fonctionne dans la pratique.\n",
    "\n",
    "> Ex√©cutez √† nouveau la cellule si vous rencontrez une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v7yJn-Ta74X"
   },
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Lake_Naivasha%2C_Kenya_%2832487531978%29.jpg/2880px-Lake_Naivasha%2C_Kenya_%2832487531978%29.jpg\" # @param {type:'string'}\n",
    "image = load_image_from_url(image_url)\n",
    "display(resize_image(image, new_width=800))\n",
    "\n",
    "# Poser une question.\n",
    "question = \"What is in the picture?\" # @param {type:'string'}\n",
    "\n",
    "# Charger le mod√®le et le processeur\n",
    "model_name = \"Salesforce/blip-vqa-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Pr√©parer les entr√©es.\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Ex√©cuter l'inf√©rence.\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "# D√©coder et afficher la r√©ponse.\n",
    "answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "clear_output()\n",
    "display(resize_image(image, new_width=800))\n",
    "print('')\n",
    "print_sample(question, answer, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXrCIqpc6PQk"
   },
   "source": [
    "**Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dy702C0vxRI1"
   },
   "outputs": [],
   "source": [
    "model = gm.nn.Gemma3_1B()\n",
    "params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_1B_IT)\n",
    "\n",
    "sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    multi_turn=True,\n",
    ")\n",
    "\n",
    "user = 'Share one metaphor linking \"shadow\" and \"laughter\".' # @param {type:'string'}\n",
    "\n",
    "turn0 = sampler.chat(user)\n",
    "print_sample(user, turn0, model_name=\"Gemma3_1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Rb8_6hP6xa4"
   },
   "outputs": [],
   "source": [
    "user = 'Expand it in a haiku.' # @param {type:'string'}\n",
    "turn1 = sampler.chat(user)\n",
    "print_sample(user, turn1, model_name=\"Gemma3_1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEpNUKkoJOUj"
   },
   "source": [
    "\n",
    "Plut√¥t cool, non ? ü§©\n",
    "Aujourd‚Äôhui, nous allons aller un peu plus loin ‚Äî en **entra√Ænant notre propre LLM inspir√© de Shakespeare** ! Cette exp√©rience pratique nous aidera √† comprendre comment ces mod√®les fonctionnent r√©ellement **dans les coulisses**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svfeQO7VIOIu"
   },
   "source": [
    "## üîç Architecture du transformateur R√©capitulatif rapide [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brwKNHT0-Hhl"
   },
   "source": [
    "\n",
    "L'architecture du transformateur a √©t√© introduite dans l'article intitul√© [l'attention est tout ce dont vous avez besoin] (https://proekedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-paper.pdf) par Vaswani et al.Comme le titre de l'article le sugg√®re, une telle architecture se compose essentiellement de m√©canismes d'attention ainsi que des couches d'alimentation et des couches lin√©aires, comme le montre le diagramme ci-dessous.\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
    "\n",
    "Les transformateurs et ses variations sont au c≈ìur des mod√®les de grande langue et ce n'est pas une exag√©ration de dire que presque tous les mod√®les de langue sont des architectures bas√©es sur les transformateurs.Comme vous pouvez le voir dans le diagramme, l'architecture du transformateur d'origine se compose de deux parties, une qui re√ßoit des entr√©es g√©n√©ralement appel√©es encodeur et une autre qui re√ßoit des sorties (c'est-√†-dire des cibles) appel√©e d√©codeur.En effet, le transformateur a √©t√© con√ßu pour la traduction automatique.\n",
    "\n",
    "Dans ce tutoriel, nous nous concentrerons uniquement sur la partie d√©codeur qui est l'architecture qui alimente les mod√®les de grande langue les plus modernes comme Chatgpt.\n",
    "\n",
    "### Pr√©sentation du d√©codeur du transformateur\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/$s_!qbpc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png\" width=\"650\" height=\"400\" />\n",
    "\n",
    "\n",
    "* **Pr√©paration de l‚Äôentr√©e**\n",
    "\n",
    "  * Texte brut ‚Üí **Tokenizer** ‚Üí s√©quence d‚Äôidentifiants de tokens\n",
    "  * Identifiants ‚Üí **Repr√©sentations des tokens** + **Repr√©sentations positionnelles** ‚Üí vecteurs d‚Äôentr√©e\n",
    "\n",
    "* **Blocs d√©codeurs empil√©s** (r√©p√©t√©s *N* fois)\n",
    "\n",
    "  1. **Normalisation de couche**\n",
    "  2. **Auto-attention multi-t√™te masqu√©e** (masque causal)\n",
    "\n",
    "     * La connexion r√©siduelle ajoute la sortie de l‚Äôattention √† son entr√©e\n",
    "  3. **Normalisation de couche**\n",
    "  4. **R√©seau feed-forward** (MLP)\n",
    "\n",
    "     * Deux couches lin√©aires + non-lin√©arit√©, appliqu√©es position par position\n",
    "     * La connexion r√©siduelle ajoute la sortie du FFN √† son entr√©e\n",
    "\n",
    "* **Projection de sortie**\n",
    "\n",
    "  * Vecteurs finaux du d√©codeur ‚Üí couche lin√©aire ‚Üí logits du vocabulaire ‚Üí softmax pour les probabilit√©s du token suivant\n",
    "\n",
    "Commen√ßons notre parcours en comprenant comment les mod√®les tokenisent le texte.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtWMaTddww65"
   },
   "source": [
    "## üß± Tokenisation [<font color = 'orange'> d√©butant </font>]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO9OWhQYMti4"
   },
   "source": [
    "Les mod√®les de langage naturel fonctionnent sur des entr√©es num√©riques discr√®tes, alors que le texte brut est une s√©quence de caract√®res. La **tokenisation** est le pont entre le texte lisible par l‚Äôhumain et les vecteurs d‚Äôentr√©e exploitables par le mod√®le. De mani√®re g√©n√©rale, la tokenisation :\n",
    "\n",
    "1. **Divise le texte en unit√©s (¬´ tokens ¬ª)**\n",
    "\n",
    "   * S√©pare une cha√Æne de caract√®res en mots, sous-mots ou caract√®res.\n",
    "2. **Associe chaque token √† un identifiant entier**\n",
    "\n",
    "   * Utilise un vocabulaire fixe pour que chaque token corresponde √† un indice unique.\n",
    "3. **Permet le traitement par lots et la recherche d‚Äôembeddings**\n",
    "\n",
    "   * Convertit des textes de longueur variable en s√©quences d‚Äôidentifiants remplies (padded) pouvant √™tre trait√©es par des r√©seaux neuronaux.\n",
    "\n",
    "Un token peut √™tre :\n",
    "\n",
    "* Un seul caract√®re (`i`, `n`, `d`, `a`, `b`)\n",
    "* Un sous-mot (`ind`, `aba`)\n",
    "* Un mot entier (`indaba`)\n",
    "\n",
    "Un vocabulaire est la liste fixe des tokens (mots, sous-mots ou caract√®res) qu‚Äôun mod√®le conna√Æt, chacun √©tant associ√© √† un identifiant entier unique pour la recherche d‚Äôembeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7uolj800B4z"
   },
   "source": [
    "\n",
    "\n",
    "Diff√©rents mod√®les ‚Äî comme GPT, Gemma, LLaMA, Mistral, et d‚Äôautres ‚Äî utilisent des tokenizers diff√©rents, chacun prenant ses propres d√©cisions sur la mani√®re de d√©couper le texte en tokens. La m√©thode de tokenisation la plus courante dans les LLM est le **Byte Pair Encoding (BPE)**. Si vous √™tes curieux de savoir comment cela fonctionne, cette [excellente vid√©o](https://www.youtube.com/watch?v=zduSFxRajkE) l‚Äôexplique tr√®s bien.\n",
    "\n",
    "> L‚Äôid√©e cl√© derri√®re la tokenisation est la **granularit√©** ‚Äî √† quel point un mod√®le doit-il d√©couper le texte pour comprendre et pr√©dire ce qui vient ensuite ? L‚Äôobjectif est de trouver un √©quilibre : d√©couper le texte en morceaux suffisamment petits pour que le mod√®le puisse bien g√©n√©raliser, mais pas trop petits au risque d‚Äôexploser le nombre de tokens. Un bon tokenizer garde un vocabulaire compact, g√®re efficacement la diversit√© des langues et compresse bien le texte, de sorte que moins de tokens sont n√©cessaires pour repr√©senter le sens ‚Äî surtout dans des contextes multilingues.\n",
    "\n",
    "La taille du vocabulaire correspond au nombre de tokens distincts (mots, sous-mots ou symboles) reconnus par le tokenizer d‚Äôun mod√®le.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ku_PEI0PF4l"
   },
   "source": [
    "### **üéØ Essayez vous-m√™me :** Playground du Tokenizer\n",
    "\n",
    "Passons √† la pratique. Visitez l‚Äôun des sites suivants :\n",
    "\n",
    "* [Tiktokenizer Playground (GPT-2)](https://tiktokenizer.vercel.app/?model=gpt2)\n",
    "* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n",
    "\n",
    "Collez la phrase :\n",
    "`Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.`\n",
    "\n",
    "<!-- üéØ Maintenant, essayez la m√™me phrase dans une autre langue que vous parlez ‚Äî yor√πb√°, kiswahili, fran√ßais, etc. Notez ce que vous avez observ√©. -->  \n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1XpIVAOk281R7i13IMYQHe0HZZG6tUrjw\" alt=\"TikTokenizer\" width=\"800\"/>\n",
    "  <figcaption><em></em></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6qNp4IhGP-K"
   },
   "source": [
    "### Jouez avec Gemma Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_Ao6l4ZGWyS"
   },
   "outputs": [],
   "source": [
    "tokenizer = gm.text.Gemma3Tokenizer()\n",
    "tokenized_prompt = tokenizer.encode('Glad to be at the Indaba!', add_bos=True)\n",
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCir3oSyGhIi"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([\n",
    " 122637,\n",
    " 531,\n",
    " 577,\n",
    " 657,\n",
    " 506,\n",
    " 1851,\n",
    " 6525,\n",
    " 236888,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFa34_iaIVwx"
   },
   "outputs": [],
   "source": [
    "#Tokenisons une langue diff√©rente\n",
    "arabic_tokens=tokenizer.encode('ÿ•ŸÜŸá ŸäŸàŸÖ ÿ¨ŸÖŸäŸÑ ÿßŸÑŸäŸàŸÖ')\n",
    "arabic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdOcWp9CInOn"
   },
   "outputs": [],
   "source": [
    "print('Arabic prompt got tokenized into the following tokens: \\n',\n",
    "      tokenizer.decode(arabic_tokens[0]), \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[1]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[2]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[3]),  \"\\n\",\n",
    "        tokenizer.decode(arabic_tokens[4]),  \"\\n\",\n",
    "\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YPYutB1TAes"
   },
   "source": [
    "## ìä≥ Embeddings [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-4wEi2kVz3v"
   },
   "source": [
    "Apr√®s la tokenisation, chaque ID de token est associ√© √† un vecteur dense via la **couche d‚Äôembedding**. Ces embeddings capturent l‚Äôinformation s√©mantique des tokens et servent d‚Äôentr√©e pour le reste du mod√®le.\n",
    "\n",
    "**Embeddings de tokens**\n",
    "Une table de correspondance apprise de forme `(taille_du_vocabulaire √ó d_model)`. Chaque ID de token devient un vecteur de dimension `d_model`.\n",
    "\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1mReprFfL9ezlIRh55Co0yzX3EjiwcHsf\" alt=\"Positional Encoding Vectors\" width=\"800\"/>\n",
    "<Figcaption> <em> </em> </gigcaption>\n",
    "</ figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A7eVrS1bCCz"
   },
   "source": [
    "Dans cette section, nous allons extraire directement les embeddings de tokens appris par le mod√®le Word2Vec pour un petit ensemble de mots exemples, puis utiliser l'analyse en composantes principales (ACP) pour projeter ces vecteurs de haute dimension en deux dimensions (2D). Enfin, nous tracerons les coordonn√©es en 2D afin de visualiser comment les tokens s√©mantiquement li√©s se regroupent naturellement dans l‚Äôespace d‚Äôembedding.\n",
    "\n",
    "L‚ÄôACP est une m√©thode de r√©duction de dimension qui pr√©serve les relations locales ‚Äî ainsi, dans le graphique obtenu, vous devriez voir des tokens similaires (comme ¬´ chien ¬ª vs ¬´ chat ¬ª ou ¬´ roi ¬ª vs ¬´ reine ¬ª) regroup√©s √† proximit√©.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDT-Faavc4sb"
   },
   "outputs": [],
   "source": [
    "# 1. Un ensemble de jetons\n",
    "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
    "word_embeddings, words = get_word2vec_embedding(words)\n",
    "\n",
    "# # 4. Appliquer l'ACP pour r√©duire la dimensionnalit√©\n",
    "# `n_components=2` r√©duit le vecteur n-dimensionnel √† 2 dimensions, soit 2 colonnes\n",
    "# tout en pr√©servant les relations locales.\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca.fit_transform(word_embeddings)\n",
    "\n",
    "# 5. Visualiser les plongements 2D\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0)\n",
    "\n",
    "# Ajoutez des annotations (les mots) √† chaque point\n",
    "for i, txt in enumerate(words):\n",
    "    ax.annotate(txt, (X_2d[i, 0], X_2d[i, 1]),\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=12,\n",
    "                fontweight='medium')\n",
    "\n",
    "plt.title('PCA Visualization of Word Embeddings from Word2Vec', fontsize=16)\n",
    "plt.xlabel('PCA  Component 1', fontsize=12)\n",
    "plt.ylabel('PCA Component 2', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxleHRaRJ6q7"
   },
   "source": [
    "L‚Äôimage APC ci-dessus vous apprend deux choses sur les embeddings :\n",
    "\n",
    "1. Similarit√© s√©mantique = proximit√© g√©om√©trique. Les mots ayant des significations ou contextes d‚Äôutilisation similaires se retrouvent proches les uns des autres.\n",
    "\n",
    "2. Analogies lin√©aires. Bien que non illustr√© ici, des d√©calages vectoriels tels que roi - homme ‚âà reine - femme sont possibles.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNO703V9SBcI"
   },
   "source": [
    "## ‚ïë Encodages de position: pourquoi l'ordre compte [<font color = 'orange'> d√©butant </font>]\n",
    "<! - (10 minutes) ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZIytxyvAqd9"
   },
   "source": [
    "\n",
    "**Pourquoi les embeddings positionnels ?**\n",
    "\n",
    "* Les embeddings de tokens seuls sont invariants par permutation, ce qui signifie qu‚Äôils ne savent pas quel token est venu en premier..\n",
    "* L‚Äôordre des mots est crucial pour le sens (¬´ Je suis heureux ¬ª ‚â† ¬´ Suis-je heureux ¬ª).\n",
    "\n",
    "**Comment fonctionnent les embeddings positionnels :**\n",
    "\n",
    "1. **Encodages fixes (sinuso√Ødaux)**\n",
    "\n",
    "   * Fonctions pr√©-calcul√©es de la position (sinus et cosinus √† diff√©rentes fr√©quences).\n",
    "   * Pas de param√®tres suppl√©mentaires ; supporte des s√©quences de longueur arbitraire.\n",
    "2. **Embeddings positionnels appris**\n",
    "\n",
    "   * Table de correspondance entra√Ænable de forme `(longueur_max_s√©quence √ó d_model)`.\n",
    "   * Chaque position a son propre vecteur d‚Äôembedding appris pendant l‚Äôentra√Ænement.\n",
    "\n",
    "**Combinaison token + position :**\n",
    "\n",
    "```text\n",
    "final_embedding[i] = token_embedding[i] + pos_embedding[i]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxkDif_aRGKy"
   },
   "source": [
    "##### **Fonctions sinus et cosinus: un moyen simple d'ajouter des informations de position**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJIyboPkRG-o"
   },
   "source": [
    "Pour r√©pondre aux propri√©t√©s souhait√©es √©voqu√©es ci-dessus, les auteurs de [*Attention is All You Need*](https://arxiv.org/pdf/1706.03762) proposent une technique simple d‚Äô**encodage positionnel**. Cette m√©thode injecte l‚Äôinformation sur l‚Äôordre des tokens dans les embeddings en appliquant une combinaison de fonctions sinus et cosinus √† diff√©rentes fr√©quences.\n",
    "\n",
    "L‚Äôencodage positionnel pour une position donn√©e `pos`, √† l‚Äôindice de dimension d‚Äôembedding `i`, avec une taille totale d‚Äôembedding `d_model`, est d√©fini par :\n",
    "\n",
    "$$\n",
    "PE_{\\text{pos}, i} =\n",
    "\\begin{cases}\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{i / d_{\\text{model}}}}\\right), & \\text{si } i \\bmod 2 = 0 \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{(i - 1) / d_{\\text{model}}}}\\right), & \\text{si } i \\bmod 2 = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "En supposant un mod√®le avec une taille d‚Äôembedding $d_{\\text{model}} = 8$, le vecteur d‚Äôencodage positionnel pour la position `pos` devient :\n",
    "\n",
    "$$\n",
    "PE_{\\text{pos}} =\n",
    "\\begin{bmatrix}\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{0 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{2 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{4 / 8}}\\right) \\\\\n",
    "\\sin\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right) \\\\\n",
    "\\cos\\left(\\frac{\\text{pos}}{10000^{6 / 8}}\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Note :** Les indices pairs utilisent le sinus, les indices impairs le cosinus. La division par des puissances de 10000 permet √† chaque dimension d‚Äôencoder une fr√©quence diff√©rente.\n",
    "\n",
    "Pour comprendre pourquoi ces encodages fonctionnent en pratique, cr√©ons une fonction pour les visualiser et exp√©rimenter avec la `token_sequence_length` et la dimension de l‚Äô`embedding` des tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQVRmEJKRLMq"
   },
   "outputs": [],
   "source": [
    "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
    "\n",
    "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
    "\n",
    "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
    "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
    "\n",
    "  i = jnp.arange(0, token_embedding, 2)\n",
    "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
    "  frequencies = positions * frequency_steps\n",
    "\n",
    "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
    "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
    "\n",
    "  return P\n",
    "\n",
    "token_sequence_length = 50 # @param {type: \"number\"}\n",
    "token_embedding = 768  # @param {type: \"number\"}\n",
    "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
    "plot_position_encodings(P, token_sequence_length, token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSxfPQhoRHBM"
   },
   "source": [
    "Remarquez comment chaque ligne du graphique, correspondant √† une position pr√©cise dans la s√©quence, affiche un motif ondul√© distinct √† travers les dimensions de l‚Äôembedding. Cela signifie que chaque position poss√®de un encodage fixe et unique, ce qui permet au mod√®le de diff√©rencier les tokens selon leur position dans la s√©quence. Ces encodages ne changent pas d‚Äôune ex√©cution √† l‚Äôautre ; ils sont enti√®rement d√©termin√©s par la formule.\n",
    "\n",
    "ü§î **Activit√© de groupe :**\n",
    "\n",
    "* <font color='orange'>Prenez un moment avec votre voisin pour explorer pourquoi ce motif sp√©cifique appara√Æt lorsque `token_sequence_length` est r√©gl√© √† 1000 et que `token_embedding` vaut 768.</font>\n",
    "* <font color='orange'>Exp√©rimentez avec des valeurs plus petites pour `token_sequence_length` et `token_embedding` afin de mieux comprendre et enrichir votre discussion.</font>\n",
    "* <font color='orange'>Vous vous demandez pourquoi la constante 10000 est utilis√©e ? Demandez √† votre voisin ce qu‚Äôil en pense.</font>\n",
    "* <font color='orange'>Essayez maintenant de r√©gler `token_sequence_length` √† 50 et `token_embedding` √† une valeur beaucoup plus grande, comme 10000. Qu‚Äôobservez-vous ? Avons-nous toujours besoin d‚Äôun embedding de token aussi grand ?</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qkh0KgRPdDf8"
   },
   "source": [
    "## Les embeddings informent l‚Äôattention [<font color = 'orange'> d√©butant </font>]\n",
    "\n",
    "<! - (10 minutes) ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo90Dfyrb9Kf"
   },
   "source": [
    "Comme nous l‚Äôavons appris ci-dessus, les embeddings projettent chaque token dans un espace vectoriel continu o√π les relations s√©mantiques se traduisent par une proximit√© g√©om√©trique. Les m√©canismes d‚Äôattention s‚Äôappuient directement sur ces embeddings en calculant des scores de similarit√© entre les vecteurs de tokens pour d√©terminer √† quel point chaque token doit ¬´ pr√™ter attention ¬ª aux autres tokens de la s√©quence. En d‚Äôautres termes, les embeddings fournissent les caract√©ristiques brutes, et l‚Äôattention utilise ces caract√©ristiques pour pond√©rer et combiner dynamiquement l‚Äôinformation entre les tokens.\n",
    "\n",
    "Ci-dessous, vous √©crirez une fonction qui impl√©mente l‚Äôattention par produit scalaire. L‚Äôobjectif est de calculer un vecteur de contexte, $c_t$, qui r√©sume l‚Äôinformation contenue dans les √©tats cach√©s ($H$) pertinente pour l‚Äô√©tat pr√©c√©dent ($q$).\n",
    "\n",
    "Cela se fait en trois √©tapes :\n",
    "\n",
    "* Calculer les scores d‚Äôattention (S) : calculer le produit scalaire du vecteur requ√™te $q$ avec tous les vecteurs cl√©s dans $H$. Cela donne la similarit√© entre chaque paire de vecteurs. Pour simplifier, nous consid√©rerons que l‚Äôembedding de chaque token sert √† la fois de cl√© et de requ√™te.\n",
    "\n",
    "* Calculer les poids d‚Äôattention ($\\alpha$) : appliquer une fonction softmax aux scores pour les normaliser en une distribution de probabilit√©.\n",
    "\n",
    "* Calculer le vecteur contexte ($c_t$) : calculer la somme pond√©r√©e des vecteurs valeurs (ici, $H$) en utilisant les poids d‚Äôattention.\n",
    "\n",
    "Ces √©tapes sont r√©sum√©es par les √©quations suivantes :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S &= q \\cdot H^T \\\\\n",
    "\\alpha &= \\text{softmax}(S) \\\\\n",
    "c_t &= \\alpha \\cdot H\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Enfin, nous visualiserons les poids d‚Äôattention r√©sultants pour un petit ensemble de mots exemples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-xcLmAFLl6G"
   },
   "outputs": [],
   "source": [
    "def dot_product_attention(hidden_states, previous_state):\n",
    "    \"\"\"\n",
    "    Calculate the dot product between the hidden states and previous states.\n",
    "\n",
    "    Args:\n",
    "        hidden_states: A tensor with shape [T_hidden, dm]\n",
    "        previous_state: A tensor with shape [T_previous, dm]\n",
    "    \"\"\"\n",
    "\n",
    "    # Astuce¬†: Pour calculer les scores d‚Äôattention, r√©fl√©chissez √† l‚Äôutilisation du vecteur ¬´¬†previous_state¬†¬ª et de la matrice ¬´¬†hidden_states¬†¬ª. Vous souhaitez d√©terminer dans quelle mesure chaque √©l√©ment de ¬´¬†previous_state¬†¬ª doit pr√™ter attention √† chaque √©l√©ment de ¬´¬†hidden_states¬†¬ª. N‚Äôoubliez pas qu‚Äôen multiplication matricielle, vous pouvez trouver la relation entre deux ensembles de vecteurs en multipliant l‚Äôun par la transpos√©e de l‚Äôautre.\n",
    "    # Astuce¬†: Utilisez ¬´¬†jnp.matmul¬†¬ª pour effectuer la multiplication matricielle entre ¬´¬†previous_state¬†¬ª et la transpos√©e de ¬´¬†hidden_states¬†¬ª (`hidden_states.T`).\n",
    "    scores = ...  # FINISH ME\n",
    "    # Astuce¬†: Maintenant que vous avez les scores, vous devez les convertir en probabilit√©s.\n",
    "    # Une fonction softmax est g√©n√©ralement utilis√©e dans les m√©canismes d'attention pour convertir les scores bruts en probabilit√©s\n",
    "    # dont la somme est √©gale √† 1. Cela aidera √† d√©terminer l'attention √† accorder √† chaque √©tat cach√©.\n",
    "    # Astuce¬†: Utilisez `jax.nn.softmax` pour appliquer la fonction softmax √† `scores`.\n",
    "    w_n = ...  # FINISH ME\n",
    "\n",
    "    # Multipliez les poids par les √©tats cach√©s pour obtenir le vecteur de contexte\n",
    "    # Astuce¬†: utilisez √† nouveau `jnp.matmul` pour multiplier les poids d'attention `w_n` par `hidden_states`\n",
    "    # pour obtenir le vecteur de contexte.\n",
    "    c_t = jnp.matmul(w_n, hidden_states)\n",
    "\n",
    "    return w_n, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFDyXq77Ll8c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title Ex√©cutez-moi pour tester votre code\n",
    "key = jax.random.PRNGKey(42)\n",
    "x = jax.random.normal(key, [2, 2])\n",
    "\n",
    "try:\n",
    "  w_n, c_t = dot_product_attention(x, x)\n",
    "\n",
    "  w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
    "  c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
    "  assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
    "  assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
    "\n",
    "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
    "except:\n",
    "  print(\"It looks like the function isn't fully implemented yet. Try modifying it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqBReNvBLl_M"
   },
   "outputs": [],
   "source": [
    "# lors de la modification de ces mots, notez que si le mot ne se trouve pas dans le\n",
    "# corpus d'entra√Ænement d'origine, il ne sera pas affich√© dans le graphique de la matrice de poids.\n",
    "# @titre R√©ponse √† la t√¢che de code (essayez de ne pas jeter un coup d'≈ìil avant d'avoir bien essay√© !)\n",
    "def dot_product_attention(hidden_states, previous_state):\n",
    "    # Calculer les scores d'attention :\n",
    "    # Multipliez le vecteur d'√©tat pr√©c√©dent par la transpos√©e de la matrice d'√©tats cach√©s.\n",
    "    # Cela nous donne une matrice de scores qui montre √† quel point chaque √©l√©ment de l'√©tat pr√©c√©dent\n",
    "    # doit pr√™ter attention √† chaque √©l√©ment des √©tats cach√©s.\n",
    "    # Le r√©sultat est une matrice de forme [T, N], o√π :\n",
    "    # T est le nombre d'√©l√©ments dans les √©tats cach√©s,\n",
    "    # N est le nombre d'√©l√©ments dans l'√©tat pr√©c√©dent.\n",
    "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
    "\n",
    "    # Appliquer la fonction softmax aux scores pour les convertir en probabilit√©s.\n",
    "    # Cela normalise les scores de sorte qu'ils totalisent 1 pour chaque √©l√©ment,\n",
    "    # nous permettant de les interpr√©ter comme la quantit√© d'attention √† accorder √† chaque √©tat cach√©.\n",
    "    w_n = jax.nn.softmax(scores)\n",
    "\n",
    "    # Calculer le vecteur de contexte (c_t) :\n",
    "    # Multiplier les poids d'attention (w_n) par les √©tats cach√©s.\n",
    "    # Cela combine les √©tats cach√©s en fonction de la quantit√© d'attention que chacun m√©rite,\n",
    "    # ce qui donne un nouveau vecteur qui repr√©sente la somme pond√©r√©e des √©tats cach√©s.\n",
    "    # La forme r√©sultante est [T, d], o√π :\n",
    "    # T est le nombre d'√©l√©ments dans l'√©tat pr√©c√©dent,\n",
    "    # d est la dimension des √©tats cach√©s.\n",
    "    c_t = jnp.matmul(w_n, hidden_states)\n",
    "\n",
    "    # Renvoyer les poids d'attention et le vecteur de contexte.\n",
    "    return w_n, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N83teuL2CkmN"
   },
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
    "word_embeddings, words = get_word2vec_embedding(words)\n",
    "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
    "plot_attention_weight_matrix(weights, words, words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXQaSJMGELFC"
   },
   "source": [
    "En regardant la matrice, on peut voir quels mots ont des significations similaires. Le groupe de mots ¬´ royal ¬ª pr√©sente des scores d‚Äôattention plus √©lev√©s entre eux que les mots du groupe ¬´ nourriture ¬ª, qui eux aussi s‚Äôattachent les uns aux autres. On observe √©galement que le mot ¬´ ordinateur ¬ª obtient des scores d‚Äôattention tr√®s faibles avec tous les autres, ce qui montre qu‚Äôil est peu li√© aux mots des groupes ¬´ royal ¬ª ou ¬´ nourriture ¬ª.\n",
    "\n",
    "Note : Le produit scalaire est seulement l‚Äôune des mani√®res d‚Äôimpl√©menter la fonction de score dans les m√©canismes d‚Äôattention. Vous trouverez une liste plus compl√®te dans cet [article de blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) de Dr Lilian Weng.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY02IFQouwjN"
   },
   "source": [
    "## üîç ATTENTION [<FONT COLOR = 'GREEN'> Interm√©diaire </font>]\n",
    "\n",
    "<! - (25 minutes) ->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qJdLHPBL1I8"
   },
   "source": [
    "### Entre l‚Äôauto-attention et l‚Äôattention multi-t√™te\n",
    "\n",
    "L‚Äôauto-attention et l‚Äôattention multi-t√™te (MHA) sont des composants fondamentaux de l‚Äôarchitecture des transformeurs. Dans cette section, nous expliquerons en d√©tail l‚Äôintuition derri√®re ces concepts ainsi que leur mise en ≈ìuvre. Plus tard, dans la section Transformeurs, vous apprendrez comment ces m√©canismes d‚Äôattention sont utilis√©s pour cr√©er un mod√®le s√©quence-√†-s√©quence reposant enti√®rement sur l‚Äôattention.\n",
    "\n",
    "Au fur et √† mesure de notre progression, nous repr√©senterons les phrases en les d√©composant en mots individuels et en encodant chaque mot √† l‚Äôaide du mod√®le word2vec pr√©sent√© pr√©c√©demment. Dans la section Transformeurs, nous √©tudierons plus en d√©tail comment les s√©quences d‚Äôentr√©e sont transform√©es en une s√©rie de vecteurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfEhWwBaLmFK"
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Int√®gre une phrase en utilisant word2vec ; √† des fins d'exemple d'utilisation uniquement.\n",
    "    \"\"\"\n",
    "    # nettoyer la phrase (pas n√©cessaire si on utilise un tokenizer de LLM appropri√©)\n",
    "    sentence = remove_punctuation(sentence)\n",
    "\n",
    "    # extraire les mots individuels\n",
    "    words = sentence.split()\n",
    "\n",
    "    # obtenir l'embedding word2vec pour chaque mot de la phrase\n",
    "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
    "\n",
    "    # retourner avec une dimension suppl√©mentaire (utile pour cr√©er des lots plus tard)\n",
    "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUPfggF9L9tE"
   },
   "source": [
    "### Auto-attention\n",
    "\n",
    "Une question simple √† propos de cette phrase est : √† quoi se r√©f√®re le mot ¬´‚ÄØit‚ÄØ¬ª ? M√™me si cela peut sembler facile, il peut √™tre difficile pour un algorithme de l‚Äôapprendre. C‚Äôest l√† qu‚Äôintervient l‚Äôauto-attention, qui peut apprendre une matrice d‚Äôattention pour le mot ¬´‚ÄØit‚ÄØ¬ª, o√π un poids important est attribu√© au mot ¬´‚ÄØanimal‚ÄØ¬ª.\n",
    "\n",
    "L‚Äôauto-attention permet √©galement au mod√®le d‚Äôapprendre √† interpr√©ter des mots ayant les m√™mes embeddings, comme apple, qui peut d√©signer une entreprise ou un fruit selon le contexte. Ce m√©canisme est tr√®s similaire √† l‚Äô√©tat cach√© que l‚Äôon trouve dans un r√©seau de neurones r√©current (RNN) (un autre type de r√©seau de neurones utilis√© pour traiter des donn√©es textuelles), mais ce processus, comme vous le verrez, permet au mod√®le de pr√™ter attention √† l‚Äôensemble de la s√©quence en parall√®le, ce qui autorise l‚Äôutilisation de s√©quences plus longues.\n",
    "\n",
    "L‚Äôauto-attention repose sur trois concepts :\n",
    "\n",
    "* Requ√™tes, cl√©s et valeurs\n",
    "* Attention par produit scalaire √† l‚Äô√©chelle\n",
    "* Masques\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1VwPK-JVOe_NyY4QwKcaCxp4YGpxVIu1u\" alt=\"Vecteurs d‚Äôencodage positionnel\" width=\"800\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnrWe3TsTpix"
   },
   "outputs": [],
   "source": [
    "class SequenceToQKV(nn.Module):\n",
    "  output_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, X):\n",
    "\n",
    "    # d√©finir la m√©thode pour l'initialisation des poids\n",
    "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
    "\n",
    "    # initialiser trois couches lin√©aires pour effectuer les transformations QKV.\n",
    "    # note : cela peut aussi √™tre une seule couche, comment pensez-vous que vous le feriez ?\n",
    "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
    "\n",
    "    # transformer et retourner les matrices\n",
    "    Q = q_layer(X)\n",
    "    K = k_layer(X)\n",
    "    V = v_layer(X)\n",
    "\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ovARyIGUt8M"
   },
   "source": [
    "Mais qu‚Äôest-ce que la requ√™te, la cl√© et la valeur ?\n",
    "\n",
    "üéØ **Analogie concr√®te :**\n",
    "Imaginez que vous √™tes dans une biblioth√®que, cherchant les informations les plus pertinentes pour r√©pondre √† une question.\n",
    "\n",
    "* Vous avez une question en t√™te : c‚Äôest votre **requ√™te** (query).\n",
    "* Chaque livre dans la biblioth√®que a un titre ou une description : c‚Äôest sa **cl√©** (key).\n",
    "* √Ä l‚Äôint√©rieur de chaque livre se trouve le contenu r√©el : c‚Äôest la **valeur** (value).\n",
    "\n",
    "Dans l‚Äôauto-attention, chaque mot d‚Äôune phrase joue les trois r√¥les :\n",
    "\n",
    "* Il cr√©e une requ√™te : ¬´ Que suis-je en train de chercher ? ¬ª\n",
    "* Il pr√©sente une cl√© : ¬´ Quelle information est contenue en moi ? ¬ª\n",
    "* Il offre une valeur : ¬´ Voici ce que je peux apporter. ¬ª\n",
    "\n",
    "En g√©n√©ral :\n",
    "\n",
    "* L‚Äôauto-attention est invariante √† la permutation (l‚Äôordre peut √™tre r√©arrang√© sans changer le r√©sultat).\n",
    "* L‚Äôauto-attention ne n√©cessite pas de param√®tres. Jusqu‚Äôici, l‚Äôinteraction entre les mots √©tait guid√©e par leurs embeddings et les encodages positionnels.\n",
    "* On s‚Äôattend √† ce que les valeurs le long de la diagonale (de la matrice) soient les plus √©lev√©es.\n",
    "* Si on ne souhaite pas que certaines positions interagissent, on peut toujours fixer leurs valeurs √† $-\\infty$.\n",
    "\n",
    "**Conclusion :** L‚Äôauto-attention permet au mod√®le de relier les mots entre eux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldE3HlzrMI0x"
   },
   "source": [
    "Maintenant que nous avons nos matrices `query`, `key` et `value`, il est temps de calculer la matrice d‚Äôattention. Rappelez-vous que dans tous les m√©canismes d‚Äôattention, il faut d‚Äôabord calculer un score pour chaque vecteur de la s√©quence, puis utiliser ces scores pour cr√©er un nouveau vecteur de contexte. Dans l‚Äôauto-attention, le calcul des scores se fait via le produit scalaire √† √©chelle (scaled dot product attention), puis les scores normalis√©s sont utilis√©s comme poids pour sommer les vecteurs valeurs et ainsi cr√©er le vecteur de contexte.\n",
    "\n",
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "o√π les scores d‚Äôattention sont calcul√©s par\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "puis ces scores sont multipli√©s par $V$ pour obtenir le vecteur de contexte.\n",
    "\n",
    "Ce qui se passe ici est similaire √† ce que nous avons vu avec l‚Äôattention par produit scalaire dans la section pr√©c√©dente, mais appliqu√© √† la s√©quence elle-m√™me. Pour chaque √©l√©ment de la s√©quence, on calcule la matrice des poids d‚Äôattention entre $q_i$ et $K$. Ensuite, on multiplie $V$ par chaque poids et on somme finalement tous les vecteurs pond√©r√©s $v_{\\text{weighted}}$ pour obtenir une nouvelle repr√©sentation pour $q_i$. Ainsi, on att√©nue les vecteurs moins pertinents et on renforce ceux importants dans la s√©quence lorsque notre attention est port√©e sur $q_1$.\n",
    "\n",
    "Le produit $QK^\\top$ est divis√© par la racine carr√©e de la dimension des vecteurs, $\\sqrt{d_k}$, afin d‚Äôassurer une meilleure stabilit√© des gradients lors de l‚Äôentra√Ænement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b5cyg7ALmKJ"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Formule pour retourner l'attention par produit scalaire √† l'√©chelle √©tant donn√© les matrices QKV.\n",
    "    \"\"\"\n",
    "    d_k = key.shape[-1]\n",
    "\n",
    "    # obtenir les scores bruts (logits) en effectuant le produit scalaire des requ√™tes et des cl√©s\n",
    "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
    "\n",
    "    # mettre les scores bruts √† l'√©chelle et appliquer la fonction softmax pour obtenir les scores/poids d'attention\n",
    "    scaled_logits = logits / jnp.sqrt(d_k)\n",
    "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "    # multiplier les poids par la matrice de valeur pour obtenir la sortie\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFaCf5ipMNoF"
   },
   "source": [
    "Voyons maintenant l‚Äôattention par produit scalaire √† √©chelle en action. Nous prendrons une phrase, encoderons chaque mot avec word2vec, puis observerons √† quoi ressemblent les poids finaux de l‚Äôauto-attention.\n",
    "\n",
    "Nous n‚Äôutiliserons pas les couches de projection lin√©aire n√©cessaires pour entra√Æner ces matrices. Pour simplifier, nous allons poser $X = Q = V = K$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E_oxD2GLmM-"
   },
   "outputs": [],
   "source": [
    "# d√©finir une phrase\n",
    "sentence = \"I drink coke, but eat steak\"\n",
    "\n",
    "# int√©grer et cr√©er des matrices QKV\n",
    "word_embeddings, words = embed_sentence(sentence)\n",
    "Q = K = V = word_embeddings\n",
    "\n",
    "# calculer les poids et tracer\n",
    "outputs, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# tracer les mots et les poids d'attention entre eux\n",
    "words = remove_punctuation(sentence).split()\n",
    "print(attention_weights[0].shape, len(words))\n",
    "plot_attention_weight_matrix(attention_weights[0], words, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz_DkHbBMf4E"
   },
   "source": [
    "Gardez √† l‚Äôesprit que nous n‚Äôavons pas encore entra√Æn√© notre matrice d‚Äôattention. Cependant, en utilisant les vecteurs word2vec comme s√©quence, on peut d√©j√† observer que l‚Äôattention par produit scalaire √† √©chelle est capable de focaliser sur le mot ¬´‚ÄØeat‚ÄØ¬ª lorsque la requ√™te est ¬´‚ÄØsteak‚ÄØ¬ª, et que la requ√™te ¬´‚ÄØdrink‚ÄØ¬ª pr√™te plus d‚Äôattention √† ¬´‚ÄØcoke‚ÄØ¬ª et ¬´‚ÄØeat‚ÄØ¬ª.\n",
    "\n",
    "Plus de ressources :\n",
    "\n",
    "[Attention avec Q, K, V (vid√©o)](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ4lTjELMj68"
   },
   "source": [
    "### Attention masqu√©e\n",
    "\n",
    "L‚Äôattention masqu√©e est une technique utilis√©e dans les m√©canismes d‚Äôattention ‚Äî en particulier dans les transformeurs ‚Äî pour emp√™cher un mod√®le de ¬´ regarder vers l‚Äôavant ¬ª quand ce n‚Äôest pas autoris√©.\n",
    "\n",
    "üìö **Intuition : comme lire sans spoilers**\n",
    "Imaginez que vous lisez un roman policier chapitre par chapitre. Vous voulez deviner qui est le coupable sans sauter √† la fin. L‚Äôattention masqu√©e fonctionne de la m√™me fa√ßon :\n",
    "\n",
    "¬´ √Ä la position $t$, le mod√®le n‚Äôa le droit de pr√™ter attention qu‚Äôaux tokens √† la position $t$ ou avant, pas apr√®s. ¬ª\n",
    "\n",
    "üïµÔ∏è‚Äç‚ôÇÔ∏è **Pourquoi utiliser l‚Äôattention masqu√©e ?**\n",
    "\n",
    "1. üß± **Remplissage (padding) dans les s√©quences de longueurs in√©gales**\n",
    "   Lorsque l‚Äôon regroupe en batch des s√©quences (phrases ou s√©ries temporelles) de longueurs diff√©rentes, on compl√®te g√©n√©ralement les plus courtes avec des tokens de padding pour que toutes aient la m√™me taille. Mais ces tokens de padding ne contiennent aucune information r√©elle.\n",
    "\n",
    "‚ùó Si on ne les masque pas, le mod√®le pourrait les consid√©rer comme du contenu pertinent, ce qui perturberait l‚Äôapprentissage.\n",
    "\n",
    "2. üîí **Emp√™cher le regard vers l‚Äôavenir dans les mod√®les d√©codeurs**\n",
    "   Dans les mod√®les g√©n√©rateurs de s√©quences (comme GPT), on les entra√Æne en utilisant la phrase enti√®re en sortie. Mais lors de la g√©n√©ration r√©elle, le mod√®le ne doit voir que les tokens pass√©s et pr√©sents, pas ceux du futur.\n",
    "\n",
    "üß† Imaginez √©crire une histoire mot par mot. Vous ne devriez pas pouvoir lire la suite avant d‚Äôavoir √©crit le mot suivant !\n",
    "\n",
    "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"sch√©ma attention masqu√©e\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMdbuQLSMlqr"
   },
   "outputs": [],
   "source": [
    "# exemple de construction d'un masque pour des tokens de taille 32\n",
    "# le masque garantit que les positions ne portent attention qu'aux positions pr√©c√©dentes dans l'entr√©e (masque causal)\n",
    "# nous l'utiliserons plus tard pour ins√©rer des valeurs -inf dans les scores bruts\n",
    "mask = jnp.tril(jnp.ones((32, 32)))\n",
    "\n",
    "# tracer\n",
    "sns.heatmap(mask, cmap=\"Blues\")\n",
    "plt.title(\"Exemple de masque qui peut √™tre appliqu√©\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_7Wpv28Mp-M"
   },
   "source": [
    "Permet d√©sormais d'adapter notre fonction d'attention du produit DOT √† mise √† l'√©chelle pour impl√©menter l'attention masqu√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T9qSjMmMpOj"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Attention par produit scalaire √† l'√©chelle avec un masque causal (seulement autoris√© √† porter attention aux positions pr√©c√©dentes)\n",
    "    \"\"\"\n",
    "    d_k = key.shape[-1]\n",
    "    T_k = key.shape[-2]\n",
    "    T_q = query.shape[-2]\n",
    "\n",
    "    # obtenir les logits √† l'√©chelle en utilisant le produit scalaire comme avant\n",
    "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
    "    scaled_logits = logits / jnp.sqrt(d_k)\n",
    "\n",
    "    # ajouter le masque optionnel o√π les valeurs le long du masque sont fix√©es √† -inf\n",
    "    if mask is not None:\n",
    "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
    "\n",
    "    # calculer les poids d'attention via softmax\n",
    "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
    "\n",
    "    # additionner avec les valeurs pour obtenir la sortie\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weightsftmax(scaled_logits, axis=-1)\n",
    "\n",
    "    # sum with the values to get the output\n",
    "    output = jnp.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X31b1Pt6MvJ8"
   },
   "source": [
    "### La b√™te aux multiples t√™tes : l‚Äôattention multi-t√™te\n",
    "\n",
    "Nous avons parl√© du m√©canisme d‚Äôauto-attention dans la section pr√©c√©dente. Comment l‚Äôattention multi-t√™te se rapporte-t-elle √† ce m√©canisme d‚Äôauto-attention (attention par produit scalaire √† √©chelle) ?\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1e0C2tC29XylPRVfwXo_-NLzisQbLIdxl\" alt=\"Vecteurs d‚Äôencodage positionnel\" width=\"800\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>  \n",
    "\n",
    "L‚Äôauto-attention multi-t√™te est une variante de l‚Äôauto-attention utilis√©e dans le mod√®le Transformer. Elle consiste √† ex√©cuter plusieurs calculs d‚Äôattention en parall√®le, chacun se focalisant sur diff√©rentes relations et aspects de la s√©quence d‚Äôentr√©e.\n",
    "\n",
    "Au lieu de calculer l‚Äôattention une seule fois, le m√©canisme MHA applique plusieurs fois en parall√®le l‚Äôattention par produit scalaire √† √©chelle. Selon l‚Äôarticle *Attention is All You Need*, ¬´ l‚Äôattention multi-t√™te permet au mod√®le de porter simultan√©ment attention √† l‚Äôinformation provenant de diff√©rents sous-espaces de repr√©sentation √† diff√©rentes positions. Avec une seule t√™te d‚Äôattention, la moyenne inhibe cela. ¬ª\n",
    "\n",
    "L‚Äôattention multi-t√™te peut √™tre vue comme une strat√©gie similaire √† l‚Äôempilement de noyaux de convolution dans une couche CNN (Convolution Neural Network). Cela permet aux noyaux de se concentrer et d‚Äôapprendre diff√©rentes caract√©ristiques et r√®gles, ce qui explique pourquoi plusieurs t√™tes d‚Äôattention fonctionnent aussi bien.\n",
    "\n",
    "<figure>  \n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1ulHkifKMzFSHl7-pJnUpc5VP-H2FssED\" alt=\"Sch√©ma attention multi-t√™te\" width=\"1300\" height=\"700\"/>  \n",
    "  <figcaption><em></em></figcaption>  \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnOdT_r1M2U2"
   },
   "source": [
    "Ou plus pr√©cis√©ment quelque chose comme ceci: une pile d'attention du produit √† point √† l'√©chelle\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1lfMZAgs6bR5_0blSB95SAPuX1TNpNaCC\" alt=\"Positional Encoding Vectors\" width=\"500\"/>\n",
    "<Figcaption> <em> </em> </gigcaption>\n",
    "</ figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npyhGNKRM9CS"
   },
   "source": [
    "Voyons maintenant comment impl√©menter l‚Äôattention multi-t√™te. En termes simples, l‚Äôattention multi-t√™te revient √† ex√©cuter plusieurs fois en parall√®le le processus d‚Äôattention, en utilisant diff√©rentes copies des matrices $Q$, $K$ et $V$ pour chaque ¬´ t√™te ¬ª. Cela permet au mod√®le de se concentrer simultan√©ment sur diff√©rentes parties de l‚Äôentr√©e.\n",
    "\n",
    "Si vous souhaitez en savoir plus, consultez [cet article de blog de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) qui offre une explication d√©taill√©e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN0q3iq9SMdn"
   },
   "source": [
    "### Attention par produit scalaire √† √©chelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kurPdKRQMuL_"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int  # Nombre de t√™tes d'attention\n",
    "    d_m: int  # Dimension des embeddings du mod√®le\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialiser le module de transformation s√©quence-vers-QKV\n",
    "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
    "\n",
    "        # D√©finir l'initialiseur pour les poids de la couche lin√©aire de sortie\n",
    "        initializer = nn.initializers.variance_scaling(\n",
    "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
    "        )\n",
    "\n",
    "        # Initialiser la couche de projection de sortie Wo (utilis√©e apr√®s l'attention)\n",
    "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
    "\n",
    "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
    "        # Si Q, K, ou V ne sont pas fournis, utiliser l'entr√©e X pour les g√©n√©rer\n",
    "        if None in [Q, K, V]:\n",
    "            assert not X is None, \"X doit √™tre fourni si Q, K, ou V ne le sont pas\"\n",
    "\n",
    "            # G√©n√©rer les matrices Q, K et V √† partir de l'entr√©e X\n",
    "            Q, K, V = self.sequence_to_qkv(X)\n",
    "\n",
    "        # Extraire la taille du lot (B), la longueur de la s√©quence (T) et la taille de l'embedding (d_m)\n",
    "        B, T, d_m = K.shape\n",
    "\n",
    "        # Calculer la taille de l'embedding de chaque t√™te d'attention (d_m / num_heads)\n",
    "        head_size = d_m // self.num_heads\n",
    "\n",
    "        # Remodeler Q, K, V pour avoir des dimensions s√©par√©es pour les t√™tes\n",
    "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
    "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
    "\n",
    "        # Appliquer l'attention par produit scalaire √† l'√©chelle √† chaque t√™te\n",
    "        attention, attention_weights = scaled_dot_product_attention(\n",
    "            q_heads, k_heads, v_heads, mask\n",
    "        )\n",
    "\n",
    "        # Remodeler la sortie de l'attention pour revenir √† ses dimensions d'origine\n",
    "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
    "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
    "\n",
    "        # Appliquer la transformation lin√©aire de sortie Wo √† la sortie de l'attention\n",
    "        X_new = self.Wo(attention)\n",
    "\n",
    "        # Si return_weights est vrai, retourner √† la fois la sortie transform√©e et les poids d'attention\n",
    "        if return_weights:\n",
    "            return X_new, attention_weights\n",
    "        else:\n",
    "            # Sinon, retourner seulement la sortie transform√©e\n",
    "            return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF3tNzT_NGIm"
   },
   "source": [
    "\n",
    "## Que garder √† l'esprit:\n",
    "\n",
    "üîç Attention:\n",
    "\n",
    "* Capture des d√©pendances √† longue port√©e.\n",
    "\n",
    "* Permet la parall√©lisation (contrairement aux RNN).\n",
    "\n",
    "* L'attention est autochtone, sauf combinaison avec le codage positionnel.\n",
    "\n",
    "\n",
    "üí° Masque d'attention:\n",
    "\n",
    "* Masque causal (look-ahead): assure un comportement autor√©gressif (par exemple, le jeton T ne voit que des jetons ‚â§ t).\n",
    "\n",
    "* Masque de rembourrage: emp√™che l'attention aux positions rembourr√©es sans signification.\n",
    "\n",
    "* Impl√©ment√© en masquant les logits d'attention avant Softmax en utilisant $-\\\\infty$ (infinity).\n",
    "\n",
    "‚ú® Attention multiples\n",
    "\n",
    "* Ne vous contentez pas de regarder dans un sens - regardez plusieurs mod√®les √† la fois\n",
    "\n",
    "* Aide √† capturer plusieurs types de d√©pendances (par exemple, syntaxe, s√©mantique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X4tRtSZxGHg"
   },
   "source": [
    "## üèóÔ∏è Entra√Ænement de votre propre LLM (Transformers) \\[<font color='green'>Interm√©diaire</font>]\n",
    "\n",
    "<!-- (30 minutes)-->\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "* Charger un jeu de donn√©es et entra√Æner un LLM\n",
    "* Visualiser les encodages positionnels \\[<font color='orange'>D√©butant</font>]\n",
    "* Impl√©menter :\n",
    "\n",
    "  * Encodages positionnels\n",
    "  * Bloc FFN\n",
    "  * Normalisation de couche (Layer norm)\n",
    "  * Bloc d√©codeur\n",
    "  * LLM complet \\[<font color='green'>Interm√©diaire</font>]\n",
    "* D√©finir la fonction de perte\n",
    "* Charger le jeu de donn√©es d‚Äôentra√Ænement\n",
    "* √âcrire le script d‚Äôentra√Ænement\n",
    "* Effectuer une inf√©rence avec le mod√®le entra√Æn√© \\[<font color='orange'>D√©butant</font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e71jR6TYRHEP"
   },
   "source": [
    "### Bloc de transformateur <Font Color = 'Green'> Interm√©diaire </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kobmi8IRHHA"
   },
   "source": [
    "Tout comme un Multi Layer perceptron MLP (un r√©seau de neurones simple qui traite les donn√©es d‚Äôentr√©e √† travers plusieurs couches) ou un Convolution Neural Network CNN (un type de r√©seau de neurones particuli√®rement efficace pour reconna√Ætre des motifs dans les images gr√¢ce √† des couches de convolution). Transformeurs sont compos√©s d‚Äôune pile de blocs de transformeurs. Dans cette section, nous allons construire chacun des composants n√©cessaires √† la cr√©ation d‚Äôun de ces blocs de transformeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yAG_MbgRWEs"
   },
   "source": [
    "\n",
    "#### Network Feed Award Network (FFN) / Multicouche Perceptron (MLP) <FONT COLOR = 'ORANGE'> d√©butant </font>\n",
    "\n",
    "\n",
    "<div style = \"Display: flex; align-items: Centre; justify-content: Centre; √©cart: 40px;\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1gyHqjfJUg_BLoFhAH6_KqsKxOQWvYtvD\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"Drawing\" width=\"260\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv5FUmp0RYd9"
   },
   "source": [
    "Dans le mod√®le original, ces blocs consistent en un simple MLP (Perceptron Multi-Couches) √† 2 couches utilisant la fonction d‚Äôactivation ReLU. Cependant, la fonction GeLU (Gaussian Error Linear Unit) est devenue tr√®s populaire, et nous l‚Äôutiliserons tout au long de ce pratique. La formule ci-dessous repr√©sente le r√©seau de neurones feedforward (FFN) avec activation GeLU. Dans ce r√©seau, l‚Äôentr√©e $x$ est d‚Äôabord pass√©e √† travers deux couches lin√©aires avec les poids $W_1$ et $W_2$, suivies des biais $b_1$ et $b_2$. La fonction d‚Äôactivation ReLU, souvent repr√©sent√©e par la fonction $\\max$, est ici remplac√©e par la fonction GeLU.\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2\n",
    "$$\n",
    "\n",
    "On peut interpr√©ter ce bloc comme traitant ce que le bloc d‚Äôattention multi-t√™te a produit, puis projetant ces nouvelles repr√©sentations de tokens dans un espace que le bloc suivant pourra exploiter plus efficacement. G√©n√©ralement, la premi√®re couche est tr√®s large, environ 2 √† 8 fois la taille des repr√©sentations de tokens. Cette architecture facilite la parall√©lisation des calculs pour une couche unique plus large pendant l‚Äôentra√Ænement, plut√¥t que de parall√©liser un bloc feedforward compos√© de plusieurs couches. Cela permet d‚Äôajouter plus de complexit√© tout en gardant un entra√Ænement et une inf√©rence optimis√©s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8mvJkVdRdZE"
   },
   "outputs": [],
   "source": [
    "# @title Impl√©mentation du code pour un r√©seau de neurones √† propagation avant (Ex√©cutez-moi !)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Un MLP √† 2 couches qui √©largit puis r√©duit l'entr√©e.\n",
    "\n",
    "    Args:\n",
    "      widening_factor [optionnel, d√©faut=4]: La taille de la couche cach√©e sera d_model * widening_factor.\n",
    "    \"\"\"\n",
    "    # widening_factor contr√¥le √† quel point la dimension d'entr√©e est √©tendue dans la premi√®re couche.\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    # init_scale contr√¥le le facteur de mise √† l'√©chelle pour l'initialisation des poids.\n",
    "    init_scale: float = 0.25\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Obtenir la taille de la derni√®re dimension de l'entr√©e (taille de l'embedding).\n",
    "        d_m = x.shape[-1]\n",
    "\n",
    "        # Calculer la taille de la premi√®re couche en multipliant la taille de l'embedding par le facteur d'√©largissement.\n",
    "        layer1_size = self.widening_factor * d_m\n",
    "\n",
    "        # Initialiser les poids pour les deux couches en utilisant un initialiseur de mise √† l'√©chelle de variance.\n",
    "        initializer = nn.initializers.variance_scaling(\n",
    "            scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
    "        )\n",
    "\n",
    "        # D√©finir la premi√®re couche dense, qui √©tend la taille de l'entr√©e.\n",
    "        layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
    "\n",
    "        # D√©finir la deuxi√®me couche dense, qui r√©duit la taille pour revenir √† la dimension d'origine.\n",
    "        layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
    "\n",
    "        # Appliquer la premi√®re couche dense suivie d'une fonction d'activation GELU.\n",
    "        x = jax.nn.gelu(layer1(x))\n",
    "\n",
    "        # Appliquer la deuxi√®me couche dense pour projeter les donn√©es vers leur dimension d'origine.\n",
    "        x = layer2(x)\n",
    "\n",
    "        # Retourner la sortie finale.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2Us0NGFRUPn"
   },
   "source": [
    "#### Bloc Add and Norm (Addition et Normalisation) <Font Color = 'Orange'> D√©butant </font>\n",
    "\n",
    "<div style = \"Display: flex; align-items: Centre; justify-content: Centre; √©cart: 40px;\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1lj8pqO6ttjbcTRUEW1rbtRlueiLPxoSr\" alt=\"Feed Forward Neural Network\" width=\"300\"/>\n",
    "  <img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-VmAFgbRkb3"
   },
   "source": [
    "Pour permettre aux transformeurs d‚Äôaller plus en profondeur, les connexions r√©siduelles sont cruciales car elles facilitent la circulation des gradients dans le r√©seau. Pour la normalisation, on utilise la **normalisation de couche** (layer norm). Celle-ci normalise chaque vecteur de token ind√©pendamment dans le batch. Il a √©t√© observ√© que normaliser ces vecteurs am√©liore la convergence et la stabilit√© des transformeurs.\n",
    "\n",
    "La normalisation de couche comporte deux param√®tres apprenables, `scale` et `bias`, qui rescalent la valeur normalis√©e. Pour chaque token d‚Äôentr√©e dans un batch, on calcule la moyenne $\\mu_i$ et la variance $\\sigma_i^2$. Puis on normalise le token par :\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "Ensuite, $\\hat{x}$ est rescal√© avec le `scale` appris $\\gamma$ et le `bias` $\\beta$ selon :\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta = \\mathrm{LN}_{\\gamma, \\beta}(x_i)\n",
    "$$\n",
    "\n",
    "Ainsi, notre bloc **Add & Norm** peut se repr√©senter par :\n",
    "\n",
    "$$\n",
    "\\mathrm{LN}(x + f(x))\n",
    "$$\n",
    "\n",
    "o√π $f(x)$ est soit un bloc MLP soit un bloc MHA (Multi-Head Attention).\n",
    "\n",
    "Pour impl√©menter ce bloc Add & Norm, on d√©finit un module Flax qui prend en entr√©e les donn√©es originales et les donn√©es trait√©es, les additionne, puis applique `flax.linen.nn.LayerNorm` sur la derni√®re dimension pour normaliser le r√©sultat. Cela permet de stabiliser l‚Äôentra√Ænement en standardisant la repr√©sentation somm√©e.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbbBqhEORmKz"
   },
   "outputs": [],
   "source": [
    "# @title Impl√©mentation du code pour le bloc Ajouter et Normaliser (Ex√©cutez-moi !)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"Un bloc qui impl√©mente l'op√©ration 'Ajouter et Normaliser'\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, processed_x):\n",
    "        # √âtape 1 : Ajouter l'entr√©e originale (x) √† l'entr√©e trait√©e (processed_x).\n",
    "        added = x + processed_x\n",
    "\n",
    "        # √âtape 2 : Appliquer la normalisation de couche (LayerNorm) au r√©sultat de l'addition.\n",
    "        # - LayerNorm aide √† stabiliser et √† am√©liorer le processus d'entra√Ænement en normalisant la sortie.\n",
    "        # - reduction_axes=-1 indique que la normalisation est appliqu√©e sur la derni√®re dimension (typiquement la dimension de l'embedding).\n",
    "        # - use_scale=True et use_bias=True permettent √† la couche d'apprendre des param√®tres de mise √† l'√©chelle et de biais pour un ajustement fin suppl√©mentaire.\n",
    "        normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
    "\n",
    "        # Retourner le r√©sultat normalis√©.\n",
    "        return normalised(added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Z_7oRRRqPg"
   },
   "source": [
    "### Construire le d√©codeur de transformateur / llm <font color = 'vert'> interm√©diaire </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2xafT-jRtct"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
    "\n",
    "La majeure partie du travail pr√©paratoire est faite. Nous avons construit le bloc d‚Äôencodage positionnel, le bloc MHA, le bloc feed-forward et le bloc add\\&norm.\n",
    "\n",
    "La seule partie restante est de passer les entr√©es √† chaque bloc d√©codeur en appliquant le bloc MHA masqu√© (masked MHA) pr√©sent dans les blocs d√©codeurs.\n",
    "\n",
    "**T√¢che de code :** Impl√©mentez un module FLAX qui r√©alise la structure suivante pour le bloc d√©codeur :\n",
    "\n",
    "$$\n",
    "\\text{FFN} \\big( \\mathrm{Norm}(\\mathrm{MHA}(\\mathrm{Norm}(X))) \\big)\n",
    "$$\n",
    "\n",
    "Autrement dit, coder un module FLAX qui applique dans cet ordre :\n",
    "\n",
    "1. Normalisation sur l‚Äôentr√©e $X$\n",
    "2. Attention multi-t√™te masqu√©e (Masked MHA)\n",
    "3. Addition et normalisation\n",
    "4. R√©seau feed-forward (FFN)\n",
    "5. Addition et normalisation finale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JR03oeHRrZM"
   },
   "outputs": [],
   "source": [
    "#@title Impl√©mentation du bloc d√©codeur\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloc d√©codeur de transformateur.\n",
    "\n",
    "    Args:\n",
    "        num_heads: Le nombre de t√™tes d'attention dans le bloc d'attention multi-t√™te (MHA).\n",
    "        d_m: La taille des embeddings de token.\n",
    "        widening_factor: Le facteur par lequel la taille de la couche cach√©e est √©tendue dans le MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    num_heads: int\n",
    "    d_m: int\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialiser le bloc d'attention multi-t√™te (MHA)\n",
    "        self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
    "\n",
    "        # Initialiser les blocs AddNorm pour les connexions r√©siduelles et la normalisation\n",
    "        self.add_norm1 = AddNorm()  # Premier bloc AddNorm apr√®s MHA\n",
    "        self.add_norm2 = AddNorm()  # Deuxi√®me bloc AddNorm apr√®s le MLP\n",
    "\n",
    "        # Initialiser le bloc FeedForward (MLP) qui traite les donn√©es apr√®s l'attention\n",
    "        self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
    "\n",
    "    def __call__(self, X, mask=None, return_att_weight=True):\n",
    "        \"\"\"\n",
    "        Passe avant √† travers le DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            X: Lot de tokens d'entr√©e aliment√©s dans le d√©codeur, de forme [B, T_decoder, d_m]\n",
    "            mask [optionnel, d√©faut=None]: Masque pour contr√¥ler les positions que l'attention est autoris√©e √† prendre en compte, de forme [T_decoder, T_decoder].\n",
    "            return_att_weight [optionnel, d√©faut=True]: Si True, retourne les poids d'attention avec la sortie.\n",
    "\n",
    "        Returns:\n",
    "            Si return_att_weight est True, retourne un tuple (X, attention_weights_1).\n",
    "            Sinon, retourne les repr√©sentations de token trait√©es X.\n",
    "        \"\"\"\n",
    "\n",
    "        # Appliquer l'attention multi-t√™te (Multi-Head Attention) aux tokens d'entr√©e (X) avec un masquage optionnel\n",
    "        attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
    "\n",
    "        # Appliquer le premier bloc AddNorm (ajoute l'entr√©e originale X et normalise)\n",
    "        X = self.add_norm1(X, attention)\n",
    "\n",
    "        # Passer le r√©sultat √† travers le bloc FeedForward (MLP) pour traiter davantage les donn√©es\n",
    "        projection = self.MLP(X)\n",
    "\n",
    "        # Appliquer le deuxi√®me bloc AddNorm (ajoute l'entr√©e de l'√©tape pr√©c√©dente et normalise)\n",
    "        X = self.add_norm2(X, projection)\n",
    "\n",
    "        # Retourner la sortie finale X, et optionnellement les poids d'attention\n",
    "        return (X, attention_weights_1) if return_att_weight else X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwyJ7wfvR3iB"
   },
   "source": [
    "Ensuite, nous allons tout assembler, en ajoutant les encodages de position ainsi que l'empilement de plusieurs blocs de transformateurs et l'ajout de notre couche de pr√©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOjE40uJR5sU"
   },
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Mod√®le de transformateur compos√© de plusieurs couches de blocs d√©codeurs.\n",
    "\n",
    "    Args:\n",
    "        num_heads: Nombre de t√™tes d'attention dans chaque bloc d'attention multi-t√™te (MHA).\n",
    "        num_layers: Nombre de blocs d√©codeurs dans le mod√®le.\n",
    "        d_m: Dimensionnalit√© des embeddings de token.\n",
    "        vocab_size: Taille du vocabulaire (nombre de tokens uniques).\n",
    "        widening_factor: Facteur pour lequel la taille de la couche cach√©e est √©tendue dans le MLP.\n",
    "    \"\"\"\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    d_m: int\n",
    "    vocab_size: int\n",
    "    widening_factor: int = 4\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialiser une liste de blocs d√©codeurs, un pour chaque couche du mod√®le\n",
    "        self.blocks = [\n",
    "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        # Initialiser une couche d'embedding pour convertir les IDs de token en embeddings de token\n",
    "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
    "\n",
    "        # Initialiser une couche dense pour pr√©dire le prochain token de la s√©quence\n",
    "        self.pred_layer = nn.Dense(self.vocab_size)\n",
    "\n",
    "    def __call__(self, X, mask=None, return_att_weights=False):\n",
    "        \"\"\"\n",
    "        Passe avant √† travers le mod√®le LLM.\n",
    "\n",
    "        Args:\n",
    "            X: Lot d'IDs de token d'entr√©e, de forme [B, T_decoder] o√π B est la taille du lot et T_decoder est la longueur de la s√©quence.\n",
    "            mask [optionnel, d√©faut=None]: Masque pour contr√¥ler les positions sur lesquelles l'attention peut se concentrer, de forme [T_decoder, T_decoder].\n",
    "            return_att_weights [optionnel, d√©faut=False]: Si True, retourne les poids d'attention.\n",
    "\n",
    "        Returns:\n",
    "            logits: Les probabilit√©s pr√©dites pour chaque token du vocabulaire.\n",
    "            Si return_att_weights est True, retourne √©galement les poids d'attention.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convertir les IDs de token en embeddings (forme [B, T_decoder, d_m])\n",
    "        X = self.embedding(X)\n",
    "\n",
    "        # Obtenir la longueur de la s√©quence de l'entr√©e\n",
    "        sequence_len = X.shape[-2]\n",
    "\n",
    "        # G√©n√©rer des encodages de position et les ajouter aux embeddings de token\n",
    "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
    "        X = X + positions\n",
    "\n",
    "        # Initialiser une liste pour stocker les poids d'attention si n√©cessaire\n",
    "        if return_att_weights:\n",
    "            att_weights = []\n",
    "\n",
    "        # Passer les embeddings √† travers chaque bloc d√©codeur s√©quentiellement\n",
    "        for block in self.blocks:\n",
    "            out = block(X, mask, return_att_weights)\n",
    "            if return_att_weights:\n",
    "                # Si les poids d'attention sont retourn√©s, d√©compresser la sortie\n",
    "                X = out[0]\n",
    "                att_weights.append(out[1])\n",
    "            else:\n",
    "                # Sinon, mettre √† jour l'entr√©e pour le prochain bloc\n",
    "                X = out\n",
    "\n",
    "        # Appliquer une couche dense suivie d'un log softmax pour obtenir les logits (probabilit√©s de token pr√©dites)\n",
    "        logits = nn.log_softmax(self.pred_layer(X))\n",
    "\n",
    "        # Retourner les logits, et optionnellement, les poids d'attention\n",
    "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAhAuP_wR9ml"
   },
   "source": [
    "Si tout est correct, si nous ex√©cutons le code ci-dessous, tout devrait s'ex√©cuter sans aucun probl√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvnKc43XR_c0"
   },
   "outputs": [],
   "source": [
    "# D√©finir les dimensions du mod√®le et les formes des entr√©es\n",
    "batch_size = 18\n",
    "sequence_length = 32\n",
    "embedding_dim = 16\n",
    "num_decoder_layers = 8\n",
    "vocab_size = 25670\n",
    "\n",
    "# Initialiser le mod√®le de langage\n",
    "llm = LLM(\n",
    "    num_heads=1,\n",
    "    num_layers=1,\n",
    "    d_m=embedding_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    widening_factor=4\n",
    ")\n",
    "\n",
    "# Cr√©er un masque d'attention triangulaire inf√©rieur pour un d√©codage causal\n",
    "causal_mask = jnp.tril(np.ones((sequence_length, sequence_length)))\n",
    "\n",
    "# G√©n√©rer des IDs de token d'entr√©e al√©atoires\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "input_token_ids = jax.random.randint(rng_key, [batch_size, sequence_length], 0, vocab_size)\n",
    "\n",
    "# Initialiser les param√®tres du mod√®le\n",
    "model_params = llm.init(rng_key, input_token_ids, mask=causal_mask)\n",
    "\n",
    "# Ex√©cuter le mod√®le et extraire les logits et les poids d'attention du d√©codeur\n",
    "logits, decoder_attention_weights = llm.apply(\n",
    "    model_params,\n",
    "    input_token_ids,\n",
    "    mask=causal_mask,\n",
    "    return_att_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8vowzz-SC2q"
   },
   "source": [
    "En tant que v√©rification finale de la sant√© mentale, nous pouvons confirmer que nos poids d'attention fonctionnent correctement.Comme le montre la figure ci-dessous, les poids d'attention du d√©codeur ne se concentrent que sur les jetons pr√©c√©dents, comme pr√©vu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NbvRSNzSF0H"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.suptitle(\"LLM attention weights\")\n",
    "sns.heatmap(decoder_attention_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nsFaXhdSKZG"
   },
   "source": [
    "### Entra√Ænement de votre LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6BUm34sSRJH"
   },
   "source": [
    "#### Objectif de formation [<Font Color = 'Green'> Interm√©diaire </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jJ2TTwjSV-u"
   },
   "source": [
    "Une phrase n‚Äôest rien d‚Äôautre qu‚Äôune suite de mots. Un LLM vise √† pr√©dire le mot suivant en tenant compte du contexte actuel, c‚Äôest-√†-dire des mots qui l‚Äôont pr√©c√©d√©.\n",
    "\n",
    "Voici l‚Äôid√©e principale :\n",
    "\n",
    "Pour calculer la probabilit√© qu‚Äôune phrase compl√®te ¬´ mot1, mot2, ..., dernier mot ¬ª apparaisse dans un contexte donn√© $c$, on d√©compose la phrase en mots individuels et on consid√®re la probabilit√© de chaque mot sachant les mots qui le pr√©c√®dent. Ces probabilit√©s individuelles sont ensuite multipli√©es entre elles :\n",
    "\n",
    "$$\n",
    "\\text{Probabilit√© de la phrase} = \\text{Probabilit√© de mot}_1 \\times \\text{Probabilit√© de mot}_2 \\times \\ldots \\times \\text{Probabilit√© du dernier mot}\n",
    "$$\n",
    "\n",
    "Cette m√©thode revient √† construire un r√©cit morceau par morceau en se basant sur ce qui a √©t√© racont√© avant.\n",
    "\n",
    "Math√©matiquement, cela s‚Äôexprime par la vraisemblance (probabilit√©) d‚Äôune s√©quence de mots $y_1, y_2, \\ldots, y_n$ dans un contexte donn√© $c$, calcul√©e en multipliant les probabilit√©s de chaque mot $y_t$ conditionn√©es aux mots pr√©c√©dents $(y_{<t})$ et au contexte $c$ :\n",
    "\n",
    "$$\n",
    "P(y_1, y_2, \\ldots, y_n \\mid c) = \\prod_{t=1}^{n} P(y_t \\mid y_{<t}, c)\n",
    "$$\n",
    "\n",
    "Ici, $y_{<t}$ d√©signe la s√©quence $y_1, y_2, \\ldots, y_{t-1}$, tandis que $c$ repr√©sente le contexte.\n",
    "\n",
    "C‚Äôest analogue √† assembler un puzzle o√π la pi√®ce suivante est plac√©e en fonction de celles d√©j√† pos√©es.\n",
    "\n",
    "---\n",
    "\n",
    "Gardez √† l‚Äôesprit que lors de l‚Äôentra√Ænement d‚Äôun transformeur, on ne travaille pas avec des mots mais avec des tokens. Pendant le processus d‚Äôentra√Ænement, les param√®tres du mod√®le sont ajust√©s en calculant la perte d‚Äôentropie crois√©e entre le token pr√©dit et le token correct, puis en effectuant la r√©tropropagation. La perte au temps $t$ est calcul√©e ainsi :\n",
    "\n",
    "$$\n",
    "\\text{Loss}_t = - \\sum_{w \\in V} y_t \\log(\\hat{y}_t)\n",
    "$$\n",
    "\n",
    "o√π $w \\in V$ correspond √† chaque mot $w$ dans le vocabulaire $V$.\n",
    "\n",
    "Ici, $y_t$ est le token r√©el au temps $t$, et $\\hat{y}_t$ est le token pr√©dit par le mod√®le au m√™me instant. La perte sur toute la phrase est ensuite calcul√©e comme :\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{phrase}} = \\frac{1}{n} \\sum_{t=1}^n \\text{Loss}_t\n",
    "$$\n",
    "\n",
    "avec $n$ la longueur de la s√©quence.\n",
    "\n",
    "Ce processus it√©ratif affine progressivement les capacit√©s pr√©dictives du mod√®le.\n",
    "\n",
    "---\n",
    "\n",
    "**T√¢che de code :** Impl√©mentez la fonction de perte d‚Äôentropie crois√©e (cross-entropy loss) ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeAWLPJbSVLM"
   },
   "outputs": [],
   "source": [
    "def sequence_loss_fn(logits, targets):\n",
    "  '''\n",
    "  Compute the cross-entropy loss between predicted token ID and true ID.\n",
    "\n",
    "  Args:\n",
    "    logits: An array of shape [batch_size, sequence_length, vocab_size]\n",
    "    targets: The targets we are trying to predict\n",
    "\n",
    "  Returns:\n",
    "    loss: A scalar value representing the mean batch loss\n",
    "  '''\n",
    "\n",
    "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
    "  assert logits.shape == target_labels.shape\n",
    "\n",
    "  mask = jnp.greater(targets, 0)\n",
    "\n",
    "# Astuce¬†: Calculez la perte d'entropie crois√©e en appliquant d'abord `jax.nn.log_softmax(logits)`\n",
    "# pour obtenir les probabilit√©s logarithmiques de chaque classe. Multipliez ensuite ces probabilit√©s logarithmiques\n",
    "# par `target_labels` pour vous concentrer sur la probabilit√© de la classe concern√©e. Additionnez ce r√©sultat\n",
    "# le long du dernier axe pour obtenir la perte pour chaque jeton. Enfin, appliquez le masque √† la perte,\n",
    "# additionnez les pertes masqu√©es et normalisez par le nombre de jetons non remplis.\n",
    "  loss = ...# FINISH ME\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwp5yuhrSdgB"
   },
   "outputs": [],
   "source": [
    "# @title Ex√©cutez-moi pour tester votre code\n",
    "VOCAB_SIZE = 25670\n",
    "targets = jnp.array([[0, 2, 0]])\n",
    "key = jax.random.PRNGKey(42)\n",
    "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
    "loss = sequence_loss_fn(X, targets)\n",
    "real_loss = jnp.array(10.966118)\n",
    "\n",
    "try:\n",
    "  jnp.allclose(real_loss, loss)\n",
    "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
    "except:\n",
    "  print(\"Not returning the correct value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIPqzOq1Sg9p"
   },
   "outputs": [],
   "source": [
    "# @title R√©ponse √† la t√¢che de code (Essayez avant de regarder !)\n",
    "def sequence_loss_fn(logits, targets):\n",
    "    \"\"\"Compute the sequence loss between predicted logits and target labels.\n",
    "       (Calcule la perte de s√©quence entre les logits pr√©dits et les √©tiquettes cibles.)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir les indices cibles en vecteurs one-hot.\n",
    "    # Chaque √©tiquette cible est convertie en un vecteur one-hot de taille VOCAB_SIZE.\n",
    "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
    "\n",
    "    # S'assurer que la forme des logits correspond √† celle des cibles encod√©es en one-hot.\n",
    "    # C'est important car nous devons calculer la perte sur des dimensions correspondantes.\n",
    "    assert logits.shape == target_labels.shape\n",
    "\n",
    "    # Cr√©er un masque qui ignore les tokens de remplissage lors du calcul de la perte.\n",
    "    # Le masque est Vrai (1) lorsque la valeur cible est sup√©rieure √† 0, et Faux (0) sinon.\n",
    "    mask = jnp.greater(targets, 0)\n",
    "\n",
    "    # Calculer la perte par entropie crois√©e pour chaque token.\n",
    "    # L'entropie crois√©e est calcul√©e comme la probabilit√© logarithmique n√©gative de la classe correcte.\n",
    "    # jax.nn.log_softmax(logits) nous donne les probabilit√©s logarithmiques de chaque classe.\n",
    "    # On multiplie par les target_labels pour s√©lectionner la probabilit√© logarithmique de la classe correcte.\n",
    "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
    "\n",
    "    # Appliquer le masque √† la perte pour ignorer les positions de remplissage et sommer les pertes.\n",
    "    # On normalise ensuite la perte totale par le nombre de tokens non-remplis.\n",
    "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jp_1cbQSnzq"
   },
   "source": [
    "#### Mod√®les d'entra√Ænement [<font color = 'vert'> interm√©diaire </font>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWx1nrZZSmZj"
   },
   "source": [
    "Dans la section suivante, nous d√©finissons tous les processus n√©cessaires pour former le mod√®le en utilisant l'objectif d√©crit ci-dessus.Une grande partie de cela est maintenant le travail requis pour faire une formation en utilisant le lin.\n",
    "\n",
    "Ci-dessous, nous rassemblons l'ensemble de donn√©es et nous allons nous entra√Æner, qui est l'ensemble de donn√©es Shakespeare de Karpathy.Il n'est pas si important de comprendre ce code, donc soit ex√©cuter la cellule pour charger les donn√©es, soit afficher le code si vous voulez le comprendre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MyUYK9DSqbo"
   },
   "outputs": [],
   "source": [
    "# @title Cr√©er un jeu de donn√©es Shakespeare et un it√©rateur (optionnel, mais ex√©cutez la cellule)\n",
    "\n",
    "# Astuce pour √©viter les erreurs lors du t√©l√©chargement de tinyshakespeare.\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
    "\n",
    "class WordBasedAsciiDatasetForLLM:\n",
    "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\n",
    "       (Jeu de donn√©es en m√©moire d‚Äôun fichier ASCII unique pour un mod√®le de type langage.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
    "        \"\"\"Load a single-file ASCII dataset in memory.\n",
    "           (Charger un jeu de donn√©es ASCII dans la m√©moire.)\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "        # Tokeniser en divisant le texte en mots\n",
    "        words = corpus.split()\n",
    "        self.vocab_size = len(set(words))  # Nombre de mots uniques\n",
    "\n",
    "        # Cr√©er une correspondance entre les mots et des identifiants uniques\n",
    "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
    "\n",
    "        # Stocker la correspondance inverse des identifiants vers les mots\n",
    "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
    "\n",
    "        # Convertir les mots du corpus en leurs identifiants correspondants\n",
    "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
    "\n",
    "        crop_len = sequence_length + 1\n",
    "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
    "        if ragged:\n",
    "            corpus = corpus[:-ragged]\n",
    "        corpus = corpus.reshape([-1, crop_len])\n",
    "\n",
    "        if num_batches < 10:\n",
    "            raise ValueError(\n",
    "                f\"Only {num_batches} batches; consider a shorter \"\n",
    "                \"sequence or a smaller batch.\"\n",
    "            )\n",
    "            # Seulement {num_batches} lots; envisagez une s√©quence plus courte\n",
    "            # ou une taille de lot plus petite.\n",
    "\n",
    "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
    "            corpus, batch_size * 10\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Yield next mini-batch.\n",
    "           (Produire le mini-lot suivant.)\n",
    "        \"\"\"\n",
    "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
    "        batch = np.stack(batch)\n",
    "        # Cr√©er les paires observation/cible pour le mod√®le de langage.\n",
    "        return dict(\n",
    "            input=batch[:, :-1], target=batch[:, 1:]\n",
    "        )\n",
    "\n",
    "    def ids_to_words(self, ids):\n",
    "        \"\"\"Convert a sequence of word IDs to words.\n",
    "           (Convertir une s√©quence d'identifiants de mots en mots.)\n",
    "        \"\"\"\n",
    "        return [self.id_to_word[id] for id in ids]\n",
    "\n",
    "    @staticmethod\n",
    "    def _infinite_shuffle(iterable, buffer_size):\n",
    "        \"\"\"Infinitely repeat and shuffle data from iterable.\n",
    "           (R√©p√©ter et m√©langer ind√©finiment les donn√©es provenant d'un it√©rable.)\n",
    "        \"\"\"\n",
    "        ds = itertools.cycle(iterable)\n",
    "        buf = [next(ds) for _ in range(buffer_size)]\n",
    "        random.shuffle(buf)\n",
    "        while True:\n",
    "            item = next(ds)\n",
    "            idx = random.randint(0, buffer_size - 1)  # Inclusif.\n",
    "            result, buf[idx] = buf[idx], item\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJf97wcmSv2y"
   },
   "source": [
    "Permet maintenant de voir comment nos donn√©es sont structur√©es pour la formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3UEb_1hSzPc"
   },
   "outputs": [],
   "source": [
    "# √âchantillonner et visualiser les donn√©es\n",
    "batch_size = 2\n",
    "seq_length = 32\n",
    "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
    "\n",
    "batch = next(train_dataset)\n",
    "\n",
    "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
    "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
    "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
    "    print(\"ASCII:\", obs)\n",
    "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
    "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
    "    print(\"ASCII:\", target)\n",
    "\n",
    "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
    "\n",
    "VOCAB_SIZE = train_dataset.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVKnUqggS5ru"
   },
   "source": [
    "Ensuite, entra√Ænons notre LLM et voyons comment elle fonctionne dans la production de texte shakespearien.Tout d'abord, nous d√©finirons ce qui se passe pour chaque √©tape de formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMNCtFxxS91p"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
    "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
    "    \"\"\"\n",
    "    Effectue une seule √©tape d'entra√Ænement.\n",
    "\n",
    "    Args:\n",
    "        params: Les param√®tres actuels du mod√®le.\n",
    "        optimizer_state: L'√©tat actuel de l'optimiseur.\n",
    "        batch: Un dictionnaire contenant les donn√©es d'entr√©e et les √©tiquettes cibles pour le lot.\n",
    "        apply_fn: La fonction utilis√©e pour appliquer le mod√®le aux entr√©es.\n",
    "        update_fn: La fonction utilis√©e pour mettre √† jour les param√®tres du mod√®le en fonction des gradients.\n",
    "\n",
    "    Returns:\n",
    "        Les param√®tres mis √† jour, l'√©tat de l'optimiseur mis √† jour, et la perte calcul√©e pour le lot.\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        # Obtenir la longueur de la s√©quence (T) √† partir des donn√©es d'entr√©e.\n",
    "        T = batch['input'].shape[1]\n",
    "\n",
    "        # Appliquer le mod√®le aux donn√©es d'entr√©e, en utilisant un masque triangulaire inf√©rieur pour imposer la causalit√©.\n",
    "        # jnp.tril(np.ones((T, T))) cr√©e une matrice triangulaire inf√©rieure de uns.\n",
    "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
    "\n",
    "        # Calculer la perte entre les logits pr√©dits et les √©tiquettes cibles.\n",
    "        loss = sequence_loss_fn(logits, batch['target'])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Calculer la perte et ses gradients par rapport aux param√®tres.\n",
    "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    # Mettre √† jour l'√©tat de l'optimiseur et calculer les mises √† jour des param√®tres en fonction des gradients.\n",
    "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
    "\n",
    "    # Appliquer les mises √† jour aux param√®tres.\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    # Retourner les param√®tres mis √† jour, l'√©tat de l'optimiseur et la perte pour le lot.\n",
    "    return params, optimizer_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12uUhzgcTRBM"
   },
   "source": [
    "Ensuite, nous initialisons notre optimiseur et notre mod√®le.N'h√©sitez pas √† jouer avec les hyperparam√®tres pendant la pratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NJjGy-qTSk8"
   },
   "outputs": [],
   "source": [
    "# D√©finir tous les hyperparam√®tres\n",
    "d_model = 128            # Dimension des embeddings de token (d_m)\n",
    "num_heads = 4            # Nombre de t√™tes d'attention dans l'attention multi-t√™te\n",
    "num_layers = 1           # Nombre de blocs d√©codeurs dans le mod√®le\n",
    "widening_factor = 2      # Facteur pour √©largir la taille de la couche cach√©e dans le MLP\n",
    "LR = 2e-3                # Taux d'apprentissage pour l'optimiseur\n",
    "batch_size = 32          # Nombre d'√©chantillons par lot d'entra√Ænement\n",
    "seq_length = 64          # Longueur de chaque s√©quence d'entr√©e (nombre de tokens)\n",
    "\n",
    "# Configurer les donn√©es d'entra√Ænement\n",
    "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
    "vocab_size = train_dataset.vocab_size  # Obtenir la taille du vocabulaire √† partir de l'ensemble de donn√©es\n",
    "batch = next(train_dataset)            # Obtenir le premier lot de donn√©es d'entr√©e\n",
    "\n",
    "# D√©finir la cl√© du g√©n√©rateur de nombres al√©atoires pour l'initialisation du mod√®le\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Initialiser le mod√®le LLM avec les hyperparam√®tres sp√©cifi√©s\n",
    "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
    "\n",
    "# Cr√©er un masque causal pour s'assurer que le mod√®le ne porte attention qu'aux tokens pr√©c√©dents\n",
    "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
    "\n",
    "# Initialiser les param√®tres du mod√®le en utilisant le premier lot de donn√©es d'entr√©e et le masque\n",
    "params = llm.init(rng, batch['input'], mask)\n",
    "\n",
    "# Configurer l'optimiseur en utilisant l'algorithme d'optimisation Adam avec le taux d'apprentissage sp√©cifi√©\n",
    "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
    "optimizer_state = optimizer.init(params)  # Initialiser l'√©tat de l'optimiseur avec les param√®tres du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdwUHcn6TZQT"
   },
   "source": [
    "Maintenant, nous nous entra√Ænons!Cela prendra quelques minutes .. Pendant qu'il s'entra√Æne, avez-vous encore salu√© votre voisin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlk7laQVTYNw"
   },
   "outputs": [],
   "source": [
    "plotlosses = PlotLosses()\n",
    "\n",
    "MAX_STEPS = 3500\n",
    "LOG_EVERY = 32\n",
    "losses = []\n",
    "VOCAB_SIZE = 25670\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "for step in range(MAX_STEPS):\n",
    "    batch = next(train_dataset)\n",
    "    params, optimizer_state, loss = train_step(\n",
    "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
    "    losses.append(loss)\n",
    "    if step % LOG_EVERY == 0:\n",
    "        loss_ = jnp.array(losses).mean()\n",
    "        plotlosses.update(\n",
    "            {\n",
    "                \"loss\": loss_,\n",
    "            }\n",
    "        )\n",
    "        plotlosses.send()\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE5N87UWT_uK"
   },
   "source": [
    "#### Inspectant le LLM Entrain√© [<font color = 'orange'> d√©butant </font>]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGBOSUgdUE3J"
   },
   "source": [
    "**Rappel :** n‚Äôoubliez pas d‚Äôex√©cuter tout le code pr√©sent√© jusqu‚Äô√† pr√©sent dans cette section avant de lancer les cellules ci-dessous !\n",
    "\n",
    "G√©n√©rons maintenant du texte et voyons les performances de notre mod√®le. NE PAS ARR√äTER LA CELLULE UNE FOIS QU‚ÄôELLE EST EN COURS D‚ÄôEX√âCUTION, CELA POURRAIT FAIRE PLANTER LA SESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4XVDaTHUICU"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(2, ))\n",
    "def generate_prediction(params, input, apply_fn):\n",
    "  logits = apply_fn(params, input)\n",
    "  argmax_out = jnp.argmax(logits, axis=-1)\n",
    "  return argmax_out[0][-1].astype(int)\n",
    "\n",
    "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
    "    '''\n",
    "    Obtenir la sortie du mod√®le\n",
    "    '''\n",
    "\n",
    "    prompt = \"Love\"\n",
    "    print(prompt, end=\"\")\n",
    "    tokens = prompt.split()\n",
    "\n",
    "    # pr√©dire et ajouter\n",
    "    for i in range(15):\n",
    "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
    "      prediction = generate_prediction(params, input, llm.apply)\n",
    "      prediction = id_2_word[int(prediction)]\n",
    "      tokens.append(prediction)\n",
    "      print(\" \"+prediction, end=\"\")\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "id_2_word = train_dataset.id_to_word\n",
    "word_2_id = train_dataset.word_to_id\n",
    "\n",
    "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_7rSlfaUOeD"
   },
   "source": [
    "Enfin, nous avons impl√©ment√© tout ce qui pr√©c√®de en s√©lectionnant l‚ÄôID du token avec la probabilit√© maximale d‚Äô√™tre correct. C‚Äôest ce qu‚Äôon appelle le d√©codage glouton (greedy decoding), car on ne prend que le token le plus probable. Cela a bien fonctionn√© dans ce cas, mais dans certaines situations, cette approche peut entra√Æner une d√©gradation des performances, notamment lorsqu‚Äôon cherche √† g√©n√©rer un texte r√©aliste.\n",
    "\n",
    "D‚Äôautres m√©thodes existent pour l‚Äô√©chantillonnage depuis le d√©codeur, parmi lesquelles l‚Äôalgorithme bien connu appel√© recherche par faisceau (beam search). Nous fournissons ci-dessous des ressources pour ceux qui souhaitent en apprendre davantage.\n",
    "\n",
    "[D√©codage glouton (Greedy Decoding)](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
    "\n",
    "[Recherche par faisceau (Beam Search)](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tixtBEtRPZ5n"
   },
   "source": [
    "## Mati√®re √† r√©flexion : Quel est le co√ªt lorsque vous discutez avec un LLM dans votre langue ?\n",
    "\n",
    "Avant de clore ce chapitre sur les Transformers, nous souhaitons vous inviter √† r√©fl√©chir √† ceci : quel est le co√ªt lorsque vous √©changez avec un LLM dans votre langue ?\n",
    "\n",
    "Le co√ªt d‚Äôinteraction avec un mod√®le de langage d√©pend du nombre de tokens dans votre message. En effet, les LLM facturent √† la token, pas au mot, √† la phrase ou au caract√®re.\n",
    "\n",
    "Par exemple :\n",
    "\n",
    "* **GPT-4.0-turbo** (en date de juin 2025) [co√ªte](https://openai.com/api/pricing/) **2 \\$ pour 1 million de tokens** en entr√©e.\n",
    "* **Gemma 2.5 Pro** (via l‚ÄôAPI Gemini, juin 2025) [co√ªte](https://ai.google.dev/gemini-api/docs/pricing) **1,25 \\$ pour 1 million de tokens**.\n",
    "\n",
    "Comme chaque mod√®le utilise une m√©thode de tokenisation diff√©rente, ils d√©composent les langues diff√©remment. Cela peut entra√Æner des co√ªts plus √©lev√©s pour certaines langues, m√™me lorsque la phrase signifie exactement la m√™me chose.\n",
    "\n",
    "### Calculons le co√ªt des tokens\n",
    "\n",
    "Si le co√ªt est de **2 \\$ pour 1 million de tokens**, voici comment le co√ªt √©volue :\n",
    "\n",
    "$$\n",
    "\\text{Co√ªt} = \\text{Nombre de tokens} \\times \\left(\\frac{2}{1{,}000{,}000}\\right)\n",
    "$$\n",
    "\n",
    "#### üí∞ Estimations exemples :\n",
    "\n",
    "| Nombre de tokens | Calcul                                      | Co√ªt (USD)     |\n",
    "| ---------------- | ------------------------------------------- | -------------- |\n",
    "| 10 tokens        | \\$10 \\times \\frac{2}{1{,}000{,}000}\\$       | **0,00002 \\$** |\n",
    "| 100 tokens       | \\$100 \\times \\frac{2}{1{,}000{,}000}\\$      | **0,0002 \\$**  |\n",
    "| 1 000 tokens     | \\$1000 \\times \\frac{2}{1{,}000{,}000}\\$     | **0,002 \\$**   |\n",
    "| 10 000 tokens    | \\$10{,}000 \\times \\frac{2}{1{,}000{,}000}\\$ | **0,02 \\$**    |\n",
    "\n",
    "Imaginez maintenant que vous g√©n√©rez ou traitez des millions de requ√™tes dans une langue locale qui se tokenise de mani√®re inefficace. Cela pourrait signifier d√©penser plus pour dire la m√™me chose, simplement parce que votre langue ne s‚Äôadapte pas bien au tokenizer utilis√© par le mod√®le de langage.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=1m0mCSEEuBxNzb8pJfMANqsvikRorBubE\" alt=\"ChatGPT Pricing\" width=\"800\"/>\n",
    "  <figcaption><em></em></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVTduxk4PdYC"
   },
   "source": [
    "#### üí∏ Combien co√ªte ma LLM ? ‚Äî Tokenisation en code\n",
    "\n",
    "**Configuration :**\n",
    "Nous allons tester comment diff√©rents mod√®les tokenisent la m√™me phrase dans plusieurs langues, en utilisant :\n",
    "\n",
    "* **GPT-2** (tokenizer g√©n√©ral pour l‚Äôanglais)\n",
    "* **Gemma** (tokenizer multilingue)\n",
    "* Un **tokenizer sp√©cifique √† une langue**\n",
    "\n",
    "Commen√ßons par **GPT-2**.\n",
    "\n",
    "> Pour tous les mod√®les compar√©s ci-dessous, nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlLkCsH4Amkt"
   },
   "outputs": [],
   "source": [
    "def token_cost(tokens: list, model_name: str):\n",
    "    \"\"\"\n",
    "    Fonction qui prend une liste de tokens et retourne le co√ªt du token pour le mod√®le donn√©.\n",
    "    \"\"\"\n",
    "    # co√ªt par token pour Gemma 2.5 Pro https://ai.google.dev/gemini-api/docs/pricing\n",
    "    # pour l'instant, supposer que tous les tokenizers co√ªtent la m√™me chose\n",
    "    cost_per_token = 2/1000000  # co√ªt par token pour GPT4.1\n",
    "\n",
    "    return len(tokens) * cost_per_token  # Gemma3 utilise un co√ªt fixe par token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wO5Y7_NPmqP"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "tokenizer = get_tokenizer(model_name)  # Tokenizer par d√©faut, peut √™tre chang√© au besoin\n",
    "sentence = \"This is a sample sentence for tokenization.\" #@param {type:\"string\"}\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(\"Phrase :\", sentence)\n",
    "print(\"Tokens :\",  tokens)\n",
    "print(\"IDs des tokens :\", token_ids)\n",
    "print(\"Nombre de tokens :\", len(tokens))\n",
    "print(\"Co√ªt du token :\", token_cost(tokens, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-Svrwz-PsEo"
   },
   "source": [
    "Next,\n",
    "- Essayez une m√™me phrase dans une langue diff√©rente, par exemple Swahili ou Yoruba\n",
    "- Observer et enregistrer le nombre de jetons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5qcyoRWPrBJ"
   },
   "outputs": [],
   "source": [
    "sentences = {\"English\": \"Welcome to the Indaba LLM tutorial happening in Kigali. Get ready to explore the world of LLMs.\",\n",
    "             \"German\": \"Willkommen zum Indaba LLM Tutorial in Kigali. Macht euch bereit die Welt von LLMs zu erkunden.\",\n",
    "             \"French\": \"Bienvenue au tutoriel Indaba sur les LLM qui se d√©roule √† Kigali. Pr√©parez-vous √† explorer le monde des LLM.\",\n",
    "             \"Lithuania\": \"Sveiki atvykƒô ƒØ Indaba LLM (Did≈æi≈≥j≈≥ Kalbini≈≥ Modeli≈≥) mokymus vykstanƒçius Kigalyje. Pasiruo≈°kite tyrinƒóti LLM pasaulƒØ.\",\n",
    "             \"Yoruba\": \"Kaab·ªç si ik·∫πk·ªç Indaba LLM ti n ·π£·∫πl·∫π ni Kigali. ·π¢etan lati ·π£awari agbaye ti LLMs.\",\n",
    "             \"Swahili\": \"Karibu kwenye mafunzo ya Indaba LLM yanayofanyika Kigali. Jitayarishe kuchunguza ulimwengu wa LLM.\",\n",
    "             \"Arabic\":  \".ŸÖÿ±ÿ≠ÿ®Ÿãÿß ÿ®ŸÉŸÖ ŸÅŸä Ÿàÿ±ÿ¥ÿ© ÿπŸÖŸÑ  ÿ≠ŸàŸÑ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ŸàŸäÿ© ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ÿßŸÑÿ™Ÿä ÿ™ŸèŸÇÿßŸÖ ŸÅŸä ŸÉŸäÿ∫ÿßŸÑŸä. ÿßÿ≥ÿ™ÿπÿØŸàÿß ŸÑÿßÿ≥ÿ™ŸÉÿ¥ÿßŸÅ ÿπÿßŸÑŸÖ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ŸàŸäÿ© ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©\",  # The arabic sentence is not a 1-1 translation\n",
    "             \"Kinyarwanda\": \"Murakaza neza mu isomo rya Indaba LLM riri kubera i Kigali. Mwitegure kuvumbura isi ya za LLM.\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhPM1MyDPw8N"
   },
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"   #@param [\"gpt2\", \"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
    "for language, sentence in sentences.items():\n",
    "    # Pour chaque langue, tokeniser la phrase et afficher les r√©sultats\n",
    "    tokens, token_ids = tokenize(sentence, model_name)\n",
    "    print(f\"Langue : {language}, Mod√®le : {model_name}\")\n",
    "    print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "    print(\"Phrase :\", sentence)\n",
    "    print(\"Tokens :\", tokens)\n",
    "    print(\"IDs des tokens :\", token_ids)\n",
    "    print(\"Nombre de tokens :\", len(tokens))\n",
    "    print(\"Co√ªt du token (mis √† l'√©chelle par 1000000) :\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "    print(\"-\" * 50)  # S√©parateur pour plus de clart√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD2dDhQ2P2cU"
   },
   "source": [
    "Avez-vous remarqu√© comment GPT-2 segmente le texte arabe caract√®re par caract√®re, souvent octet par octet, au lieu de saisir des unit√©s significatives ? C‚Äôest parce que GPT-2 a √©t√© principalement entra√Æn√© sur des donn√©es en anglais et n‚Äôa pas √©t√© optimis√© pour g√©rer l‚Äôarabe. Bien qu‚Äôil soit possible de tokenizer l‚Äôarabe avec le tokenizer de GPT-2, cela conduit g√©n√©ralement √† un nombre de tokens bien plus √©lev√© compar√© au m√™me contenu en anglais ‚Äî ce qui implique √©galement un co√ªt plus √©lev√©.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alop4gEjP48e"
   },
   "source": [
    "Essayons le tokenizer [Gemma 3](https://developers.googleblog.com/en/introducing-gemma3/#:~:text=Gemma%203%20uses%20a%20new,TPUs%20using%20the%20JAX%20Framework.). Il s‚Äôagit d‚Äôun nouveau tokenizer multilingue con√ßu pour prendre en charge plus de 140 langues.\n",
    "\n",
    "> Nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajJMBSYkP09h"
   },
   "outputs": [],
   "source": [
    "model_name = \"gemma3\"\n",
    "for language, sentence in sentences.items():\n",
    "    # Pour chaque langue, tokeniser la phrase et afficher les r√©sultats\n",
    "    tokens, token_ids = tokenize(sentence, model_name)\n",
    "    print(f\"Langue : {language}, Mod√®le : {model_name}\")\n",
    "    print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "    print(\"Phrase :\", sentence)\n",
    "    print(\"Tokens :\", tokens)\n",
    "    print(\"IDs des tokens :\", token_ids)\n",
    "    print(\"Nombre de tokens :\", len(tokens))\n",
    "    print(\"Co√ªt du token (mis √† l'√©chelle par 1000000) :\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "    print(\"-\" * 50)  # S√©parateur pour plus de clart√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8EekZSfP_Lz"
   },
   "source": [
    "Qu‚Äôen est-il de l‚Äôutilisation d‚Äôun tokenizer sp√©cifique √† une langue ? Par exemple, essayez `asafaya/bert-base-arabic` sur un texte en arabe ‚Äî il est con√ßu pour g√©rer bien mieux la structure et les nuances de la langue que les tokenizers g√©n√©ralistes. Remarquez comment le nombre de tokens ‚Äî et donc le co√ªt ‚Äî diminue consid√©rablement lorsque vous utilisez un tokenizer sp√©cifiquement adapt√© √† l‚Äôarabe ?\n",
    "\n",
    "> Nous utiliserons le co√ªt du token GPT-4.1 comme base de comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjzJYNccP9uZ"
   },
   "outputs": [],
   "source": [
    "model_name = \"asafaya/bert-base-arabic\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Obtenir la phrase en arabe du dictionnaire\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "clear_output()\n",
    "print(f\"Langue : {language}, Mod√®le : {model_name}\")\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "print(\"Phrase :\", sentence)\n",
    "print(\"Tokens :\", tokens)\n",
    "print(\"IDs des tokens :\", token_ids)\n",
    "print(\"Nombre de tokens :\", len(tokens))\n",
    "print(\"Co√ªt du token (mis √† l'√©chelle par 1000000) :\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "\n",
    "\n",
    "model_name = \"gemma3\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Obtenir la phrase en arabe du dictionnaire\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(f\"Langue : {language}, Mod√®le : {model_name}\")\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "print(\"Phrase :\", sentence)\n",
    "print(\"Tokens :\", tokens)\n",
    "print(\"IDs des tokens :\", token_ids)\n",
    "print(\"Nombre de tokens :\", len(tokens))\n",
    "print(\"Co√ªt du token (mis √† l'√©chelle par 1000000) :\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "language = \"Arabic\"\n",
    "sentence = sentences[language]  # Obtenir la phrase en arabe du dictionnaire\n",
    "tokens, token_ids = tokenize(sentence, model_name)\n",
    "print(f\"Langue : {language}, Mod√®le : {model_name}\")\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©\n",
    "print(\"Phrase :\", sentence)\n",
    "print(\"Tokens :\", tokens)\n",
    "print(\"IDs des tokens :\", token_ids)\n",
    "print(\"Nombre de tokens :\", len(tokens))\n",
    "print(\"Co√ªt du token (mis √† l'√©chelle par 1000000) :\", f'${token_cost(tokens, model_name) * 1000000:0.1f}')\n",
    "print(\"-\" * 50)  # S√©parateur pour plus de clart√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH-jn4JvQGJW"
   },
   "outputs": [],
   "source": [
    "# D√©finir les mod√®les que vous voulez comparer\n",
    "models = [\"asafaya/bert-base-arabic\", \"gemma3\", \"gpt2\"]\n",
    "costs = []\n",
    "language = \"Arabic\"  # Langue √† utiliser pour la comparaison des co√ªts\n",
    "# Calculer le co√ªt du token pour chaque mod√®le en utilisant le dictionnaire des phrases\n",
    "for model in models:\n",
    "    total_cost = 0\n",
    "    tokens, _ = tokenize(sentences[language], model)\n",
    "    total_cost = token_cost(tokens, model)\n",
    "    costs.append(total_cost * 1000000)  # Mettre √† l'√©chelle par 1 million pour l'affichage\n",
    "\n",
    "# Cr√©er le graphique √† barres\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(models, costs, color='k')\n",
    "plt.xlabel('Mod√®le')\n",
    "plt.ylabel('Co√ªt mis √† l\\'√©chelle (USD)')\n",
    "plt.title(f'Comparaison des co√ªts des mod√®les pour la langue {language}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwCo9941QRo2"
   },
   "source": [
    "#### üßµ Points cl√©s\n",
    "\n",
    "* **Soyez conscient de la mani√®re dont les LLM repr√©sentent votre langue**, surtout si vous utilisez des API commerciales. La fa√ßon dont votre texte est tokenis√© impacte directement le co√ªt.\n",
    "* Si vous entra√Ænez votre propre LLM, **portez une attention particuli√®re √† la tokenisation**. Vous pourriez vouloir **adapter le tokenizer √† votre langue** pour r√©duire le nombre de tokens et rendre la repr√©sentation plus compacte et efficace.\n",
    "* Des travaux r√©cents de **Cohere** explorent la cr√©ation d‚Äôun [**tokenizer universel**](https://arxiv.org/pdf/2506.10766) qui fonctionne bien pour plusieurs langues. Ce type de recherche cherche √† niveler les in√©galit√©s.\n",
    "* √Ä consulter √©galement :\n",
    "\n",
    "  * üìÑ *[Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models](https://aclanthology.org/2023.emnlp-main.614.pdf)*\n",
    "  * üìÑ *[Language Model Tokenizers Introduce Unfairness Between Languages](https://arxiv.org/pdf/2305.154255)*\n",
    "\n",
    "Ces √©tudes ont montr√© d√®s le d√©part que les tokenizers introduisent une **injustice structurelle**, en particulier pour les langues peu dot√©es en ressources. Pour cette raison, plusieurs fournisseurs commerciaux de LLM ont depuis commenc√© √† entra√Æner des **tokenizers plus repr√©sentatifs** afin de r√©duire les disparit√©s de co√ªt entre langues.\n",
    "\n",
    "En r√©sum√© : **La tokenisation n‚Äôest pas qu‚Äôun d√©tail technique, c‚Äôest une question d‚Äôacc√®s linguistique.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV3YG7QOZD-B"
   },
   "source": [
    "Voici la traduction en fran√ßais de ta conclusion :\n",
    "\n",
    "---\n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "**R√©sum√© :**\n",
    "\n",
    "Vous avez maintenant acquis les bases essentielles du fonctionnement d‚Äôun Large Language Model (LLM), depuis les m√©canismes d‚Äôattention jusqu‚Äô√† l‚Äôentra√Ænement de votre propre mod√®le ! Ces outils puissants ont le potentiel de transformer de nombreuses t√¢ches. Cependant, comme pour tout mod√®le de deep learning, leur magie r√©side dans la capacit√© √† les appliquer aux bons probl√®mes avec les bonnes donn√©es.\n",
    "\n",
    "Pr√™t¬∑e √† passer au niveau sup√©rieur ? Plongez-vous dans le fine-tuning de vos propres LLMs pour lib√©rer encore plus de potentiel ! Je recommande vivement d‚Äôexplorer le practical de l‚Äôann√©e derni√®re sur les m√©thodes de fine-tuning efficaces en param√®tres pour une vue compl√®te des techniques avanc√©es. Le voyage ne s‚Äôarr√™te pas ici ‚Äî il y a tellement plus √† d√©couvrir !\n",
    "\n",
    "Le monde des LLMs est √† votre port√©e ‚Äî lancez-vous et cr√©ez quelque chose d‚Äôincroyable ! üåüüöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaines √©tapes :**\n",
    "[**Fine-tuning efficace en param√®tres des grands mod√®les de langage**](https://colab.research.google.com/drive/1_QGpdDOlKSiEyV2E1NsMQBhSqspFOs64?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "**R√©f√©rences :** Pour plus de ressources, consultez les liens r√©f√©renc√©s dans les diff√©rentes sections de ce colab.\n",
    "\n",
    "* [Article \"Attention is all you need\"](https://arxiv.org/abs/1706.03762)\n",
    "* [Qu‚Äôest-ce que les mod√®les Transformer et comment fonctionnent-ils ?](https://www.youtube.com/watch?v=qaWMOYf4ri8)\n",
    "* [Cl√©s, requ√™tes, et valeurs : la m√©canique c√©leste de l‚Äôattention](https://www.youtube.com/watch?v=RFdb2rKAqFw)\n",
    "* [Vid√©os suppl√©mentaires sur les Transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
    "* [LLMs pour tous DLI2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
    "* [Fondations des LLM DLI2024](https://github.com/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Foundations_of_LLMs/foundations_of_llms_practical.ipynb)\n",
    "\n",
    "Pour d√©couvrir d‚Äôautres practicals du Deep Learning Indaba, rendez-vous [ici](https://github.com/deep-learning-indaba/indaba-pracs-2025).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1ndpYE50BpG"
   },
   "source": [
    "## Avis\n",
    "\n",
    "Merci de remplir ce formulaire, c‚Äôest une partie tr√®s importante des travaux pratiques. Vos retours nous aideront √† **am√©liorer les sessions et compteront √©galement pour le prix √† la fin des sessions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "OIZvkhfRz9Jz"
   },
   "outputs": [],
   "source": [
    "# @title G√©n√©rer un formulaire de commentaires. (Ex√©cuter la cellule)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/AJr8t3mzXV2WRgHy6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oglV4kHMWnIN"
   },
   "source": [
    "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7e0353a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Zgn0dW3IH9NQ",
    "ElXJ1BGzH5BJ",
    "svfeQO7VIOIu",
    "AtWMaTddww65",
    "9YPYutB1TAes",
    "WNO703V9SBcI",
    "Qkh0KgRPdDf8",
    "vY02IFQouwjN",
    "5X4tRtSZxGHg",
    "tixtBEtRPZ5n",
    "QVTduxk4PdYC",
    "WwCo9941QRo2",
    "o1ndpYE50BpG"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
