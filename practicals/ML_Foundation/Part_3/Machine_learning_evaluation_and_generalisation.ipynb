{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2025/blob/ml-foundations-prac-3/practicals/ML_Foundation/Part_3/Machine_learning_evaluation_and_generalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8vAhgUlCrhn"
      },
      "source": [
        "# **Introduction to DL -- Evaluation, Generalization, and Optimization Algorithms**\n",
        "\n",
        "<img src=\"https://incubator.ucf.edu/wp-content/uploads/2023/07/artificial-intelligence-new-technology-science-futuristic-abstract-human-brain-ai-technology-cpu-central-processor-unit-chipset-big-data-machine-learning-cyber-mind-domination-generative-ai-scaled-1-1500x1000.jpg\" width=\"600\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0b-9wxi_XDD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2025/blob/main/practicals/ML_Foundation/Part_3/Machine_learning_evaluation_and_generalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "Â© Deep Learning Indaba 2025. Apache License 2.0.\n",
        "\n",
        "**Authors:** Ulrich Mbou Sob, Geraud Nangue Tasse\n",
        "\n",
        "**Reviewers:**\n",
        "\n",
        "**Introduction:**\n",
        "In machine learning, our main goal is to **train a model on data** so that it can perform well at a specific task such as **classification** or **regression**.\n",
        "\n",
        "When training a model, we usually minimize a **loss function**.  \n",
        "The loss helps guide the learning process, but it doesnâ€™t always match how we actually measure performance in practice. In this tutorial, we will focus on machine learning model evaluation and introduce different optimization techniques that can be leverage to improve machine learning model's performance.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: <font color='green'>`Supervised Learning, Evaluation, Optimization`</font>\n",
        "\n",
        "Level: <font color='grey'>`Beginner`</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "In this tutorial we will learn the following key concepts:\n",
        "\n",
        "- **Model Evaluation** â†’ How do we measure how good a model really is?  \n",
        "- **Generalization** â†’ How well does a model perform on data it has never seen before?  \n",
        "- **Optimization & Regularization** â†’ What algorithms and techniques can we use to train models more effectively and prevent overfitting?\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Practical 1\n",
        "  - Regression\n",
        "  - Basic knowledge of Jax\n",
        "- Practical 2\n",
        "  - Machine learning classification\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[Part 1 - Evaluation and Generalization](#scrollTo=zHc7_PbomVIN)\n",
        "\n",
        ">>[Model Evaluation](#scrollTo=nHCa0Tj-0_ZC)\n",
        "\n",
        ">>>[Breast cancer classification](#scrollTo=VnKyBQeS7auc)\n",
        "\n",
        ">>>[Evaluation metrics](#scrollTo=S8cWX6wEe0rN)\n",
        "\n",
        ">>>[âœ… Accuracy](#scrollTo=nw5RpOOekkII)\n",
        "\n",
        ">>>[ðŸŽ¯ Precision](#scrollTo=41S2rXHulrai)\n",
        "\n",
        ">>>[ðŸš¨ Recall (Sensitivity)](#scrollTo=BshBAzvElsyw)\n",
        "\n",
        ">>>[ðŸ“Š Aggregate Metrics](#scrollTo=aUlfhmD9tED-)\n",
        "\n",
        ">>>[Cross validation and Generalisation](#scrollTo=KpjY8k_64kjT)\n",
        "\n",
        ">[Part 2 - Optimization algorithms, learning rate schedulers, and hyperparameter tunning](#scrollTo=5HNpEM4DnMNe)\n",
        "\n",
        ">[Appendix](#scrollTo=8dmPgHGhH8oU)\n",
        "\n",
        ">>[References](#scrollTo=d6YYbpyXpqib)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "**Note:** To get the most out of this tutorial, try answering the questions, quizzes, and code tasks on your own before checking the solutions. Actively working through them is the most effective way to learn.\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "Run the \"Installation and Imports\" cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua49uaQKFSGQ"
      },
      "source": [
        "### Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "spIesIEZFVH1"
      },
      "outputs": [],
      "source": [
        "!pip install jax flax optax clu --quiet\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "from matplotlib import cm\n",
        "import tensorflow as tf\n",
        "import optax\n",
        "import flax\n",
        "from clu import metrics\n",
        "from flax import struct\n",
        "import flax.linen as nn\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeUWtf7PtE-K"
      },
      "source": [
        "### Helper functions (Run Cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "925nZyXLtLBV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from flax.core import freeze, unfreeze\n",
        "\n",
        "@struct.dataclass\n",
        "class TrainMetrics(metrics.Collection):\n",
        "  loss: metrics.Average.from_output('loss')\n",
        "\n",
        "@struct.dataclass\n",
        "class EvalMetrics(metrics.Collection):\n",
        "  loss: metrics.Average.from_output('loss')\n",
        "\n",
        "def train_step(params, model, optimizer, opt_state, loss_grad_fn, metrics, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  (loss, logits), grads = loss_grad_fn(params, model, batch)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  labels = batch[1].astype(jnp.int32)\n",
        "  metric_updates = TrainMetrics.single_from_model_output(\n",
        "    logits=logits, labels=labels, loss=loss)\n",
        "\n",
        "  metrics = metrics.merge(metric_updates)\n",
        "  return params, opt_state, metrics\n",
        "\n",
        "def eval_step(params, model, loss_fn, metrics, batch):\n",
        "  loss, logits = loss_fn(params, model, batch)\n",
        "  labels = batch[1].astype(jnp.int32)\n",
        "  metric_updates = EvalMetrics.single_from_model_output(\n",
        "    logits=logits, labels=labels, loss=loss)\n",
        "\n",
        "  metrics = metrics.merge(metric_updates)\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def train(\n",
        "    epochs, params, model, optimizer, opt_state, loss_grad_fn,\n",
        "    loss_fn, train_ds, test_ds, metrics_history,\n",
        "  ):\n",
        "\n",
        "  for i in range(epochs):\n",
        "    train_metrics = TrainMetrics.empty()\n",
        "    for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
        "      params, opt_state, train_metrics = train_step(params, model, optimizer, opt_state, loss_grad_fn, train_metrics, batch)\n",
        "\n",
        "    for metric, value in train_metrics.compute().items():\n",
        "      metrics_history[f\"train_{metric}\"].append(value)\n",
        "\n",
        "    eval_metrics = EvalMetrics.empty()\n",
        "    for step, batch in enumerate(test_ds.as_numpy_iterator()):\n",
        "      eval_metrics = eval_step(params, model, loss_fn, eval_metrics, batch)\n",
        "\n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "      metrics_history[f\"test_{metric}\"].append(value)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  # Plot loss and accuracy in subplots\n",
        "  fig, ax = plt.subplots(figsize=(7, 5))\n",
        "  ax.set_title('Loss')\n",
        "  for dataset in ('train', 'test'):\n",
        "    ax.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}')\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return params, opt_state, metrics_history\n",
        "\n",
        "\n",
        "# Recursively print all parameter names and their shapes\n",
        "def print_param_shapes(params, prefix=\"\"):\n",
        "    for key, val in params.items():\n",
        "        if isinstance(val, dict):\n",
        "            print_param_shapes(val, prefix=f\"{prefix}{key}/\")\n",
        "        else:\n",
        "            print(f\"{prefix}{key}: shape={val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHc7_PbomVIN"
      },
      "source": [
        "## Evaluation and Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHCa0Tj-0_ZC"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "In simple terms:  \n",
        "- **Evaluation** tells us how well our model is performing on a given dataset.  \n",
        "- **Generalization** tells us how well the model can perform on data it has never seen before.  \n",
        "\n",
        "Our goal is not just to build a model that performs well on the **training data**, but one that learns the **underlying patterns in the data**.  \n",
        "This way, it can make good predictions on **unseen examples** â€” and even handle slightly different (out-of-distribution) cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnKyBQeS7auc"
      },
      "source": [
        "#### **Breast cancer classification**\n",
        "For this section, we will revisit the breast cancer classification task from Part 2 of the practicals. Here will pay more attention and focus on the performance of our model.\n",
        "\n",
        "Let's load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FERV5Vp292GA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load breast cancer dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert the dataset to a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add the target variable to the DataFrame\n",
        "# We reverse here because sklearn stores \"Malignant\" which is cancerous as 0\n",
        "# and Benign non cancerous as 1\n",
        "df['target'] = 1 - data.target\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pNjSOHj__xP"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNfS8AKF_ZNw"
      },
      "outputs": [],
      "source": [
        "# Check the proportion of 1s and 0s\n",
        "proportion = df[\"target\"].value_counts(normalize=True)\n",
        "\n",
        "print(\"Counts:\\n\", df[\"target\"].value_counts())\n",
        "print(\"\\nProportions:\\n\", proportion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e91oMWDIDgNu"
      },
      "source": [
        "ðŸ¤” **Pause and reflect:** If we look at the proportions of our labels more then 62% belong to one class. How do you think this can affect the performance of our model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-HYCrzHGJ_G"
      },
      "outputs": [],
      "source": [
        "# split dataset into test and train\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"target\"])\n",
        "\n",
        "# split each set into input and target\n",
        "y_train = train_set.pop('target').astype(np.int32)\n",
        "x_train = train_set\n",
        "\n",
        "y_test = test_set.pop('target').astype(np.int32)\n",
        "x_test = test_set\n",
        "\n",
        "print(f\"training data input shape {x_train.shape}\")\n",
        "print(f\"training target shape {y_train.shape}\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hGvJgQCIDV7"
      },
      "source": [
        "Before diving into different evaluation metrics let implement and train a model with out datasets. The [helper function](#scrollTo=QeUWtf7PtE-K) contain the training loop function we will use.\n",
        "\n",
        "ðŸ’» Code Task: Train a Multi-Layer Perceptron (MLP) Binary Classifier\n",
        "\n",
        "Your goal is to complete the definition of an MLP model using Flaxâ€™s Linen API. This model will take input features from a tumor dataset and output a single logit representing whether the tumor is malignant or benign (binary classification).\n",
        "\n",
        "You'll implement the model by completing the __call__ method of the MLP class.\n",
        "\n",
        "1. Architecture\n",
        "\n",
        "   - Design your own architecture using activation functions we learned previously.\n",
        "   - The input to your MLP should be a list of hidden layers. Previously we passed each hidden layer separately\n",
        "\n",
        "2. Implement the loss function.\n",
        "\n",
        "   - Use the binary cross entropy loss.\n",
        "\n",
        "3.  Call the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfGamgULOjtH"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  layer_sizes: list  # e.g., [64, 32, 10, 1] (hidden layers + output layer)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "\n",
        "    # Implement the various hidden layers with your choosen activation function\n",
        "    # Hint: loop through all the hidden layers first. Then implement the output layer out of the loop.\n",
        "    for size ... # update me\n",
        "      x =  ... # update me\n",
        "\n",
        "\n",
        "\n",
        "    # Output layer\n",
        "    x = ... # update me\n",
        "\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repTv7GKO7ph"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”“Solution - MLP in Jax(Try not to peek until you've given it a good try!')\n",
        "class MLP(nn.Module):\n",
        "  layer_sizes: list  # e.g., [64, 32, 10, 1] (hidden layers + output layer)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "      # Apply all hidden layers with GeLU\n",
        "      for size in self.layer_sizes[:-1]:\n",
        "          x = nn.Dense(size)(x)\n",
        "          x = nn.gelu(x)\n",
        "\n",
        "      # Output layer (no activation)\n",
        "      x = nn.Dense(self.layer_sizes[-1])(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HxL_A01PXjC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model_and_optimizer(input_size, output_sizes=None, seed=32, lr=1e-3):\n",
        "  # Helper function to quickly initialise MLP models\n",
        "  # output sizes is list of different output layer e.g. [10,10,1]\n",
        "  # the final value in the output size should be 1 since this what we want for our MLP\n",
        "\n",
        "  model = MLP(output_sizes)\n",
        "\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "\n",
        "  dummy_data = jnp.zeros((1, input_size), dtype=float)\n",
        "\n",
        "\n",
        "  params = model.init(key, dummy_data)\n",
        "\n",
        "  # Print model parameters\n",
        "  print_param_shapes(params['params'])\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=lr)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  return model, params, optimizer, opt_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya7pPg98ULMQ"
      },
      "outputs": [],
      "source": [
        "def loss_func(params, model, batch):\n",
        "  \"\"\"Compute the sigmoid binary cross-entropy loss and return logits.\"\"\"\n",
        "\n",
        "  # Extract inputs and labels from batch\n",
        "\n",
        "  # Calculate the logits\n",
        "\n",
        "  labels = jnp.reshape(labels, logits.shape) # Reshape the labels to match the shape of the logits.\n",
        "\n",
        "  # Compute binary cross-entropy loss\n",
        "\n",
        "  return loss, logits\n",
        "\n",
        "# Calculate gradients on loss here\n",
        "loss_grad_fn = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mtSSxHhiUg93"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”“Solution - loss and grads computations (Try not to peek until you've given it a good try!')\n",
        "def loss_func(params, model, batch):\n",
        "  # Your code here\n",
        "  inputs = batch[0]\n",
        "  labels = batch[1]\n",
        "\n",
        "  logits = model.apply(params, inputs)\n",
        "  labels = jnp.reshape(labels, logits.shape)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(\n",
        "      logits=logits, labels=labels\n",
        "  ).mean()\n",
        "  return loss, logits\n",
        "\n",
        "# Calculate gradients on loss here\n",
        "loss_grad_fn = jax.value_and_grad(loss_func, has_aux=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ3U2LKZXcQS"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # you can modify this if you wish\n",
        "epochs = 100 # you can modify this if you wish\n",
        "seed = 32 # you can modify this if you wish\n",
        "lr = 1e-3 # you can modify this if you wish\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"test_loss\": [],\n",
        "}\n",
        "\n",
        "train_ds = train_dataset.shuffle(1000).batch(batch_size)\n",
        "test_ds = test_dataset.batch(batch_size)\n",
        "\n",
        "input_size = 30\n",
        "output_sizes = ... # update this based on your MLP design\n",
        "model, params, optimizer, opt_state = get_model_and_optimizer(input_size, output_sizes, seed=seed, lr=lr)\n",
        "\n",
        "params, opt_state, metric_history = train(epochs, params, model, optimizer, opt_state,\n",
        "                                          loss_grad_fn, loss_func, train_ds, test_ds, metrics_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYngbnOXUKtG"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”“Solution - calling the training loop (Try not to peek until you've given it a good try!')\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"test_loss\": [],\n",
        "}\n",
        "\n",
        "train_ds = train_dataset.shuffle(1000).batch(batch_size)\n",
        "test_ds = test_dataset.batch(batch_size)\n",
        "\n",
        "input_size = 30\n",
        "output_sizes = [30,30,20,1]\n",
        "model, params, optimizer, opt_state = get_model_and_optimizer(input_size, output_sizes, seed=32, lr=1e-3)\n",
        "\n",
        "params, opt_state, metric_history = train(epochs, params, model, optimizer, opt_state,\n",
        "                                          loss_grad_fn, loss_func, train_ds, test_ds, metrics_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8cWX6wEe0rN"
      },
      "source": [
        "#### Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyaFEtfVfBWX"
      },
      "source": [
        "Now that we have trained our model, we will use different metrics to guage the performance of our model.\n",
        "\n",
        "ðŸ’» Code Task: Implement a prediction function for our model.\n",
        "This function should take as input, the model, the paramemters, input features and threshold for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA59i5mqgcCK"
      },
      "outputs": [],
      "source": [
        "def predict(params, model, x, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Apply model and return class predictions based on threshold.\n",
        "\n",
        "    Args:\n",
        "        params: trained model parameters\n",
        "        model: Flax MLP model\n",
        "        x: input array\n",
        "        threshold: decision threshold (default=0.5)\n",
        "\n",
        "    Returns:\n",
        "        jnp.array of predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    logits = model.apply... # update me\n",
        "    probs = ... # update me\n",
        "    preds = ... # update me using threshold\n",
        "\n",
        "\n",
        "    return preds.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gbyLn2OciBaV"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”“Solution - calling the training loop (Try not to peek until you've given it a good try!')\n",
        "def predict(params, model, x, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Apply model and return class predictions based on threshold.\n",
        "\n",
        "    Args:\n",
        "        params: trained model parameters\n",
        "        model: Flax MLP model\n",
        "        x: input array\n",
        "        threshold: decision threshold (default=0.5)\n",
        "\n",
        "    Returns:\n",
        "        jnp.array of predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    logits = model.apply(params, x)\n",
        "\n",
        "    probs = nn.sigmoid(logits)\n",
        "    preds = (probs >= threshold).astype(jnp.int32).squeeze()\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5RpOOekkII"
      },
      "source": [
        "Let define the most common metrics used in classification tasks.\n",
        "\n",
        "#### âœ… Accuracy\n",
        "\n",
        "Accuracy measures the proportion of correct predictions out of total predictions:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "$$\n",
        "\n",
        "While useful, accuracy can be **misleading**, especially on **imbalanced datasets**. Imagine if only 5% of tumors are malignant. A model that always predicts \"benign\" will still have 95% accuracy, but be completely useless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41S2rXHulrai"
      },
      "source": [
        "#### ðŸŽ¯ Precision\n",
        "\n",
        "Precision tells us **how many predicted positives were actually correct**:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Useful when **false positives are costly**, e.g., incorrectly diagnosing a healthy patient as having cancer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshBAzvElsyw"
      },
      "source": [
        "#### ðŸš¨ Recall (Sensitivity)\n",
        "\n",
        "Recall tells us **how many actual positives were predicted**:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Important when **missing a true positive case is dangerous** e.g. failing to identify a malignant tumor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z56xbQZWl-0h"
      },
      "source": [
        "ðŸ’» Code Task: Complete the functions below to implement the above 3 metrics, accuracy, precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjY7tBkemdDD"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"Compute accuracy = correct predictions / total\"\"\"\n",
        "    acc = ... # update me\n",
        "    return acc\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Compute precision = TP / (TP + FP)\"\"\"\n",
        "    tp = ... # update me -- true positve\n",
        "    fp = ... # update me -- false positive\n",
        "    result = ... # update me\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Compute recall = TP / (TP + FN)\"\"\"\n",
        "    tp = ... # update me -- true positve\n",
        "    fn = ... # update me -- false negative\n",
        "    result = ... # update me\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9zg7JDb5nYMD"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”“Solution - accuracy, precision, recall (Try not to peek until you've given it a good try!')\n",
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"Compute accuracy = correct predictions / total\"\"\"\n",
        "    return jnp.mean(y_true == y_pred)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Compute precision = TP / (TP + FP)\"\"\"\n",
        "    tp = jnp.sum((y_true == 1) & (y_pred == 1))\n",
        "    fp = jnp.sum((y_true == 0) & (y_pred == 1))\n",
        "    return tp / (tp + fp + 1e-8)  # add epsilon to avoid division by zero\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Compute recall = TP / (TP + FN)\"\"\"\n",
        "    tp = jnp.sum((y_true == 1) & (y_pred == 1))\n",
        "    fn = jnp.sum((y_true == 1) & (y_pred == 0))\n",
        "    return tp / (tp + fn + 1e-8) # add epsilon to avoid division by zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzGer9HFn_by"
      },
      "source": [
        "Let compute these metrics on test and training sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vzdaTFon3eA"
      },
      "outputs": [],
      "source": [
        "ypreds_train = predict(params, model, x_train, threshold=0.5)\n",
        "ypreds_test = predict(params, model, x_test, threshold=0.5)\n",
        "\n",
        "# convert this to numpy from dataframes\n",
        "y_train = jnp.array(y_train.to_numpy())\n",
        "y_test = jnp.array(y_test.to_numpy())\n",
        "\n",
        "train_acc = accuracy(y_train, ypreds_train)\n",
        "test_acc = accuracy(y_test, ypreds_test)\n",
        "\n",
        "train_precision = precision(y_train, ypreds_train)\n",
        "train_recall = recall(y_train, ypreds_train)\n",
        "\n",
        "test_precision = precision(y_test, ypreds_test)\n",
        "test_recall = recall(y_test, ypreds_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy {test_acc:.4f}\")\n",
        "print(f\"Train Precision: {train_precision:.4f}, Test Precision {test_precision:.4f}\")\n",
        "print(f\"Train Recall: {train_recall:.4f}, Test Recall {test_recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Pf4_B_qeJX"
      },
      "source": [
        "ðŸ¤” Pause and reflect: What can you say about your model performance?\n",
        "\n",
        "Is your model performing similarly on the training set and the test set?\n",
        "\n",
        "Are you satisified with your precision and recall scores given their implications?\n",
        "\n",
        "How can we improve our model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUlfhmD9tED-"
      },
      "source": [
        "#### ðŸ“Š Aggregate Metrics\n",
        "In the preciding section, we define various metrics which can be analysed independently. Most often in machine learning and science studies, we want to summarise everthing in single number that can tell the full story. Now we look at few other metrics that try exactly to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Lbanc8va3I"
      },
      "source": [
        "**ROC Curve**\n",
        "\n",
        "The **Receiver Operating Characteristic (ROC) curve** shows how well a binary classifier can separate the two classes across different thresholds.  \n",
        "- The x-axis is the **False Positive Rate (FPR)**  \n",
        "- The y-axis is the **True Positive Rate (TPR / Recall)**  \n",
        "- The closer the curve is to the **top-left corner**, the better the model.  \n",
        "- The **Area Under the Curve (AUC)** summarizes performance:  \n",
        "  - AUC = 1 â†’ perfect classifier  \n",
        "  - AUC = 0.5 â†’ random guessing\n",
        "\n",
        "**F1-Score**\n",
        "\n",
        "The **F1-score** balances **precision** and **recall** in one number.  \n",
        "It is useful when classes are imbalanced.  \n",
        "$$\n",
        "F1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "- High F1 means the model has both **good precision** (few false positives) and **good recall** (few false negatives).  \n",
        "- Useful for medical tasks like cancer detection where both errors matter.\n",
        "\n",
        "**Matthews Correlation Coefficient (MCC)**\n",
        "\n",
        "The **MCC** is a more balanced evaluation metric that uses all four confusion matrix values i.e. TP, TN, FP, and FN.  \n",
        "\n",
        "$$\n",
        "\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "$$\n",
        "\n",
        "- MCC = +1 â†’ perfect prediction  \n",
        "- MCC = 0 â†’ random prediction  \n",
        "- MCC = -1 â†’ total disagreement  \n",
        "\n",
        "MCC is especially good for **imbalanced datasets**, where accuracy alone can be misleading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni-QsUWMv5KT"
      },
      "source": [
        "ðŸ’» Code Task: Complete the functions below to implement the F1 score and the MCC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDSBu4F9wYBW"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    p = ... # update me -- precision\n",
        "    r = ... # update me -- recall\n",
        "    result = ... # update me\n",
        "    return result\n",
        "\n",
        "def matthews_corrcoef(y_true, y_pred):\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = ... # update me --- false positive\n",
        "    fn = ... # update me --- false negative\n",
        "\n",
        "    numerator = (tp * tn) - (fp * fn)\n",
        "    denominator = ... # update me\n",
        "    return numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLlidf-2wmw6"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * (p * r) / (p + r + 1e-8)\n",
        "\n",
        "def matthews_corrcoef(y_true, y_pred):\n",
        "    tp = float(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "    tn = float(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "    fp = float(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "    fn = float(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "    numerator = (tp * tn) - (fp * fn)\n",
        "    denominator = np.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) + 1e-8)\n",
        "    return numerator / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wzqCNTx-S7"
      },
      "source": [
        "Let's compute the F1 score and MCC of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHiAk0IyEF2"
      },
      "outputs": [],
      "source": [
        "train_f1_score = f1_score(y_train, ypreds_train)\n",
        "test_f1_score = f1_score(y_test, ypreds_test)\n",
        "\n",
        "train_mcc = matthews_corrcoef(y_train, ypreds_train)\n",
        "test_mcc = matthews_corrcoef(y_test, ypreds_test)\n",
        "\n",
        "print(f\"Train f1 score: {train_f1_score:.4f}, Test f1 score {test_f1_score:.4f}\")\n",
        "print(f\"Train MCC: {train_mcc:.4f}, Test MCC {test_mcc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyCnrQu-2YIX"
      },
      "source": [
        "ðŸ¤” Pause and reflect: What can you say about your model performance based on these metrics?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpjY8k_64kjT"
      },
      "source": [
        "### Cross validation and Generalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH4Tv3Ar-yIW"
      },
      "source": [
        "In the previous sections we discussed different metrics which can be use to measure the performance of machine learning models in classifications tasks.\n",
        "\n",
        "So far we've used an approach where we split the data into a training and test set. This implies we are are only measuring the generalisation ability of the model using a fixed training and test set.\n",
        "\n",
        "ðŸ¤” Pause and reflect: What approach can we use in our train/test splitting strategy to improve model generalisation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMqYam1oBwn1"
      },
      "source": [
        "\n",
        "**Cross-validation** is a popular machine learning technique used to test the **generalization capability** of models.  \n",
        "\n",
        "In this approach, we split the dataset into several parts (called *folds*). The model is trained on some of these folds and tested on the remaining ones. This process is repeated so that each fold serves as the test set once. Finally, we evaluate the modelâ€™s performance on each test set and take the **average score** as the overall performance.  \n",
        "\n",
        "ðŸ“– You can read more about different cross-validation techniques [here](https://www.geeksforgeeks.org/machine-learning/cross-validation-machine-learning/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HNpEM4DnMNe"
      },
      "source": [
        "## Optimization algorithms, learning rate schedulers, and hyperparameter tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dmPgHGhH8oU"
      },
      "source": [
        "## Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6YYbpyXpqib"
      },
      "source": [
        "### References\n",
        "1. Flax module documentation: https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/module.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/CJCNwwcLW9Y3jZDG7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
