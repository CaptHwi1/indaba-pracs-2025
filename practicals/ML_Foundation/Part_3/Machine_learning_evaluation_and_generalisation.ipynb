{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8vAhgUlCrhn"
      },
      "source": [
        "# **Introduction to DL -- Evaluation, Generalization, and Optimization Algorithms**\n",
        "\n",
        "<img src=\"https://incubator.ucf.edu/wp-content/uploads/2023/07/artificial-intelligence-new-technology-science-futuristic-abstract-human-brain-ai-technology-cpu-central-processor-unit-chipset-big-data-machine-learning-cyber-mind-domination-generative-ai-scaled-1-1500x1000.jpg\" width=\"600\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0b-9wxi_XDD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2025/blob/main/practicals/ML_Foundation/Part_3/Machine_learning_evaluation_and_generalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "© Deep Learning Indaba 2025. Apache License 2.0.\n",
        "\n",
        "**Authors:** Ulrich Mbou Sob, Geraud Nangue Tasse\n",
        "\n",
        "**Reviewers:**\n",
        "\n",
        "**Introduction:**\n",
        "In machine learning, our main goal is to **train a model on data** so that it can perform well at a specific task such as **classification** or **regression**.\n",
        "\n",
        "When training a model, we typically minimize a **loss function**. The loss guides the learning process, but it doesn’t always align with how we actually evaluate performance on the intended task. In this tutorial, we will explore different approaches to evaluating machine learning models and introduce various optimization techniques that can be leveraged to improve their performance.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: <font color='green'>`Supervised Learning, Evaluation, Optimization`</font>\n",
        "\n",
        "Level: <font color='grey'>`Beginner`</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "In this tutorial we will learn the following key concepts:\n",
        "\n",
        "- **Model Evaluation** → How do we measure how good a model really is?  \n",
        "- **Generalization** → How well does a model perform on data it has never seen before?  \n",
        "- **Optimization & Regularization** → What algorithms and techniques can we use to train models more efficiently and prevent overfitting?\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Practical 1\n",
        "  - Regression\n",
        "  - Basic knowledge of Jax\n",
        "- Practical 2\n",
        "  - Machine learning classification\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[Introduction to DL -- Evaluation, Generalization, and Optimization Algorithms](#scrollTo=f8vAhgUlCrhn)\n",
        "\n",
        ">>[Installation and Imports](#scrollTo=Ua49uaQKFSGQ)\n",
        "\n",
        ">>[Helper functions (Run Cell)](#scrollTo=QeUWtf7PtE-K)\n",
        "\n",
        ">>[Part 1 — Evaluation and Generalization](#scrollTo=zHc7_PbomVIN)\n",
        "\n",
        ">>>[Model Evaluation](#scrollTo=nHCa0Tj-0_ZC)\n",
        "\n",
        ">>>[Breast cancer classification](#scrollTo=VnKyBQeS7auc)\n",
        "\n",
        ">>>[Evaluation metrics](#scrollTo=S8cWX6wEe0rN)\n",
        "\n",
        ">>>>[✅ Accuracy](#scrollTo=nw5RpOOekkII)\n",
        "\n",
        ">>>>[🎯 Precision](#scrollTo=41S2rXHulrai)\n",
        "\n",
        ">>>>[🚨 Recall (Sensitivity)](#scrollTo=BshBAzvElsyw)\n",
        "\n",
        ">>>>[📊 Aggregate Metrics](#scrollTo=aUlfhmD9tED-)\n",
        "\n",
        ">>>[Cross validation and Generalisation](#scrollTo=KpjY8k_64kjT)\n",
        "\n",
        ">>[Part 2 — Optimization Algorithms](#scrollTo=5HNpEM4DnMNe)\n",
        "\n",
        ">>>[Parameter Initialization](#scrollTo=xprTxw9s4BCo)\n",
        "\n",
        ">>>>[1.1 Random Initialization](#scrollTo=Gy3rT0eqBNYa)\n",
        "\n",
        ">>>>[1.2 He Initialisation (and Xavier)](#scrollTo=g05u_7iXF6NL)\n",
        "\n",
        ">>>[Gradient Steps Optimisation](#scrollTo=4owuWGJDHT8a)\n",
        "\n",
        ">>>>[2.1 Effect of Learning Rate](#scrollTo=8jvzMdwRHYn2)\n",
        "\n",
        ">>>>[2.2 Momentum](#scrollTo=INHNJNakHtHf)\n",
        "\n",
        ">>>>[2.3 Adam Optimizer](#scrollTo=rZyAEH_zIED7)\n",
        "\n",
        ">>>[Regularisation](#scrollTo=BIGSComnIgLR)\n",
        "\n",
        ">>>>[3.1 L2 Regularisation (Weight Decay)](#scrollTo=FdpZtxRoIjhQ)\n",
        "\n",
        ">>>>[3.2 Early Stopping](#scrollTo=DoarzCZlI5fi)\n",
        "\n",
        ">>>>[3.3 Other approaches (briefly)](#scrollTo=hlEtTtPfJGFR)\n",
        "\n",
        ">>>[Wrap-up](#scrollTo=6tpSuRDAJIZ-)\n",
        "\n",
        ">>[Appendix](#scrollTo=8dmPgHGhH8oU)\n",
        "\n",
        ">>[References](#scrollTo=d6YYbpyXpqib)\n",
        "\n",
        ">>[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "**Note:** To get the most out of this tutorial, try answering the questions, quizzes, and code tasks on your own before checking the solutions. Actively working through them is the most effective way to learn.\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "Run the \"Installation and Imports\" cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua49uaQKFSGQ"
      },
      "source": [
        "### Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "spIesIEZFVH1"
      },
      "outputs": [],
      "source": [
        "!pip install jax flax optax clu --quiet\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "from matplotlib import cm\n",
        "import tensorflow as tf\n",
        "import optax\n",
        "import flax\n",
        "from clu import metrics\n",
        "from flax import struct\n",
        "import flax.linen as nn\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeUWtf7PtE-K"
      },
      "source": [
        "### Helper functions (Run Cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "925nZyXLtLBV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from flax.core import freeze, unfreeze\n",
        "\n",
        "@struct.dataclass\n",
        "class TrainMetrics(metrics.Collection):\n",
        "  loss: metrics.Average.from_output('loss')\n",
        "\n",
        "@struct.dataclass\n",
        "class EvalMetrics(metrics.Collection):\n",
        "  loss: metrics.Average.from_output('loss')\n",
        "\n",
        "def train_step(params, model, optimizer, opt_state, loss_grad_fn, metrics, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  (loss, logits), grads = loss_grad_fn(params, model, batch)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  labels = batch[1].astype(jnp.int32)\n",
        "  metric_updates = TrainMetrics.single_from_model_output(\n",
        "    logits=logits, labels=labels, loss=loss)\n",
        "\n",
        "  metrics = metrics.merge(metric_updates)\n",
        "  return params, opt_state, metrics\n",
        "\n",
        "def eval_step(params, model, loss_fn, metrics, batch):\n",
        "  loss, logits = loss_fn(params, model, batch)\n",
        "  labels = batch[1].astype(jnp.int32)\n",
        "  metric_updates = EvalMetrics.single_from_model_output(\n",
        "    logits=logits, labels=labels, loss=loss)\n",
        "\n",
        "  metrics = metrics.merge(metric_updates)\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def train(\n",
        "    epochs, params, model, optimizer, opt_state, loss_grad_fn,\n",
        "    loss_fn, train_ds, test_ds, metrics_history, plot_now=True\n",
        "  ):\n",
        "\n",
        "  train_metrics = TrainMetrics.empty()\n",
        "  eval_metrics = EvalMetrics.empty()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
        "      params, opt_state, train_metrics = train_step(params, model, optimizer, opt_state, loss_grad_fn, train_metrics, batch)\n",
        "\n",
        "      for metric, value in train_metrics.compute().items():\n",
        "        metrics_history[f\"train_{metric}\"].append(value)\n",
        "\n",
        "      for _, full_test_batch in enumerate(test_ds.as_numpy_iterator()):\n",
        "        eval_metrics = eval_step(params, model, loss_fn, eval_metrics, full_test_batch)\n",
        "\n",
        "      for metric, value in eval_metrics.compute().items():\n",
        "        metrics_history[f\"test_{metric}\"].append(value)\n",
        "\n",
        "  if plot_now:\n",
        "    plot_metrics([metrics_history], [\"\"])\n",
        "\n",
        "  return params, opt_state, metrics_history\n",
        "\n",
        "\n",
        "# Recursively print all parameter names and their shapes\n",
        "def print_param_shapes(params, prefix=\"\"):\n",
        "    for key, val in params.items():\n",
        "        if isinstance(val, dict):\n",
        "            print_param_shapes(val, prefix=f\"{prefix}{key}/\")\n",
        "        else:\n",
        "            print(f\"{prefix}{key}: shape={val.shape}\")\n",
        "\n",
        "def plot_metrics(metrics_histories, labels, metric=\"loss\"):\n",
        "    \"\"\"\n",
        "    Plot training and test metrics for multiple histories.\n",
        "\n",
        "    Args:\n",
        "        metrics_histories (list[dict]): Each dict contains train/test metrics history\n",
        "                                        e.g. metrics_history['train_loss'], ['test_loss']\n",
        "        labels (list[str]): Labels for each run (e.g. 'Model A', 'Model B')\n",
        "        metric (str): Metric to plot (default: 'loss')\n",
        "    \"\"\"\n",
        "    clear_output(wait=True)\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    colors = ['blue', 'orange', 'green', 'pink', 'black']\n",
        "    nc = len(colors)\n",
        "\n",
        "    for i, (history, label) in enumerate(zip(metrics_histories, labels)):\n",
        "        # train curve (solid)\n",
        "        ax.plot(history[f\"train_{metric}\"], label=f\"{label} train\", color=colors[i%nc])\n",
        "        # test curve (dashed, same color as train)\n",
        "        ax.plot(history[f\"test_{metric}\"], linestyle=\"--\", label=f\"{label} test\", color=colors[i%nc])\n",
        "\n",
        "    ax.set_ylabel(metric.capitalize())\n",
        "    ax.set_xlabel(\"Iteration\")\n",
        "    ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHc7_PbomVIN"
      },
      "source": [
        "## Evaluation and Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHCa0Tj-0_ZC"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "In simple terms:  \n",
        "- **Evaluation** tells us how well our model is performing on a given dataset.  \n",
        "- **Generalization** tells us how well the model can perform on data it has never seen before.  \n",
        "\n",
        "Our goal is not just to build a model that performs well on the **training data**, but one that learns the **underlying patterns in the data**.  \n",
        "This way, it can make good predictions on **unseen examples** — and even handle slightly different (out-of-distribution) cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnKyBQeS7auc"
      },
      "source": [
        "#### **Breast cancer classification**\n",
        "For this section, we will revisit the breast cancer classification task from Part 2 of the practicals. Here will pay more attention and focus on the performance of our model.\n",
        "\n",
        "Let's load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FERV5Vp292GA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load breast cancer dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert the dataset to a pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add the target variable to the DataFrame\n",
        "# We reverse here because sklearn stores \"Malignant\" which is cancerous as 0\n",
        "# and Benign non cancerous as 1\n",
        "df['target'] = 1 - data.target\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pNjSOHj__xP"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNfS8AKF_ZNw"
      },
      "outputs": [],
      "source": [
        "# Check the proportion of 1s and 0s\n",
        "proportion = df[\"target\"].value_counts(normalize=True)\n",
        "\n",
        "print(\"Counts:\\n\", df[\"target\"].value_counts())\n",
        "print(\"\\nProportions:\\n\", proportion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e91oMWDIDgNu"
      },
      "source": [
        "🤔 **Pause and reflect:** If we look at the proportions of our labels more then 62% belong to one class. How do you think this can affect the performance of our model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-HYCrzHGJ_G"
      },
      "outputs": [],
      "source": [
        "# split dataset into test and train\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"target\"])\n",
        "\n",
        "# split each set into input and target\n",
        "y_train = train_set.pop('target').astype(np.int32)\n",
        "x_train = train_set\n",
        "\n",
        "y_test = test_set.pop('target').astype(np.int32)\n",
        "x_test = test_set\n",
        "\n",
        "print(f\"training data input shape {x_train.shape}\")\n",
        "print(f\"training target shape {y_train.shape}\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hGvJgQCIDV7"
      },
      "source": [
        "Before diving into different evaluation metrics let implement and train a model with out datasets. The [helper functions](#scrollTo=QeUWtf7PtE-K) contain the training loop function we will use.\n",
        "\n",
        "💻 Code Task: Train a Multi-Layer Perceptron (MLP) Binary Classifier.\n",
        "\n",
        "Your goal is to complete the definition of an MLP model using Flax’s Linen API. This model will take input features from a tumor dataset and output a single logit representing the probability whether the tumor is malignant or benign (binary classification).\n",
        "\n",
        "You'll implement the model by completing the __call__ method of the MLP class.\n",
        "\n",
        "1. Architecture\n",
        "\n",
        "   - Design your own architecture using activation functions we learned previously.\n",
        "   - The input to your MLP should be a list of hidden layers. Previously we passed each hidden layer separately.\n",
        "\n",
        "2. Implement the loss function.\n",
        "\n",
        "   - Use the binary cross entropy loss.\n",
        "\n",
        "3.  Call the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfGamgULOjtH"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  layer_sizes: list  # e.g., [64, 32, 10, 1] (hidden layers + output layer)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "\n",
        "    # Implement the various hidden layers with your choosen activation functions\n",
        "    # Hint: loop through all the hidden layers first. Then implement the output layer outside of the for loop.\n",
        "    for size ... # update me\n",
        "      x =  ... # update me\n",
        "\n",
        "\n",
        "\n",
        "    # Output layer\n",
        "    x = ... # update me\n",
        "\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repTv7GKO7ph"
      },
      "outputs": [],
      "source": [
        "# @title 🔓Solution - MLP in Jax(Try not to peek until you've given it a good try!')\n",
        "class MLP(nn.Module):\n",
        "  layer_sizes: list  # e.g., [64, 32, 10, 1] (hidden layers + output layer)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "      # Apply all hidden layers with GeLU\n",
        "      for size in self.layer_sizes[:-1]:\n",
        "          x = nn.Dense(size)(x)\n",
        "          x = nn.gelu(x)\n",
        "\n",
        "      # Output layer (no activation)\n",
        "      x = nn.Dense(self.layer_sizes[-1])(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HxL_A01PXjC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_model_and_optimizer(input_size, output_sizes=None, seed=32, lr=1e-3):\n",
        "  # Helper function to quickly initialise MLP models\n",
        "  # output sizes is list of different output layer e.g. [10,10,1]\n",
        "  # the final value in the output size should be 1 since this what we want for our MLP\n",
        "\n",
        "  model = MLP(output_sizes)\n",
        "\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "\n",
        "  dummy_data = jnp.zeros((1, input_size), dtype=float)\n",
        "\n",
        "\n",
        "  params = model.init(key, dummy_data)\n",
        "\n",
        "  # Print model parameters\n",
        "  print_param_shapes(params['params'])\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=lr)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  return model, params, optimizer, opt_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya7pPg98ULMQ"
      },
      "outputs": [],
      "source": [
        "def loss_func(params, model, batch):\n",
        "  \"\"\"Compute the sigmoid binary cross-entropy loss and return logits.\"\"\"\n",
        "\n",
        "  # Extract inputs and labels from batch\n",
        "  inputs = batch[0]\n",
        "  labels = batch[1]\n",
        "\n",
        "  # Compute logits\n",
        "  logits = model.apply(params, inputs)\n",
        "\n",
        "  # Calculate the logits\n",
        "  labels = jnp.reshape(labels, logits.shape) # Reshape the labels to match the shape of the logits.\n",
        "\n",
        "  # Compute binary cross-entropy loss\n",
        "  loss = ... # update me\n",
        "\n",
        "  return loss, logits\n",
        "\n",
        "# Define the value_and_grad function using jax.value_and_grad\n",
        "loss_grad_fn = ... # update me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtSSxHhiUg93"
      },
      "outputs": [],
      "source": [
        "# @title 🔓Solution - loss and grads computations (Try not to peek until you've given it a good try!')\n",
        "def loss_func(params, model, batch):\n",
        "  # Your code here\n",
        "  inputs = batch[0]\n",
        "  labels = batch[1]\n",
        "\n",
        "  logits = model.apply(params, inputs)\n",
        "  labels = jnp.reshape(labels, logits.shape)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(\n",
        "      logits=logits, labels=labels\n",
        "  ).mean()\n",
        "  return loss, logits\n",
        "\n",
        "# Define the value_and_grad function using jax.value_and_grad\n",
        "loss_grad_fn = jax.value_and_grad(loss_func, has_aux=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ3U2LKZXcQS"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 # you can modify this if you wish\n",
        "epochs = 100 # you can modify this if you wish\n",
        "seed = 32 # you can modify this if you wish\n",
        "lr = 1e-3 # you can modify this if you wish\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"test_loss\": [],\n",
        "}\n",
        "\n",
        "train_ds = train_dataset.shuffle(1000).batch(batch_size)\n",
        "test_batch_size = len(test_dataset)\n",
        "test_ds = test_dataset.batch(test_batch_size)\n",
        "\n",
        "input_size = 30\n",
        "output_sizes = ... # update this based on your MLP design\n",
        "model, params, optimizer, opt_state = get_model_and_optimizer(input_size, output_sizes, seed=seed, lr=lr)\n",
        "\n",
        "params, opt_state, metric_history = train(epochs, params, model, optimizer, opt_state,\n",
        "                                          loss_grad_fn, loss_func, train_ds, test_ds, metrics_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYngbnOXUKtG"
      },
      "outputs": [],
      "source": [
        "# @title 🔓Solution - calling the training loop (Try not to peek until you've given it a good try!')\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"test_loss\": [],\n",
        "}\n",
        "\n",
        "train_ds = train_dataset.shuffle(1000).batch(batch_size)\n",
        "test_batch_size = len(test_dataset)\n",
        "test_ds = test_dataset.batch(test_batch_size)\n",
        "\n",
        "input_size = 30\n",
        "output_sizes = [30,30,20,1]\n",
        "model, params, optimizer, opt_state = get_model_and_optimizer(input_size, output_sizes, seed=32, lr=1e-3)\n",
        "\n",
        "params, opt_state, metric_history = train(epochs, params, model, optimizer, opt_state,\n",
        "                                          loss_grad_fn, loss_func, train_ds, test_ds, metrics_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8cWX6wEe0rN"
      },
      "source": [
        "#### Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyaFEtfVfBWX"
      },
      "source": [
        "Now that we have trained our model, we will use different metrics to gauge the performance of our model.\n",
        "\n",
        "💻 Code Task: Implement a prediction function for our model.\n",
        "This function should take as input, the model, the paramemters, input features and a threshold for classification. This should return a label 0 or 1 for each input's features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA59i5mqgcCK"
      },
      "outputs": [],
      "source": [
        "def predict(params, model, x, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Apply model and return class predictions based on threshold.\n",
        "\n",
        "    Args:\n",
        "        params: trained model parameters\n",
        "        model: Flax MLP model\n",
        "        x: input array\n",
        "        threshold: decision threshold (default=0.5)\n",
        "\n",
        "    Returns:\n",
        "        jnp.array of predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    logits = model.apply... # update me\n",
        "    probs = ... # update me\n",
        "    preds = ... # update me using threshold\n",
        "\n",
        "\n",
        "    return preds.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbyLn2OciBaV"
      },
      "outputs": [],
      "source": [
        "# @title 🔓Solution - calling the training loop (Try not to peek until you've given it a good try!')\n",
        "def predict(params, model, x, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Apply model and return class predictions based on threshold.\n",
        "\n",
        "    Args:\n",
        "        params: trained model parameters\n",
        "        model: Flax MLP model\n",
        "        x: input array\n",
        "        threshold: decision threshold (default=0.5)\n",
        "\n",
        "    Returns:\n",
        "        jnp.array of predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    logits = model.apply(params, x)\n",
        "\n",
        "    probs = nn.sigmoid(logits)\n",
        "    preds = (probs >= threshold).astype(jnp.int32).squeeze()\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5RpOOekkII"
      },
      "source": [
        "Let define the most common metrics used in classification tasks.\n",
        "\n",
        "#### ✅ Accuracy\n",
        "\n",
        "Accuracy measures the proportion of correct predictions out of total predictions:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "$$\n",
        "\n",
        "While useful, accuracy can be **misleading**, especially on **imbalanced datasets**. Imagine if only 5% of tumors are malignant. A model that always predicts \"benign\" will still have 95% accuracy, but be completely useless."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "**Confusion matrix**: [Image source](https://medium.com/data-science/understanding-confusion-matrix-a9ad42dcfd62)"
      ],
      "metadata": {
        "id": "Ltsb0pe15Fct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While **accuracy** is the most intuitive and commonly used metric for classification tasks, it has limitations as mentioned above. To address these, other evaluation metrics are derived from the **confusion matrix**.  \n",
        "\n",
        "The confusion matrix provides a breakdown of predictions into four categories:  \n",
        "\n",
        "- **True Positives (TP)** – the model predicts positive and it is actually positive.  \n",
        "- **False Positives (FP)** – the model predicts positive but it is actually negative.  \n",
        "- **False Negatives (FN)** – the model predicts negative but it is actually positive.  \n",
        "- **True Negatives (TN)** – the model predicts negative and it is actually negative.  \n",
        "\n",
        "A **perfect confusion matrix** would have all off-diagonal entries equal to 0.  \n"
      ],
      "metadata": {
        "id": "etyFItTC9JQg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41S2rXHulrai"
      },
      "source": [
        "#### 🎯 Precision\n",
        "\n",
        "Precision tells us the **fraction of labels** predicted as **positive** that are truely **positive**:\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "Useful when **false positives** are **costly**, e.g., incorrectly diagnosing a healthy patient as having cancer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshBAzvElsyw"
      },
      "source": [
        "#### 🚨 Recall (Sensitivity)\n",
        "\n",
        "Recall tells us **how many actual positives** were predicted correctly:\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Important when **missing a true positive case is dangerous** e.g. failing to identify a malignant tumor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z56xbQZWl-0h"
      },
      "source": [
        "💻 Code Task: Complete the functions below to implement the above 3 metrics, accuracy, precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjY7tBkemdDD"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"Compute accuracy = correct predictions / total\"\"\"\n",
        "    acc = ... # update me\n",
        "    return acc\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Compute precision = TP / (TP + FP)\"\"\"\n",
        "    tp = ... # update me -- true positve\n",
        "    fp = ... # update me -- false positive\n",
        "    result = ... # update me\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Compute recall = TP / (TP + FN)\"\"\"\n",
        "    tp = ... # update me -- true positve\n",
        "    fn = ... # update me -- false negative\n",
        "    result = ... # update me\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zg7JDb5nYMD"
      },
      "outputs": [],
      "source": [
        "# @title 🔓Solution - accuracy, precision, recall (Try not to peek until you've given it a good try!')\n",
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"Compute accuracy = correct predictions / total\"\"\"\n",
        "    return jnp.mean(y_true == y_pred)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Compute precision = TP / (TP + FP)\"\"\"\n",
        "    tp = jnp.sum((y_true == 1) & (y_pred == 1))\n",
        "    fp = jnp.sum((y_true == 0) & (y_pred == 1))\n",
        "    return tp / (tp + fp + 1e-8)  # add epsilon to avoid division by zero\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Compute recall = TP / (TP + FN)\"\"\"\n",
        "    tp = jnp.sum((y_true == 1) & (y_pred == 1))\n",
        "    fn = jnp.sum((y_true == 1) & (y_pred == 0))\n",
        "    return tp / (tp + fn + 1e-8) # add epsilon to avoid division by zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzGer9HFn_by"
      },
      "source": [
        "Let compute these metrics on test and training sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vzdaTFon3eA"
      },
      "outputs": [],
      "source": [
        "ypreds_train = predict(params, model, x_train, threshold=0.5)\n",
        "ypreds_test = predict(params, model, x_test, threshold=0.5)\n",
        "\n",
        "# convert this to numpy from dataframes\n",
        "# create a new variable to avoid overwriting previous if we need to call this function multiple times\n",
        "y_train_tmp = jnp.array(y_train.to_numpy())\n",
        "y_test_tmp = jnp.array(y_test.to_numpy())\n",
        "\n",
        "train_acc = accuracy(y_train_tmp, ypreds_train)\n",
        "test_acc = accuracy(y_test_tmp, ypreds_test)\n",
        "\n",
        "train_precision = precision(y_train_tmp, ypreds_train)\n",
        "train_recall = recall(y_train_tmp, ypreds_train)\n",
        "\n",
        "test_precision = precision(y_test_tmp, ypreds_test)\n",
        "test_recall = recall(y_test_tmp, ypreds_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy {test_acc:.4f}\")\n",
        "print(f\"Train Precision: {train_precision:.4f}, Test Precision {test_precision:.4f}\")\n",
        "print(f\"Train Recall: {train_recall:.4f}, Test Recall {test_recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Pf4_B_qeJX"
      },
      "source": [
        "🤔 Pause and reflect: What can you say about your model performance?\n",
        "\n",
        "Is your model performing similarly on the training set and the test set?\n",
        "\n",
        "Are you satisified with your precision and recall scores given their implications?\n",
        "\n",
        "How can we improve our model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUlfhmD9tED-"
      },
      "source": [
        "#### 📊 Aggregate Metrics\n",
        "In the preciding section, we define various metrics which can be analysed independently. Most often in machine learning and science studies, we want to summarise everthing in single number that can tell the full story. Now we look at few other metrics that try exactly to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Lbanc8va3I"
      },
      "source": [
        "**ROC Curve**\n",
        "\n",
        "The **Receiver Operating Characteristic (ROC) curve** shows how well a binary classifier can separate the two classes across different thresholds.  \n",
        "- The x-axis is the **False Positive Rate (FPR)**  \n",
        "- The y-axis is the **True Positive Rate (TPR / Recall)**  \n",
        "- The closer the curve is to the **top-left corner**, the better the model.  \n",
        "- The **Area Under the Curve (AUC)** summarizes performance:  \n",
        "  - AUC = 1 → perfect classifier  \n",
        "  - AUC = 0.5 → random guessing\n",
        "\n",
        "**F1-Score**\n",
        "\n",
        "The **F1-score** balances **precision** and **recall** in one number.  \n",
        "It is useful when classes are imbalanced.  \n",
        "$$\n",
        "F1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}\n",
        "$$\n",
        "\n",
        "- High F1-score means the model has both **good precision** (few false positives) and **good recall** (few false negatives).  \n",
        "- Useful for medical tasks like cancer detection where both errors matter.\n",
        "\n",
        "**Matthews Correlation Coefficient (MCC)**\n",
        "\n",
        "The **MCC** is a more balanced evaluation metric that uses all four confusion matrix values i.e. TP, TN, FP, and FN.  \n",
        "\n",
        "$$\n",
        "\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "$$\n",
        "\n",
        "- MCC = +1 → perfect prediction  \n",
        "- MCC = 0 → random prediction  \n",
        "- MCC = -1 → total disagreement  \n",
        "\n",
        "Both F1-score and MCC are particularly good for **imbalanced datasets**, where accuracy alone can be misleading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni-QsUWMv5KT"
      },
      "source": [
        "💻 Code Task: Complete the functions below to implement the F1 score and the MCC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDSBu4F9wYBW"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    p = ... # update me -- precision\n",
        "    r = ... # update me -- recall\n",
        "    result = ... # update me\n",
        "    return result\n",
        "\n",
        "def matthews_corrcoef(y_true, y_pred):\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = ... # update me --- false positive\n",
        "    fn = ... # update me --- false negative\n",
        "\n",
        "    numerator = (tp * tn) - (fp * fn)\n",
        "    denominator = ... # update me\n",
        "    return numerator / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLlidf-2wmw6"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * (p * r) / (p + r + 1e-8)\n",
        "\n",
        "def matthews_corrcoef(y_true, y_pred):\n",
        "    tp = float(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "    tn = float(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "    fp = float(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "    fn = float(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "    numerator = (tp * tn) - (fp * fn)\n",
        "    denominator = np.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn) + 1e-8)\n",
        "    return numerator / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wzqCNTx-S7"
      },
      "source": [
        "Let's compute the F1 score and MCC of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHiAk0IyEF2"
      },
      "outputs": [],
      "source": [
        "train_f1_score = f1_score(y_train_tmp, ypreds_train)\n",
        "test_f1_score = f1_score(y_test_tmp, ypreds_test)\n",
        "\n",
        "train_mcc = matthews_corrcoef(y_train_tmp, ypreds_train)\n",
        "test_mcc = matthews_corrcoef(y_test_tmp, ypreds_test)\n",
        "\n",
        "print(f\"Train f1 score: {train_f1_score:.4f}, Test f1 score {test_f1_score:.4f}\")\n",
        "print(f\"Train MCC: {train_mcc:.4f}, Test MCC {test_mcc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyCnrQu-2YIX"
      },
      "source": [
        "🤔 Pause and reflect: What can you say about your model performance based on these metrics?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpjY8k_64kjT"
      },
      "source": [
        "### Cross validation and Generalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH4Tv3Ar-yIW"
      },
      "source": [
        "In the previous sections we discussed different metrics which can be use to measure the performance of machine learning models in classifications tasks.\n",
        "\n",
        "So far we've used an approach where we split the data into a training fixed training and test set. This implies we are are only measuring the generalisation ability of the model using a fixed training and test set.\n",
        "\n",
        "🤔 Pause and reflect: What approach can we use in our train/test splitting strategy to improve model generalisation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMqYam1oBwn1"
      },
      "source": [
        "\n",
        "**Cross-validation** is a popular machine learning technique used to test the **generalization capability** of models.  \n",
        "\n",
        "In this approach, we split the dataset into several parts (called *folds*). The model is trained on some of these folds and tested on the remaining ones. This process is repeated so that each fold serves as the test set once. Finally, we evaluate the model’s performance on each test set and take the **average score** as the overall performance.  \n",
        "\n",
        "📖 You can read more about different cross-validation techniques [here](https://www.geeksforgeeks.org/machine-learning/cross-validation-machine-learning/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test your understanding**: Below is a quiz to help us revise our understanding of these metrics and how to apply them in different situations. Note that some questions also include metrics for regression tasks, which we did not explain here but were introduced in the first practical session."
      ],
      "metadata": {
        "id": "V_uKUQtBO6D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🔓Quiz\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# -----------------------------\n",
        "# QUIZ DATA\n",
        "# -----------------------------\n",
        "quiz = [\n",
        "    {\n",
        "        \"question\": \"Q1. Model A has accuracy = 92%, Model B = 89% but lower false negatives. \"\n",
        "                    \"Which is better for medical diagnosis?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Either one\", \"D. Both are equally good\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"In medical diagnosis, recall (avoiding false negatives) matters more than raw accuracy.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q2. Model A: Precision=0.95, Recall=0.60. Model B: Precision=0.70, Recall=0.90. \"\n",
        "                    \"Which is better for fraud detection?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both are the same\", \"D. Depends on dataset size\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"Fraud detection requires catching as many fraudulent cases as possible (high recall).\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q3. Model A: False Positive Rate=0.05. Model B: False Positive Rate=0.15. \"\n",
        "                    \"Which is better for spam detection?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both are equal\", \"D. Can't decide without precision\"],\n",
        "        \"answer\": 0,\n",
        "        \"explanation\": \"A lower false positive rate means fewer legitimate emails flagged as spam.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q4. Model A: AUC=0.81. Model B: AUC=0.87. Which has better discrimination ability?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both are equal\", \"D. AUC not relevant\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"A higher AUC means the model separates classes better.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q5. Class imbalance (1% positive). Model A: Accuracy=99%, Recall=0%. \"\n",
        "                    \"Model B: Accuracy=95%, Recall=85%. Which is more useful?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Depends on precision\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"Model A ignores the minority class. Model B captures positives, so it's more useful.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q6. Model A: MAE=2.5. Model B: RMSE=2.8. Which handles large errors better?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Need more info\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"RMSE penalizes large errors more strongly.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q7. Model A: R²=0.72. Model B: R²=0.65. Which explains more variance?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. R² doesn’t measure variance\"],\n",
        "        \"answer\": 0,\n",
        "        \"explanation\": \"A higher R² means more variance explained.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q8. Model A: High training acc, low test acc. Model B: Lower training acc, higher test acc. \"\n",
        "                    \"Which should you deploy?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Depends on training acc\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"Generalization matters more; Model B overfits less.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q9. Model A: 1% higher accuracy but 10x slower. Model B: Slightly less accurate but faster. \"\n",
        "                    \"Which is better for real-time systems?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Accuracy always first\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"Speed is critical in real-time systems; Model B is preferable.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q10. Model A: Best accuracy, black box. Model B: Slightly worse but interpretable. \"\n",
        "                    \"Which is better in healthcare/finance?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Accuracy only matters\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"Interpretability is crucial in regulated domains.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Q11. Model A: Accuracy=95%, MCC=0.10. Model B: Accuracy=90%, MCC=0.65. \"\n",
        "                    \"Which is better overall?\",\n",
        "        \"options\": [\"A. Model A\", \"B. Model B\", \"C. Both equal\", \"D. Depends on recall\"],\n",
        "        \"answer\": 1,\n",
        "        \"explanation\": \"MCC is robust with imbalanced datasets. Model B is better overall.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "checkboxes = []\n",
        "question_widgets = []\n",
        "\n",
        "for q in quiz:\n",
        "    # Question text displayed as HTML (so it's on top)\n",
        "    question_label = widgets.HTML(value=f\"<b>{q['question']}</b>\")\n",
        "\n",
        "    # Options below\n",
        "    cb = widgets.RadioButtons(\n",
        "        options=q[\"options\"],\n",
        "        layout={'width': 'max-content'}\n",
        "    )\n",
        "\n",
        "    # Group them vertically\n",
        "    box = widgets.VBox([question_label, cb])\n",
        "\n",
        "    question_widgets.append(box)\n",
        "    checkboxes.append(cb)\n",
        "\n",
        "submit_btn = widgets.Button(description=\"Submit\", button_style=\"success\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_submit(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        for i, cb in enumerate(checkboxes):\n",
        "            print(quiz[i][\"question\"])\n",
        "            print(\"Your answer:\", cb.value)\n",
        "            correct = quiz[i][\"options\"][quiz[i][\"answer\"]]\n",
        "            if cb.value == correct:\n",
        "                print(\"✅ Correct!\")\n",
        "            else:\n",
        "                print(f\"❌ Wrong! Correct answer: {correct}\")\n",
        "            print(\"Explanation:\", quiz[i][\"explanation\"])\n",
        "            print(\"-\"*70)\n",
        "\n",
        "submit_btn.on_click(on_submit)\n",
        "\n",
        "# Display questions nicely\n",
        "for qw in question_widgets:\n",
        "    display(qw)\n",
        "\n",
        "display(submit_btn, output)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g-A9zs5ILEdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HNpEM4DnMNe"
      },
      "source": [
        "## Part 2 — Optimization Algorithms\n",
        "\n",
        "In this part we’ll explore **how** we set parameters and **how** we update them so that training is stable, fast, and generalises well.\n",
        "\n",
        "We’ll cover:\n",
        "1) **Parameter initialization** (random vs. variance scaling like He/Xavier)  \n",
        "2) **Gradient step optimisation** (learning rate, momentum, Adam)  \n",
        "3) **Regularisation** (L2/weight decay, early stopping, and brief notes on other techniques)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Parameter Initialization\n",
        "\n",
        "When training deep networks, parameter initialization matters because it controls the **variance of signals** as they propagate forward (activations) and backward (gradients). Poor choices lead to:\n",
        "- **Vanishing** signals (too small), slowing learning.\n",
        "- **Exploding** signals (too large), destabilising learning.\n",
        "\n",
        "A good initialization keeps the variance of activations roughly constant across layers. We will explore various simple and state-of-the-art initialisation schemes, starting with plain random initialisation and then move to **variance scaling** (Xavier/Glorot and He/Kaiming) that preserves variance layer-to-layer under certain activations.\n"
      ],
      "metadata": {
        "id": "xprTxw9s4BCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Random Initialization\n",
        "\n",
        "Most frameworks (like Flax) use normal or uniform random initializations by default.\n",
        "\n",
        "💡 Idea: Let’s observe what happens when we initialize weights randomly.\n",
        "\n",
        "To this end, let's wire `jax.random.normal` and `jax.random.normal` into our existing `get_model_and_optimizer`. Consider an MLP that takes the initialization type \"kernel_init\" as an additional argument.\n"
      ],
      "metadata": {
        "id": "Gy3rT0eqBNYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    layer_sizes: list\n",
        "    kernel_init: callable = nn.initializers.lecun_normal()  # default fallback\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for size in self.layer_sizes[:-1]:\n",
        "            x = nn.Dense(size, kernel_init=self.kernel_init)(x)\n",
        "            x = nn.gelu(x)\n",
        "        x = nn.Dense(self.layer_sizes[-1], kernel_init=self.kernel_init)(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4bySKhWxEp7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Modify `get_model_and_optimizer` so that:\n",
        "- It accepts `init_type` in `{\"normal\", \"uniform\"}` plus two tunables: `normal_std` and `uniform_limit`.\n",
        "- It passes the corresponding `kernel_init=` to each `nn.Dense`.\n",
        "- Everything else remains the same (optimizer still Adam by default).\n",
        "\n",
        "*Fill the `... # update me` spots; then run the tests cell.*"
      ],
      "metadata": {
        "id": "GYFbqbl7Cgi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EXERCISE: Random initialisation inside get_model_and_optimizer\n",
        "\n",
        "def normal_std(std=0.02):\n",
        "    def _init(key, shape, dtype=jnp.float32):\n",
        "        return ...  # update me:\n",
        "    return _init\n",
        "\n",
        "def uniform_range(limit=0.05):\n",
        "    def _init(key, shape, dtype=jnp.float32):\n",
        "        return ...  # update me:\n",
        "    return _init\n",
        "\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"normal\",\n",
        "                            normal_std_val=0.02,\n",
        "                            uniform_limit_val=0.05):\n",
        "    \"\"\"\n",
        "    Create MLP + Optax optimizer with configurable kernel initialization.\n",
        "    init_type in {\"normal\",\"uniform\"}.\n",
        "    \"\"\"\n",
        "    # choose kernel_init based on init_type\n",
        "    if init_type == \"normal\":\n",
        "        kernel_init = normal_std(normal_std_val)\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = uniform_range(uniform_limit_val)\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "\n",
        "    # Print model parameters\n",
        "    print_param_shapes(params[\"params\"])\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "id": "rjSHyBM36SeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: Random initialisation inside get_model_and_optimizer\n",
        "\n",
        "def normal_std(std=0.02):\n",
        "    def _init(key, shape, dtype=jnp.float32):\n",
        "        return std * jax.random.normal(key, shape, dtype)\n",
        "    return _init\n",
        "\n",
        "def uniform_range(limit=0.05):\n",
        "    def _init(key, shape, dtype=jnp.float32):\n",
        "        return jax.random.uniform(key, shape, dtype, minval=-limit, maxval=limit)\n",
        "    return _init\n",
        "\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"normal\",\n",
        "                            normal_std_val=0.02,\n",
        "                            uniform_limit_val=0.05):\n",
        "    if init_type == \"normal\":\n",
        "        kernel_init = normal_std(normal_std_val)\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = uniform_range(uniform_limit_val)\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "\n",
        "    print_param_shapes(params[\"params\"])\n",
        "    optimizer = optax.adam(learning_rate=lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "id": "90c7KjZlFDNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔬 TEST: Build a model with random normal and uniform initialisation\n",
        "batch_size = 32\n",
        "train_ds = train_dataset.shuffle(1000).batch(batch_size)\n",
        "test_ds  = test_dataset.batch(batch_size)\n",
        "\n",
        "input_size = 30\n",
        "output_sizes = [64, 64, 1]\n",
        "\n",
        "# normal\n",
        "model_n, params_n, opt_n, state_n = get_model_and_optimizer(\n",
        "    input_size, output_sizes, seed=0, lr=1e-3, init_type=\"normal\", normal_std_val=0.05)\n",
        "\n",
        "# uniform\n",
        "model_u, params_u, opt_u, state_u = get_model_and_optimizer(\n",
        "    input_size, output_sizes, seed=0, lr=1e-3, init_type=\"uniform\", uniform_limit_val=0.1)\n",
        "\n",
        "print(\"OK: models initialised with normal and uniform.\")\n"
      ],
      "metadata": {
        "id": "V_-YhUnTFQQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualising initial activations across layers**\n",
        "\n",
        "We push a batch of inputs through the untrained network and look at the **activation histograms** per layer. With poor initialisation you’ll see saturation (squashed near 0) or explosion (very wide).\n"
      ],
      "metadata": {
        "id": "YlP7xBeRFVRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualise initial activations layer-by-layer (fixed)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def forward_activations(params, model, x):\n",
        "    acts = []\n",
        "    variables = {\"params\": params[\"params\"]}\n",
        "    x0 = x\n",
        "    for lyr, _ in enumerate(model.layer_sizes[:-1]):\n",
        "        W = variables[\"params\"][f\"Dense_{lyr}\"][\"kernel\"]\n",
        "        B = variables[\"params\"][f\"Dense_{lyr}\"][\"bias\"]\n",
        "        x0 = jnp.dot(x0, W) + B\n",
        "        x0 = jax.nn.gelu(x0)\n",
        "        acts.append(np.asarray(x0))\n",
        "    return acts\n",
        "\n",
        "# --- get a small batch as a NumPy/JAX array ---\n",
        "# Preferred: as_numpy_iterator (works on eager TF)\n",
        "try:\n",
        "    xb = next(train_ds.as_numpy_iterator())[0]   # (X_batch, y_batch) -> take features\n",
        "except AttributeError:\n",
        "    # Fallback: take(1) + .numpy()\n",
        "    xb = next(iter(train_ds.take(1)))[0].numpy()\n",
        "x_batch = jnp.array(xb)\n",
        "\n",
        "# --- run with a few different inits ---\n",
        "to_run = {\n",
        "    \"normal std=0.01\": get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[64,64,1], seed=1, lr=1e-3,\n",
        "        init_type=\"normal\", normal_std_val=0.01)[:2],\n",
        "    \"normal std=0.2\": get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[64,64,1], seed=1, lr=1e-3,\n",
        "        init_type=\"normal\", normal_std_val=0.2)[:2],\n",
        "    \"uniform lim=0.05\": get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[64,64,1], seed=1, lr=1e-3,\n",
        "        init_type=\"uniform\", uniform_limit_val=0.05)[:2],\n",
        "}\n",
        "\n",
        "for label, (m, p) in to_run.items():\n",
        "    acts = forward_activations(p, m, x_batch)\n",
        "    fig, axes = plt.subplots(1, len(acts), figsize=(12, 3))\n",
        "    fig.suptitle(f\"Initial activations — {label}\")\n",
        "    if len(acts) == 1:\n",
        "        axes = [axes]\n",
        "    for i, a in enumerate(acts):\n",
        "        ax = axes[i]\n",
        "        ax.hist(a.flatten(), bins=30)\n",
        "        ax.set_title(f\"Layer {i+1}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "t90gNaKlFcxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🤔 Pause and reflect:\n",
        "\n",
        "What happens if you increase the standard deviation of initialization?\n",
        "\n",
        "Do activations or losses diverge?"
      ],
      "metadata": {
        "id": "ZCK0BVCpGI8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 He Initialisation (and Xavier)\n",
        "\n",
        "To preserve variance across layers, He and Xavier propose scaling the initialization by the number of input units (fan-in) $n_{\\text{in}}$:\n",
        "\n",
        "- **He normal (ReLU-like):**  \n",
        "  $ W \\sim \\mathcal{N}\\!\\left(0,\\ \\frac{2}{n_{\\text{in}}}\\right) $\n",
        "\n",
        "- **Xavier normal (tanh/sigmoid):**  \n",
        "  $ W \\sim \\mathcal{N}\\!\\left(0,\\ \\frac{1}{n_{\\text{in}}}\\right) $\n",
        "\n",
        "**Exercise:** Add `init_type` values `\"he\"` and `\"xavier\"` to `get_model_and_optimizer`, using the corresponding `kernel_init=`.\n"
      ],
      "metadata": {
        "id": "g05u_7iXF6NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EXERCISE: Add He/Xavier to get_model_and_optimizer\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"he\",  # default to he for ReLU/GELU\n",
        "                            normal_std_val=0.02,\n",
        "                            uniform_limit_val=0.05):\n",
        "    # choose kernel_init\n",
        "    if init_type == \"normal\":\n",
        "        kernel_init = ...  # update me\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = ...  # update me\n",
        "    elif init_type == \"he\":\n",
        "        kernel_init = ...  # update me (use he_normal)\n",
        "    elif init_type == \"xavier\":\n",
        "        kernel_init = ...  # update me (use xavier_normal)\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "    print_param_shapes(params[\"params\"])\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "id": "UfGnQaIQGtO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: Add He/Xavier to get_model_and_optimizer\n",
        "\n",
        "def he_normal(key, shape, dtype=jnp.float32):\n",
        "    # N(0, 2 / fan_in)\n",
        "    fan_in = shape[0] if len(shape) >= 2 else 1\n",
        "    std = jnp.sqrt(2.0 / fan_in)\n",
        "    return std * jax.random.normal(key, shape, dtype)\n",
        "\n",
        "def xavier_normal(key, shape, dtype=jnp.float32):\n",
        "    # N(0, 1 / fan_in)\n",
        "    fan_in = shape[0] if len(shape) >= 2 else 1\n",
        "    std = jnp.sqrt(1.0 / fan_in)\n",
        "    return std * jax.random.normal(key, shape, dtype)\n",
        "\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"he\",\n",
        "                            normal_std_val=0.02,\n",
        "                            uniform_limit_val=0.05):\n",
        "    if init_type == \"normal\":\n",
        "        kernel_init = normal_std(normal_std_val)\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = uniform_range(uniform_limit_val)\n",
        "    elif init_type == \"he\":\n",
        "        kernel_init = he_normal\n",
        "    elif init_type == \"xavier\":\n",
        "        kernel_init = xavier_normal\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "    print_param_shapes(params[\"params\"])\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EX5gije6Gxc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demo: random vs. He initialisation**\n",
        "\n",
        "We’ll train short runs with (a) normal std=0.05 and (b) He, then compare train/test losses.\n"
      ],
      "metadata": {
        "id": "RRanEexDHDtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train & compare: Random (normal) vs He\n",
        "epochs = 40\n",
        "output_sizes = [64, 64, 1]\n",
        "metrics_history_a = {\"train_loss\": [], \"test_loss\": []}\n",
        "metrics_history_b = {\"train_loss\": [], \"test_loss\": []}\n",
        "\n",
        "# (a) random normal\n",
        "model_a, params_a, opt_a, state_a = get_model_and_optimizer(\n",
        "    input_size=30, output_sizes=output_sizes, seed=0, lr=1e-3, init_type=\"normal\", normal_std_val=0.05)\n",
        "params_a, state_a, mh_a = train(epochs, params_a, model_a, opt_a, state_a,\n",
        "                                loss_grad_fn, loss_func, train_ds, test_ds, metrics_history_a)\n",
        "\n",
        "# (b) He\n",
        "model_b, params_b, opt_b, state_b = get_model_and_optimizer(\n",
        "    input_size=30, output_sizes=output_sizes, seed=0, lr=1e-3, init_type=\"he\")\n",
        "params_b, state_b, mh_b = train(epochs, params_b, model_b, opt_b, state_b,\n",
        "                                loss_grad_fn, loss_func, train_ds, test_ds, metrics_history_b)\n",
        "\n",
        "plot_metrics([mh_a, mh_b], [\"normal\", \"he\"])"
      ],
      "metadata": {
        "id": "A9Uqgd0bHJtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gradient Steps Optimisation\n",
        "\n",
        "Even with good initialisation, **how** we step matters: the **learning rate** controls stability/speed; **momentum** helps traverse ravines; **Adam** adapts per-parameter step sizes with bias correction.\n"
      ],
      "metadata": {
        "id": "4owuWGJDHT8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Effect of Learning Rate\n",
        "\n",
        "First, we’ll see plain gradient descent on a toy quadratic:\n",
        "$$\n",
        "f(\\theta) = \\tfrac12\\,\\theta^\\top A \\theta \\quad(\\text{convex})\n",
        "$$\n",
        "and visualise trajectories with different learning rates.\n",
        "\n",
        "Then we’ll compare train/test curves for a small MLP on our dataset with **low**, **good**, and **high** learning rates.\n"
      ],
      "metadata": {
        "id": "8jvzMdwRHYn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualising GD paths on a 2D quadratic for different learning rates\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "A = np.array([[3., 0.], [0., 1.]])\n",
        "def f(theta): return 0.5 * theta.T @ A @ theta\n",
        "def grad(theta): return A @ theta\n",
        "\n",
        "def gd(theta0, lr, steps=25):\n",
        "    path = [theta0.copy()]\n",
        "    theta = theta0.copy()\n",
        "    for _ in range(steps):\n",
        "        theta -= lr * grad(theta)\n",
        "        path.append(theta.copy())\n",
        "    return np.array(path)\n",
        "\n",
        "thetas = {}\n",
        "theta0 = np.array([2.5, 1.5])\n",
        "for lr in [0.05, 0.2, 0.6]:\n",
        "    thetas[lr] = gd(theta0, lr, steps=30)\n",
        "\n",
        "# contour plot\n",
        "xs = np.linspace(-3, 3, 200)\n",
        "ys = np.linspace(-3, 3, 200)\n",
        "X, Y = np.meshgrid(xs, ys)\n",
        "Z = 0.5*(3*X**2 + 1*Y**2)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "cs = plt.contour(X, Y, Z, levels=20)\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "for lr, path in thetas.items():\n",
        "    plt.plot(path[:,0], path[:,1], marker='o', label=f\"lr={lr}\")\n",
        "plt.legend(); plt.title(\"GD paths on quadratic\")\n",
        "plt.xlabel(\"theta1\"); plt.ylabel(\"theta2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aqz0D3pgHhr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train small MLP with low/good/high learning rates (SGD)\n",
        "def run_lr(lr):\n",
        "    model, params, opt, state = get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[32, 32, 1], seed=0, lr=lr, init_type=\"he\")\n",
        "    mh = {\"train_loss\": [], \"test_loss\": []}\n",
        "    # swap optimizer to SGD for this experiment\n",
        "    opt = optax.sgd(learning_rate=lr)\n",
        "    state = opt.init(params)\n",
        "    params2, state2, mh = train(epochs=60, params=params, model=model,\n",
        "                                optimizer=opt, opt_state=state,\n",
        "                                loss_grad_fn=loss_grad_fn, loss_fn=loss_func,\n",
        "                                train_ds=train_ds, test_ds=test_ds,\n",
        "                                metrics_history=mh)\n",
        "    return mh\n",
        "\n",
        "mh_low  = run_lr(1e-4)\n",
        "mh_good = run_lr(1e-2)\n",
        "mh_high = run_lr(5e-1)\n",
        "\n",
        "plot_metrics([mh_low, mh_good, mh_high], [\"Low\", \"Good\", \"High\"])"
      ],
      "metadata": {
        "id": "duLEq-vHHplJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VoNrYhsoT0qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7yaOeVcT43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Momentum\n",
        "\n",
        "**Update rule:**\n",
        "$$\n",
        "v_t = \\beta v_{t-1} + (1-\\beta)\\,\\nabla_\\theta L(\\theta),\\quad\n",
        "\\theta \\leftarrow \\theta - \\eta\\, v_t\n",
        "$$\n",
        "Momentum accelerates along shallow valleys and damps oscillations.\n",
        "\n",
        "**Exercise:** Add an `optimizer_name` switch to `get_model_and_optimizer` so we can choose `\"sgd\"`, `\"momentum\"`, or `\"adam\"` (default). For momentum, expose `beta` (e.g., 0.9).\n"
      ],
      "metadata": {
        "id": "INHNJNakHtHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EXERCISE: Add optimizer_name= {\"sgd\", \"momentum\", \"adam\"} to get_model_and_optimizer\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"he\",\n",
        "                            optimizer_name=\"adam\",\n",
        "                            momentum_beta=0.9):\n",
        "    # pick kernel_init (same as before)\n",
        "    if init_type == \"he\":\n",
        "        kernel_init = he_normal\n",
        "    elif init_type == \"xavier\":\n",
        "        kernel_init = xavier_normal\n",
        "    elif init_type == \"normal\":\n",
        "        kernel_init = normal_std(0.05)\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = uniform_range(0.05)\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "\n",
        "    # TODO: select optimizer based on optimizer_name\n",
        "    if optimizer_name == \"sgd\":\n",
        "        optimizer = ...  # update me: optax.sgd(learning_rate=lr)\n",
        "    elif optimizer_name == \"momentum\":\n",
        "        optimizer = ...  # update me: optax.sgd(learning_rate=lr, momentum=momentum_beta, nesterov=False)\n",
        "    else:\n",
        "        optimizer = ...  # update me: optax.adam(learning_rate=lr)\n",
        "\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "id": "niICXn0IH5yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: optimizer_name switch\n",
        "def get_model_and_optimizer(input_size,\n",
        "                            output_sizes=None,\n",
        "                            seed=32,\n",
        "                            lr=1e-3,\n",
        "                            init_type=\"he\",\n",
        "                            optimizer_name=\"adam\",\n",
        "                            momentum_beta=0.9):\n",
        "    if init_type == \"he\":\n",
        "        kernel_init = he_normal\n",
        "    elif init_type == \"xavier\":\n",
        "        kernel_init = xavier_normal\n",
        "    elif init_type == \"normal\":\n",
        "        kernel_init = normal_std(0.05)\n",
        "    elif init_type == \"uniform\":\n",
        "        kernel_init = uniform_range(0.05)\n",
        "    else:\n",
        "        kernel_init = nn.initializers.lecun_normal()\n",
        "\n",
        "    model = MLP(output_sizes, kernel_init=kernel_init)\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "    params = model.init(key, dummy)\n",
        "\n",
        "    if optimizer_name == \"sgd\":\n",
        "        optimizer = optax.sgd(learning_rate=lr)\n",
        "    elif optimizer_name == \"momentum\":\n",
        "        optimizer = optax.sgd(learning_rate=lr, momentum=momentum_beta, nesterov=False)\n",
        "    else:\n",
        "        optimizer = optax.adam(learning_rate=lr)\n",
        "\n",
        "    opt_state = optimizer.init(params)\n",
        "    return model, params, optimizer, opt_state\n"
      ],
      "metadata": {
        "id": "QZngUCRIH9aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Demo: Visual momentum effect on our dataset\n",
        "epochs = 80\n",
        "cfgs = [\n",
        "    (\"SGD\",       dict(optimizer_name=\"sgd\", lr=1e-2)),\n",
        "    (\"Momentum\",  dict(optimizer_name=\"momentum\", lr=1e-2, momentum_beta=0.9)),\n",
        "]\n",
        "histories = {}\n",
        "for name, kwargs in cfgs:\n",
        "    model, params, opt, state = get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[64,64,1], seed=0, init_type=\"he\", **kwargs)\n",
        "    mh = {\"train_loss\": [], \"test_loss\": []}\n",
        "    params, state, mh = train(epochs, params, model, opt, state,\n",
        "                              loss_grad_fn, loss_func, train_ds, test_ds, mh)\n",
        "    histories[name] = mh\n"
      ],
      "metadata": {
        "id": "CWngW2jEIBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Adam Optimizer\n",
        "\n",
        "Adam maintains **exponentially decaying moving averages** of gradients and squared gradients and applies **bias correction**:\n",
        "\n",
        "**Pseudocode:**\n"
      ],
      "metadata": {
        "id": "rZyAEH_zIED7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise:** Using the `optimizer_name` switch you added, instantiate Adam and compare **SGD vs. Momentum vs. Adam**.\n"
      ],
      "metadata": {
        "id": "qOuZCZwlIOCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: Compare SGD / Momentum / Adam side-by-side\n",
        "epochs = 80\n",
        "cfgs = [\n",
        "    (\"SGD\",      dict(optimizer_name=\"sgd\", lr=1e-2)),\n",
        "    (\"Momentum\", dict(optimizer_name=\"momentum\", lr=1e-2, momentum_beta=0.9)),\n",
        "    (\"Adam\",     dict(optimizer_name=\"adam\", lr=1e-3)),\n",
        "]\n",
        "\n",
        "histories = {}\n",
        "for name, kwargs in cfgs:\n",
        "    model, params, opt, state = get_model_and_optimizer(\n",
        "        input_size=30, output_sizes=[64,64,1], seed=1, init_type=\"he\", **kwargs)\n",
        "    mh = {\"train_loss\": [], \"test_loss\": []}\n",
        "    params, state, mh = train(epochs, params, model, opt, state,\n",
        "                              loss_grad_fn, loss_func, train_ds, test_ds, mh)\n",
        "    histories[name] = mh\n"
      ],
      "metadata": {
        "id": "kbBA7roJIMIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Regularisation\n",
        "\n",
        "Regularisation discourages overly complex solutions that overfit the training data. We’ll implement **L2/weight decay** and **early stopping**, then briefly list other useful methods.\n",
        "\n",
        "> Reference slides: **CM20315 Regularization slides** (add your link here).\n"
      ],
      "metadata": {
        "id": "BIGSComnIgLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 L2 Regularisation (Weight Decay)\n",
        "\n",
        "**Objective:**\n",
        "$$\n",
        "L_{\\text{reg}}(\\theta) = L_{\\text{data}}(\\theta) + \\lambda \\|\\theta\\|_2^2\n",
        "$$\n",
        "\n",
        "**Probabilistic view:** Equivalent to a **Gaussian prior** on weights $ \\theta \\sim \\mathcal{N}(0,\\ \\sigma^2 I) $, where $\\lambda \\propto 1/\\sigma^2$.\n",
        "\n",
        "**Exercise:** Modify `loss_func` to add L2 penalty over all parameters when `weight_decay > 0`. Keep the original return signature `(loss, logits)`.\n"
      ],
      "metadata": {
        "id": "FdpZtxRoIjhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EXERCISE: Add weight decay to loss_func\n",
        "def loss_func(params, model, batch, weight_decay=0.0):\n",
        "    \"\"\"\n",
        "    Compute BCE loss + optional L2 penalty (weight decay).\n",
        "    Returns: (loss, logits)\n",
        "    \"\"\"\n",
        "    x, y = batch\n",
        "    # 1) logits\n",
        "    logits = ...  # update me: model.apply({\"params\": params[\"params\"]}, x)\n",
        "    y = jnp.reshape(jnp.array(y), logits.shape)\n",
        "\n",
        "    # 2) BCE with logits\n",
        "    bce = jnp.mean(optax.sigmoid_binary_cross_entropy(logits, y))\n",
        "\n",
        "    # 3) L2 penalty across all params if weight_decay > 0\n",
        "    if weight_decay > 0:\n",
        "        def sqsum(tree):\n",
        "            return sum([jnp.sum(jnp.square(v)) for v in jax.tree_util.tree_leaves(tree)])\n",
        "        l2 = ...  # update me: sqsum(params[\"params\"])\n",
        "    else:\n",
        "        l2 = 0.0\n",
        "\n",
        "    loss = bce + weight_decay * l2\n",
        "    return loss, logits\n",
        "\n",
        "# and its gradient fn\n",
        "loss_grad_fn = jax.value_and_grad(lambda p, m, batch: loss_func(p, m, batch)[0:2], has_aux=True)\n"
      ],
      "metadata": {
        "id": "kSjM7N_QIr15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: L2 weight decay in loss_func\n",
        "def loss_func(params, model, batch, weight_decay=0.0):\n",
        "    x, y = batch\n",
        "    logits = model.apply({\"params\": params[\"params\"]}, jnp.array(x.to_numpy()) if hasattr(x, \"to_numpy\") else x)\n",
        "    y = jnp.reshape(jnp.array(y), logits.shape)\n",
        "\n",
        "    bce = jnp.mean(optax.sigmoid_binary_cross_entropy(logits, y))\n",
        "\n",
        "    l2 = 0.0\n",
        "    if weight_decay > 0:\n",
        "        def sqsum(tree):\n",
        "            return sum([jnp.sum(jnp.square(v)) for v in jax.tree_util.tree_leaves(tree)])\n",
        "        l2 = sqsum(params[\"params\"])\n",
        "\n",
        "    loss = bce + weight_decay * l2\n",
        "    return loss, logits\n",
        "\n",
        "loss_grad_fn = jax.value_and_grad(lambda p, m, batch: loss_func(p, m, batch)[0:2], has_aux=True)\n"
      ],
      "metadata": {
        "id": "QxZT5W8NIxqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Practical: train with/without L2 and compare test accuracy\n",
        "def predict(params, model, x, threshold=0.5):\n",
        "    logits = model.apply({\"params\": params[\"params\"]}, jnp.array(x.to_numpy()) if hasattr(x, \"to_numpy\") else x)\n",
        "    probs = jax.nn.sigmoid(logits)\n",
        "    preds = (probs >= threshold).astype(jnp.int32)\n",
        "    return preds.squeeze()\n",
        "\n",
        "def evaluate_accuracy(params, model, x, y):\n",
        "    yhat = predict(params, model, x)\n",
        "    ytrue = jnp.array(y.to_numpy() if hasattr(y, \"to_numpy\") else y)\n",
        "    return (yhat == ytrue).mean()\n",
        "\n",
        "epochs = 80\n",
        "output_sizes = [64,64,1]\n",
        "\n",
        "# no weight decay\n",
        "model0, params0, opt0, state0 = get_model_and_optimizer(30, output_sizes, seed=0, init_type=\"he\", optimizer_name=\"adam\", lr=1e-3)\n",
        "mh0 = {\"train_loss\": [], \"test_loss\": []}\n",
        "params0, state0, mh0 = train(epochs, params0, model0, opt0, state0,\n",
        "                             loss_grad_fn, lambda p,m,b: loss_func(p,m,b,weight_decay=0.0),\n",
        "                             train_ds, test_ds, mh0)\n",
        "acc0 = float(evaluate_accuracy(params0, model0, x_test, y_test))\n",
        "\n",
        "# with weight decay\n",
        "model1, params1, opt1, state1 = get_model_and_optimizer(30, output_sizes, seed=0, init_type=\"he\", optimizer_name=\"adam\", lr=1e-3)\n",
        "mh1 = {\"train_loss\": [], \"test_loss\": []}\n",
        "params1, state1, mh1 = train(epochs, params1, model1, opt1, state1,\n",
        "                             loss_grad_fn, lambda p,m,b: loss_func(p,m,b,weight_decay=1e-5),\n",
        "                             train_ds, test_ds, mh1)\n",
        "acc1 = float(evaluate_accuracy(params1, model1, x_test, y_test))\n",
        "\n",
        "print(f\"Test accuracy (no WD): {acc0:.4f}\")\n",
        "print(f\"Test accuracy (WD=1e-5): {acc1:.4f}\")\n"
      ],
      "metadata": {
        "id": "AedbJtiyI19n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Early Stopping\n",
        "\n",
        "**Idea:** Stop training when validation loss stops improving for some **patience**. This reduces the *effective capacity* and helps generalisation.\n",
        "\n",
        "We’ll add a simple early-stopping wrapper.  \n",
        "\n",
        "**Exercise (Easy):** Implement `train_with_early_stopping` with `patience` and `min_delta`. Save the **best** parameters (lowest test loss).\n"
      ],
      "metadata": {
        "id": "DoarzCZlI5fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EXERCISE: Early stopping training loop\n",
        "def train_with_early_stopping(epochs, params, model, optimizer, opt_state,\n",
        "                              loss_grad_fn, loss_fn, train_ds, test_ds,\n",
        "                              metrics_history, patience=10, min_delta=0.0):\n",
        "    best_loss = jnp.inf\n",
        "    best_params = params\n",
        "    wait = 0\n",
        "\n",
        "    for i in range(epochs):\n",
        "        train_metrics = TrainMetrics.empty()\n",
        "        for batch in train_ds.as_numpy_iterator():\n",
        "            params, opt_state, train_metrics = train_step(params, model, optimizer, opt_state, loss_grad_fn, train_metrics, batch)\n",
        "        for metric, value in train_metrics.compute().items():\n",
        "            metrics_history[f\"train_{metric}\"].append(value)\n",
        "\n",
        "        eval_metrics = EvalMetrics.empty()\n",
        "        for batch in test_ds.as_numpy_iterator():\n",
        "            eval_metrics = eval_step(params, model, loss_fn, eval_metrics, batch)\n",
        "        # get current validation loss\n",
        "        val_loss = ...  # update me: list(eval_metrics.compute().items())[0][1] or eval_metrics.compute()[\"loss\"]\n",
        "        metrics_history[\"test_loss\"].append(val_loss)\n",
        "\n",
        "        # early stopping check\n",
        "        if best_loss - val_loss > min_delta:\n",
        "            best_loss = val_loss\n",
        "            best_params = ...  # update me\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {i+1}. Best val loss={float(best_loss):.4f}\")\n",
        "                break\n",
        "\n",
        "    return best_params, opt_state, metrics_history\n"
      ],
      "metadata": {
        "id": "BjDCB3s1I7wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✅ SOLUTION: Early stopping training loop\n",
        "def train_with_early_stopping(epochs, params, model, optimizer, opt_state,\n",
        "                              loss_grad_fn, loss_fn, train_ds, test_ds,\n",
        "                              metrics_history, patience=10, min_delta=0.0):\n",
        "    best_loss = jnp.inf\n",
        "    best_params = params\n",
        "    wait = 0\n",
        "\n",
        "    for i in range(epochs):\n",
        "        train_metrics = TrainMetrics.empty()\n",
        "        for batch in train_ds.as_numpy_iterator():\n",
        "            params, opt_state, train_metrics = train_step(params, model, optimizer, opt_state, loss_grad_fn, train_metrics, batch)\n",
        "        for metric, value in train_metrics.compute().items():\n",
        "            metrics_history[f\"train_{metric}\"].append(value)\n",
        "\n",
        "        eval_metrics = EvalMetrics.empty()\n",
        "        for batch in test_ds.as_numpy_iterator():\n",
        "            eval_metrics = eval_step(params, model, loss_fn, eval_metrics, batch)\n",
        "\n",
        "        cur = eval_metrics.compute()\n",
        "        val_loss = cur[\"loss\"] if \"loss\" in cur else list(cur.values())[0]\n",
        "        metrics_history[\"test_loss\"].append(val_loss)\n",
        "\n",
        "        if best_loss - val_loss > min_delta:\n",
        "            best_loss = val_loss\n",
        "            best_params = params\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"Early stopping at epoch {i+1}. Best val loss={float(best_loss):.4f}\")\n",
        "                break\n",
        "\n",
        "    return best_params, opt_state, metrics_history\n"
      ],
      "metadata": {
        "id": "UV0UvstiI9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compare with vs. without early stopping\n",
        "epochs = 200\n",
        "output_sizes = [64,64,1]\n",
        "\n",
        "# baseline long training (no ES)\n",
        "model_a, params_a, opt_a, state_a = get_model_and_optimizer(30, output_sizes, seed=0, init_type=\"he\", optimizer_name=\"adam\", lr=1e-3)\n",
        "mh_a = {\"train_loss\": [], \"test_loss\": []}\n",
        "params_a, state_a, mh_a = train(epochs, params_a, model_a, opt_a, state_a,\n",
        "                                loss_grad_fn, loss_func, train_ds, test_ds, mh_a)\n",
        "acc_a = float(evaluate_accuracy(params_a, model_a, x_test, y_test))\n",
        "\n",
        "# with ES\n",
        "model_b, params_b, opt_b, state_b = get_model_and_optimizer(30, output_sizes, seed=0, init_type=\"he\", optimizer_name=\"adam\", lr=1e-3)\n",
        "mh_b = {\"train_loss\": [], \"test_loss\": []}\n",
        "best_params_b, state_b, mh_b = train_with_early_stopping(\n",
        "    epochs, params_b, model_b, opt_b, state_b,\n",
        "    loss_grad_fn, loss_func, train_ds, test_ds, mh_b,\n",
        "    patience=15, min_delta=1e-4)\n",
        "acc_b = float(evaluate_accuracy(best_params_b, model_b, x_test, y_test))\n",
        "\n",
        "print(f\"Test accuracy (no ES): {acc_a:.4f}\")\n",
        "print(f\"Test accuracy (with ES): {acc_b:.4f}\")\n"
      ],
      "metadata": {
        "id": "u8gQUEjEJCuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Other approaches (briefly)\n",
        "\n",
        "- **Dropout:** Randomly zero out activations during training to reduce co-adaptation.  \n",
        "- **Ensembling:** Average predictions from multiple models.  \n",
        "- **Noise injection:** Add noise to inputs/weights/gradients.  \n",
        "- **Bayesian approaches:** Place priors over parameters and infer posteriors.  \n",
        "- **Transfer learning / Multi-task learning:** Share representations to regularise.  \n",
        "- **Self-supervised pretraining:** Better initial representations → less overfitting.  \n",
        "- **Data augmentation:** Domain-appropriate transforms that preserve labels.\n",
        "\n",
        "> Try these as stretch tasks: add dropout layers to the MLP, or augment features if you had images/text/time-series data.\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/d/1wDmuNI-kTziT5ZUFxhpstCRkjOE9obSr\" width=\"1000\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "hlEtTtPfJGFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrap-up\n",
        "\n",
        "- **Initialisation** affects signal propagation and convergence — He works well with ReLU/GELU.  \n",
        "- **Optimisers** trade stability and speed — Momentum and Adam generally accelerate training.  \n",
        "- **Regularisation** (L2, early stopping, …) improves generalisation.\n",
        "\n",
        "**Next:** You can extend this part with **learning-rate schedules** (e.g., cosine, step decay) and **hyperparameter sweeps** (grid/random/Bayesian) to complete the section promised in your outline.\n"
      ],
      "metadata": {
        "id": "6tpSuRDAJIZ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dmPgHGhH8oU"
      },
      "source": [
        "## Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6YYbpyXpqib"
      },
      "source": [
        "### References\n",
        "1. Flax module documentation: https://flax-linen.readthedocs.io/en/latest/api_reference/flax.linen/module.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajMt83KFmeMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/CJCNwwcLW9Y3jZDG7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}